{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["CWtMt1Akb6qy","T2KT1C3hVoRt","vWvLfsg3ynWQ"],"authorship_tag":"ABX9TyM09PicRwbvIu5IGeGr0m+H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"260dc09cfd084e39b0353cf0bfb46d54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b3382c482734a87b8f2ec62fd9431d8","IPY_MODEL_9ffc4e42326844c9b1f1563122ad738d","IPY_MODEL_b175e995e8694a448325ad2fc1e9d410"],"layout":"IPY_MODEL_a758f340857840dd96e59bb9d51cb3a9"}},"4b3382c482734a87b8f2ec62fd9431d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6a0102bc6e741c8bd6bd03c8a9f0583","placeholder":"​","style":"IPY_MODEL_fc14a9fe94cc446d8e96d3adb7a4ad16","value":"modules.json: 100%"}},"9ffc4e42326844c9b1f1563122ad738d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8529f352807a48a799c8dc28ccd934e9","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e9005ddc5a14ad09b37a5211440c099","value":349}},"b175e995e8694a448325ad2fc1e9d410":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37730e1dc7784b85b00f3283d45f5739","placeholder":"​","style":"IPY_MODEL_0a7d895ba997491a9b4c7fbd814e58b0","value":" 349/349 [00:00&lt;00:00, 13.0kB/s]"}},"a758f340857840dd96e59bb9d51cb3a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6a0102bc6e741c8bd6bd03c8a9f0583":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc14a9fe94cc446d8e96d3adb7a4ad16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8529f352807a48a799c8dc28ccd934e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e9005ddc5a14ad09b37a5211440c099":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37730e1dc7784b85b00f3283d45f5739":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a7d895ba997491a9b4c7fbd814e58b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4764a572c0c641cf800a11d73dfb43b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c40b6effa729441e8e5a679409583d1f","IPY_MODEL_41c75f7079594fa0b8e29ac31a43e528","IPY_MODEL_66d8efdf67f447a7844a35cb7955af4d"],"layout":"IPY_MODEL_ada382641da64902bbd18856e4afd13f"}},"c40b6effa729441e8e5a679409583d1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94cf118b3ce9479a9aaa518903b9999f","placeholder":"​","style":"IPY_MODEL_6360b54750034c75b7be9bdde3cac7f9","value":"config_sentence_transformers.json: 100%"}},"41c75f7079594fa0b8e29ac31a43e528":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f8733cb021f414b9594f94db35ca5a4","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4beff1346b64caa911a3938d06385f3","value":124}},"66d8efdf67f447a7844a35cb7955af4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5d59a85687d479b9bbaec70784b48a3","placeholder":"​","style":"IPY_MODEL_63102f11f33e46efa3e5787ea4e1a27a","value":" 124/124 [00:00&lt;00:00, 7.40kB/s]"}},"ada382641da64902bbd18856e4afd13f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94cf118b3ce9479a9aaa518903b9999f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6360b54750034c75b7be9bdde3cac7f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f8733cb021f414b9594f94db35ca5a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4beff1346b64caa911a3938d06385f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5d59a85687d479b9bbaec70784b48a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63102f11f33e46efa3e5787ea4e1a27a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83d7f8bbbea5484ba47004b5f20c8742":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76b1e496af32491bbdd5272dc683c90f","IPY_MODEL_b1629993b0504943ab706e0ddf9063aa","IPY_MODEL_2190b4135853473b9a5a5610d6185201"],"layout":"IPY_MODEL_998fd66e0e834d4096b2caf93b726b7e"}},"76b1e496af32491bbdd5272dc683c90f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2759f30be5a4e3a9fea9b84fe2eaf1b","placeholder":"​","style":"IPY_MODEL_6b7aa99201f644dc958fbf599ec1b119","value":"README.md: 100%"}},"b1629993b0504943ab706e0ddf9063aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ecf358ade1b495f85b466e3b87d5e27","max":94607,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ed47004f08742bf8f25faae9669c4b3","value":94607}},"2190b4135853473b9a5a5610d6185201":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59a4118732914fb2ae0332aed4130b62","placeholder":"​","style":"IPY_MODEL_b10e9d9dcb544a57a2be4075ef6a7b2f","value":" 94.6k/94.6k [00:00&lt;00:00, 2.79MB/s]"}},"998fd66e0e834d4096b2caf93b726b7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2759f30be5a4e3a9fea9b84fe2eaf1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b7aa99201f644dc958fbf599ec1b119":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ecf358ade1b495f85b466e3b87d5e27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ed47004f08742bf8f25faae9669c4b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59a4118732914fb2ae0332aed4130b62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b10e9d9dcb544a57a2be4075ef6a7b2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50c45f92fea3470ebc514a4b76018816":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12ab644a3fef4e5e8db9bac78e7f3e32","IPY_MODEL_f68d1325fd1f4f4e8746705306b738bd","IPY_MODEL_90230940e2ce432b839b8418aaccc3c4"],"layout":"IPY_MODEL_92ed089d23f743769ed41133f8cdd29e"}},"12ab644a3fef4e5e8db9bac78e7f3e32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad52d9d85f7843368de767cc93b37c19","placeholder":"​","style":"IPY_MODEL_26f0b0e8227349c09d5515fd32aad459","value":"sentence_bert_config.json: 100%"}},"f68d1325fd1f4f4e8746705306b738bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f83b00560914fa2ad13e8b9851fc1c9","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a09dbf1a6f794865843d0d1ae25945f9","value":52}},"90230940e2ce432b839b8418aaccc3c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bd580c1e8fa44c297c1db5c38d8856d","placeholder":"​","style":"IPY_MODEL_6d65c27b1f80452baddc117a4b370af6","value":" 52.0/52.0 [00:00&lt;00:00, 3.25kB/s]"}},"92ed089d23f743769ed41133f8cdd29e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad52d9d85f7843368de767cc93b37c19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26f0b0e8227349c09d5515fd32aad459":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f83b00560914fa2ad13e8b9851fc1c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a09dbf1a6f794865843d0d1ae25945f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bd580c1e8fa44c297c1db5c38d8856d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d65c27b1f80452baddc117a4b370af6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c257ac7cbad641e880f2b87055ea79d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c63521309b07460bba58c740700c1c73","IPY_MODEL_245d3a63d0144a688c94211fb0a04576","IPY_MODEL_6371a60f697a42cfbd2c4d0b60512e60"],"layout":"IPY_MODEL_037dc1196793444baad9e9c989d45aa9"}},"c63521309b07460bba58c740700c1c73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c377718d6e29406387c434660f3b7c07","placeholder":"​","style":"IPY_MODEL_81b37f5c3fe04f79bed75fdec9faa994","value":"config.json: 100%"}},"245d3a63d0144a688c94211fb0a04576":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18a416573f864e82b5dbfb7285c07888","max":779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1894cd22fb104ce599c994da440e8ff5","value":779}},"6371a60f697a42cfbd2c4d0b60512e60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08cfd79236734b2eaced2ccca7c3096c","placeholder":"​","style":"IPY_MODEL_d419839d49714a40b507b2e2fa43b841","value":" 779/779 [00:00&lt;00:00, 33.9kB/s]"}},"037dc1196793444baad9e9c989d45aa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c377718d6e29406387c434660f3b7c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81b37f5c3fe04f79bed75fdec9faa994":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18a416573f864e82b5dbfb7285c07888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1894cd22fb104ce599c994da440e8ff5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08cfd79236734b2eaced2ccca7c3096c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d419839d49714a40b507b2e2fa43b841":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"817fb1800c07446bb3719260eac2d9d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adaceb2fede5403895132b373bfec869","IPY_MODEL_705e70006b80435c83998b9c8f5fb8d4","IPY_MODEL_120635bb691441ef987312eb166cfa7e"],"layout":"IPY_MODEL_b0d8dc5e1f3c46f5844197fff82f12ad"}},"adaceb2fede5403895132b373bfec869":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2367bdb1c384985be915ed87669967d","placeholder":"​","style":"IPY_MODEL_3bcb5855ba604431823c4ebd56510ba1","value":"model.safetensors: 100%"}},"705e70006b80435c83998b9c8f5fb8d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_409a5629168649f78d5981b84266a069","max":1340616616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_801f2981c0b34685bb0d70f957ec9b15","value":1340616616}},"120635bb691441ef987312eb166cfa7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e38725c33a0494e9a4228c148c37d0f","placeholder":"​","style":"IPY_MODEL_7f542eb9d7bb42258927c5d4f8c62205","value":" 1.34G/1.34G [00:13&lt;00:00, 146MB/s]"}},"b0d8dc5e1f3c46f5844197fff82f12ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2367bdb1c384985be915ed87669967d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bcb5855ba604431823c4ebd56510ba1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"409a5629168649f78d5981b84266a069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"801f2981c0b34685bb0d70f957ec9b15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e38725c33a0494e9a4228c148c37d0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f542eb9d7bb42258927c5d4f8c62205":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef3474be4aff4966b7d9687125cbe9a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_856144115b894206aacc319bef8adfe9","IPY_MODEL_dba9e1397a2249559824ef4ddc59330b","IPY_MODEL_f651eb710dff49acae05ed3b4a4d451e"],"layout":"IPY_MODEL_a06ec23c749f40f3b42024375d6a8c8a"}},"856144115b894206aacc319bef8adfe9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_569c8a05973847569d42865196b50415","placeholder":"​","style":"IPY_MODEL_841d8e2ddb3e4d5d8b7310b2933dfc3e","value":"tokenizer_config.json: 100%"}},"dba9e1397a2249559824ef4ddc59330b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef1a6a04f63e4d52935af2d89d83c28d","max":366,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd5acce5364f4f79818ab622076db35b","value":366}},"f651eb710dff49acae05ed3b4a4d451e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_072ed81db91a4a46bcdb38ac770c3f6c","placeholder":"​","style":"IPY_MODEL_64348bf2d8b54bd8b6b43dbe0d5a74d4","value":" 366/366 [00:00&lt;00:00, 22.4kB/s]"}},"a06ec23c749f40f3b42024375d6a8c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"569c8a05973847569d42865196b50415":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"841d8e2ddb3e4d5d8b7310b2933dfc3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef1a6a04f63e4d52935af2d89d83c28d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd5acce5364f4f79818ab622076db35b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"072ed81db91a4a46bcdb38ac770c3f6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64348bf2d8b54bd8b6b43dbe0d5a74d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6be3a732a9374f228ae65dc598b35178":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d84c87949edc493bb512da6c01c06fe4","IPY_MODEL_50e2782867e548e89debee2a53df0fea","IPY_MODEL_90bbf2262c28462a974292f963b66202"],"layout":"IPY_MODEL_884f677ecadd4763987b9d2185cb9ddb"}},"d84c87949edc493bb512da6c01c06fe4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0feacd333ccb444994b4f8ec843982f5","placeholder":"​","style":"IPY_MODEL_697a186e2e3d487d9496eaae4ede6921","value":"vocab.txt: 100%"}},"50e2782867e548e89debee2a53df0fea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c56d127dc044f2cb9a78fcb0d974764","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ba2ca6da7494402a0f8adf6c92a6d23","value":231508}},"90bbf2262c28462a974292f963b66202":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_289272d831da43a88809b5fb0d043949","placeholder":"​","style":"IPY_MODEL_ad9a0b185b4e400b92028fd4e81d721f","value":" 232k/232k [00:00&lt;00:00, 8.04MB/s]"}},"884f677ecadd4763987b9d2185cb9ddb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0feacd333ccb444994b4f8ec843982f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"697a186e2e3d487d9496eaae4ede6921":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c56d127dc044f2cb9a78fcb0d974764":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba2ca6da7494402a0f8adf6c92a6d23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"289272d831da43a88809b5fb0d043949":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad9a0b185b4e400b92028fd4e81d721f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7957c5419a8844cba0533dd809ecb540":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ddbea77cd12f4936a495b6468c974ad8","IPY_MODEL_ed9ba31fdf434da898c3d604392335dc","IPY_MODEL_537bd3f74a364379854c6f67b051db85"],"layout":"IPY_MODEL_0736212120634877b559ce692ec916db"}},"ddbea77cd12f4936a495b6468c974ad8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7ec5f2dea6a4384b945faaed42a0c5c","placeholder":"​","style":"IPY_MODEL_6ad59bfb7f004a73bcbffca0f6bacc16","value":"tokenizer.json: 100%"}},"ed9ba31fdf434da898c3d604392335dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da27d2bc72124d4084b50e3b2788b48b","max":711396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3880470ecc09417785703bf60071441b","value":711396}},"537bd3f74a364379854c6f67b051db85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_564e5906d30d48cbb3af8502392ae6c2","placeholder":"​","style":"IPY_MODEL_40c961078fd144229b5a2e2a29de64cc","value":" 711k/711k [00:00&lt;00:00, 17.3MB/s]"}},"0736212120634877b559ce692ec916db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7ec5f2dea6a4384b945faaed42a0c5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ad59bfb7f004a73bcbffca0f6bacc16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da27d2bc72124d4084b50e3b2788b48b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3880470ecc09417785703bf60071441b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"564e5906d30d48cbb3af8502392ae6c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40c961078fd144229b5a2e2a29de64cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dac5e2b0f4a34507a45936bc641b9ba1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59dad249ea89493a821d29e6c3c1cc9d","IPY_MODEL_b6fe2c8235fb40e0a7771a828314ef78","IPY_MODEL_412cfcb8e7684c08a589607856492cd2"],"layout":"IPY_MODEL_06fa8b66009d49579a3a8103ce4cfa7f"}},"59dad249ea89493a821d29e6c3c1cc9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aa2d52ef22a40ec8124080b293f97c5","placeholder":"​","style":"IPY_MODEL_60d4c129877f47d2a6f1355b4d877121","value":"special_tokens_map.json: 100%"}},"b6fe2c8235fb40e0a7771a828314ef78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_186f76ba58b9450d96ef587e95ff11c9","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24d5cd0f9a4b4a83b164a1cf438d10ea","value":125}},"412cfcb8e7684c08a589607856492cd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f80e08154c44276b528e8665108840c","placeholder":"​","style":"IPY_MODEL_e3fecd3dfbb9431f9d99d5d23b8bcbda","value":" 125/125 [00:00&lt;00:00, 3.71kB/s]"}},"06fa8b66009d49579a3a8103ce4cfa7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aa2d52ef22a40ec8124080b293f97c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60d4c129877f47d2a6f1355b4d877121":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"186f76ba58b9450d96ef587e95ff11c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24d5cd0f9a4b4a83b164a1cf438d10ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f80e08154c44276b528e8665108840c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3fecd3dfbb9431f9d99d5d23b8bcbda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8069b56d25be467c89a44de9c1c35489":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4447af7b80884ff884bf7e00bcd334f7","IPY_MODEL_ecce0e898cbc4fde817518ae6d06d307","IPY_MODEL_f7ee2a38bdda426bb4967eb54c18359c"],"layout":"IPY_MODEL_5456f87ff727437e942d7aad6a07d8e0"}},"4447af7b80884ff884bf7e00bcd334f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7405576c1f854ba7b5e150bd393b49fc","placeholder":"​","style":"IPY_MODEL_9250909f1df64cfb9506c519309a7f13","value":"1_Pooling/config.json: 100%"}},"ecce0e898cbc4fde817518ae6d06d307":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98d4ba58225049e395575abfe3e537e3","max":191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36c04320bb084e8d991f377c84a84136","value":191}},"f7ee2a38bdda426bb4967eb54c18359c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fd790d7c69142b2b0f725ecd6dd2fa5","placeholder":"​","style":"IPY_MODEL_34280fb96a84410eaffbf3728fe15d48","value":" 191/191 [00:00&lt;00:00, 5.86kB/s]"}},"5456f87ff727437e942d7aad6a07d8e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7405576c1f854ba7b5e150bd393b49fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9250909f1df64cfb9506c519309a7f13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98d4ba58225049e395575abfe3e537e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36c04320bb084e8d991f377c84a84136":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fd790d7c69142b2b0f725ecd6dd2fa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34280fb96a84410eaffbf3728fe15d48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Notebook Params"],"metadata":{"id":"I-Jj-f0ta61F"}},{"cell_type":"code","source":["re_fetch_flag = False\n","update_chroma_db = False"],"metadata":{"id":"JxstJh5oa9px","executionInfo":{"status":"ok","timestamp":1722402898566,"user_tz":-330,"elapsed":394,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Helper Functions"],"metadata":{"id":"gB139p3_x78c"}},{"cell_type":"code","source":["def pretty_print_docs(docs):\n","    print(\n","        f\"\\n{'-' * 100}\\n\".join(\n","            [\n","                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n","                for i, d in enumerate(docs)\n","            ]\n","        )\n","    )"],"metadata":{"id":"ZpD1JQgAx92R","executionInfo":{"status":"ok","timestamp":1722402899865,"user_tz":-330,"elapsed":603,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPE-pNXCLdFG"},"source":["## Setup"]},{"cell_type":"code","source":["!pip install -U -q langchain langchain-chroma langchain_huggingface langchain_community sentence_transformers openai"],"metadata":{"id":"Ix8Y3aN-wm_6","executionInfo":{"status":"ok","timestamp":1722403010601,"user_tz":-330,"elapsed":110737,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d23e587-b73e-48a8-fa9d-02271ef95519"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.6/377.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1722403010601,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"},"user_tz":-330},"id":"mYU28FWjG35R"},"outputs":[],"source":["# !pip install openai  chromadb faiss-cpu pypdf tiktoken docarray --quiet"]},{"cell_type":"code","source":["# Import the necessary libraies\n","import os\n","import base64\n","import requests\n","from collections import Counter\n","import pickle\n","import openai\n","\n","from langchain_core.documents import Document\n","from langchain.vectorstores import Chroma"],"metadata":{"id":"ItKLMAtIQun-","executionInfo":{"status":"ok","timestamp":1722403011800,"user_tz":-330,"elapsed":1202,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51298,"status":"ok","timestamp":1722403063096,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"},"user_tz":-330},"id":"bLZTSTNMG9xd","outputId":"8afffb8d-cfbf-403f-ef6d-416840e2d985"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"]},{"cell_type":"code","source":["# Set the API key by reading the folder path. Use this code if you're running the code on Google Colab. Otherwise, use the actual folder path\n","folder_path = '/content/drive/MyDrive/upgrad'\n","\n","# Folder path\n","os.chdir(folder_path)"],"metadata":{"id":"QtuNS6jkWldz","executionInfo":{"status":"ok","timestamp":1722403063096,"user_tz":-330,"elapsed":3,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CWtMt1Akb6qy"},"source":["## Set the OpenAI & GitHub Keys"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"xIX5n9S5TmYt","executionInfo":{"status":"ok","timestamp":1722403064687,"user_tz":-330,"elapsed":1594,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"outputs":[],"source":["# Read the text file containing the API key\n","with open(\"api_keys/OpenAI_key.txt\", \"r\") as f:\n","  openai.api_key = ' '.join(f.readlines())\n","\n","# Update the OpenAI API key by updating the environment variable\n","os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n","\n","# Read the text file containing the API key\n","with open(\"api_keys/Github_key.txt\", \"r\") as f:\n","  github_key = ' '.join(f.readlines())\n","\n","# Update the OpenAI API key by updating the environment variable\n","os.environ['GITHUB_PERSONAL_ACCESS_TOKEN'] = github_key"]},{"cell_type":"markdown","source":["## Getting Files from Github"],"metadata":{"id":"T2KT1C3hVoRt"}},{"cell_type":"code","source":["headers = {\n","  \"Accept\": \"application/vnd.github+json\",\n","  \"Authorization\": f\"Bearer {os.environ['GITHUB_PERSONAL_ACCESS_TOKEN']}\",\n","}\n","\n","base_url = (\n","  f\"https://api.github.com/repos/dmlc/xgboost/contents\"\n",")\n","\n","def get_content(files) :\n","  return [requests.get(x['url'], headers=headers).json() for x in files]"],"metadata":{"id":"iHKTcY1lYiHQ","executionInfo":{"status":"ok","timestamp":1722403064688,"user_tz":-330,"elapsed":5,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["response = requests.get(base_url, headers=headers)\n","response.raise_for_status()\n","files = response.json()\n","menu = get_content(files)"],"metadata":{"id":"rvqK378M8REM","executionInfo":{"status":"ok","timestamp":1722403073148,"user_tz":-330,"elapsed":8463,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["final_menu = []\n","\n","def flat_content(menu=menu, debug=False):\n","\n","  for item in menu:\n","\n","    if isinstance(item,list) :\n","      menu = get_content(item)\n","      flat_content(menu)\n","\n","    else :\n","      if debug :\n","        print(item['path'])\n","      final_menu.append(item)"],"metadata":{"id":"eSupGoCH3d78","executionInfo":{"status":"ok","timestamp":1722403073149,"user_tz":-330,"elapsed":13,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["if re_fetch_flag:\n","    flat_content(menu=menu, debug=True)"],"metadata":{"id":"kEgLE_ZWiZs2","executionInfo":{"status":"ok","timestamp":1722403073149,"user_tz":-330,"elapsed":11,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["if re_fetch_flag:\n","\n","    with open(\"Course 5 - SemanticSpotter /Final_Project/all_files.pkl\", \"wb\") as file:\n","        pickle.dump(final_menu, file)\n","else:\n","    with open(\"Course 5 - SemanticSpotter /Final_Project/all_files.pkl\", \"rb\") as file:\n","        final_menu = pickle.load(file)"],"metadata":{"id":"Ps4TG75434IG","executionInfo":{"status":"ok","timestamp":1722403074420,"user_tz":-330,"elapsed":1282,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["final_menu[0], final_menu[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cp7vZHpdV5XD","executionInfo":{"status":"ok","timestamp":1722403074420,"user_tz":-330,"elapsed":8,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"843a4565-6866-4cd2-d82e-1f7ca2b25fc3"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'name': '.clang-format',\n","  'path': '.clang-format',\n","  'sha': '737cf9006baef17d7aa40da120587aeb641e0692',\n","  'size': 5763,\n","  'url': 'https://api.github.com/repos/dmlc/xgboost/contents/.clang-format?ref=master',\n","  'html_url': 'https://github.com/dmlc/xgboost/blob/master/.clang-format',\n","  'git_url': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/737cf9006baef17d7aa40da120587aeb641e0692',\n","  'download_url': 'https://raw.githubusercontent.com/dmlc/xgboost/master/.clang-format',\n","  'type': 'file',\n","  'content': 'LS0tCkxhbmd1YWdlOiAgICAgICAgQ3BwCiMgQmFzZWRPblN0eWxlOiAgR29v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\\nLi4K\\n',\n","  'encoding': 'base64',\n","  '_links': {'self': 'https://api.github.com/repos/dmlc/xgboost/contents/.clang-format?ref=master',\n","   'git': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/737cf9006baef17d7aa40da120587aeb641e0692',\n","   'html': 'https://github.com/dmlc/xgboost/blob/master/.clang-format'}},\n"," {'name': 'utils.py',\n","  'path': 'tests/test_distributed/test_with_spark/utils.py',\n","  'sha': 'adc6b6069ba3cc0142096b99c7380dd4436f8381',\n","  'size': 4058,\n","  'url': 'https://api.github.com/repos/dmlc/xgboost/contents/tests/test_distributed/test_with_spark/utils.py?ref=master',\n","  'html_url': 'https://github.com/dmlc/xgboost/blob/master/tests/test_distributed/test_with_spark/utils.py',\n","  'git_url': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/adc6b6069ba3cc0142096b99c7380dd4436f8381',\n","  'download_url': 'https://raw.githubusercontent.com/dmlc/xgboost/master/tests/test_distributed/test_with_spark/utils.py',\n","  'type': 'file',\n","  'content': 'aW1wb3J0IGNvbnRleHRsaWIKaW1wb3J0IGxvZ2dpbmcKaW1wb3J0IHNodXRp\\nbAppbXBvcnQgc3lzCmltcG9ydCB0ZW1wZmlsZQppbXBvcnQgdW5pdHRlc3QK\\nZnJvbSBpbyBpbXBvcnQgU3RyaW5nSU8KCmltcG9ydCBweXRlc3QKCmZyb20g\\neGdib29zdCBpbXBvcnQgdGVzdGluZyBhcyB0bQoKcHl0ZXN0bWFyayA9IFtw\\neXRlc3QubWFyay5za2lwaWYoKip0bS5ub19zcGFyaygpKV0KCmZyb20gcHlz\\ncGFyay5zcWwgaW1wb3J0IFNwYXJrU2Vzc2lvbgoKZnJvbSB4Z2Jvb3N0LnNw\\nYXJrLnV0aWxzIGltcG9ydCBfZ2V0X2RlZmF1bHRfcGFyYW1zX2Zyb21fZnVu\\nYwoKCmNsYXNzIFV0aWxzVGVzdCh1bml0dGVzdC5UZXN0Q2FzZSk6CiAgICBk\\nZWYgdGVzdF9nZXRfZGVmYXVsdF9wYXJhbXMoc2VsZik6CiAgICAgICAgY2xh\\nc3MgRm9vOgogICAgICAgICAgICBkZWYgZnVuYzEoc2VsZiwgeCwgeSwga2V5\\nMT1Ob25lLCBrZXkyPSJ2YWwyIiwga2V5Mz0wLCBrZXk0PU5vbmUpOgogICAg\\nICAgICAgICAgICAgcGFzcwoKICAgICAgICB1bnN1cHBvcnRlZF9wYXJhbXMg\\nPSB7ImtleTIiLCAia2V5NCJ9CiAgICAgICAgZXhwZWN0ZWRfZGVmYXVsdF9w\\nYXJhbXMgPSB7CiAgICAgICAgICAgICJrZXkxIjogTm9uZSwKICAgICAgICAg\\nICAgImtleTMiOiAwLAogICAgICAgIH0KICAgICAgICBhY3R1YWxfZGVmYXVs\\ndF9wYXJhbXMgPSBfZ2V0X2RlZmF1bHRfcGFyYW1zX2Zyb21fZnVuYygKICAg\\nICAgICAgICAgRm9vLmZ1bmMxLCB1bnN1cHBvcnRlZF9wYXJhbXMKICAgICAg\\nICApCiAgICAgICAgc2VsZi5hc3NlcnRFcXVhbCgKICAgICAgICAgICAgbGVu\\nKGV4cGVjdGVkX2RlZmF1bHRfcGFyYW1zLmtleXMoKSksIGxlbihhY3R1YWxf\\nZGVmYXVsdF9wYXJhbXMua2V5cygpKQogICAgICAgICkKICAgICAgICBmb3Ig\\naywgdiBpbiBhY3R1YWxfZGVmYXVsdF9wYXJhbXMuaXRlbXMoKToKICAgICAg\\nICAgICAgc2VsZi5hc3NlcnRFcXVhbChleHBlY3RlZF9kZWZhdWx0X3BhcmFt\\nc1trXSwgdikKCgpAY29udGV4dGxpYi5jb250ZXh0bWFuYWdlcgpkZWYgcGF0\\nY2hfc3Rkb3V0KCk6CiAgICAiIiJwYXRjaCBzdGRvdXQgYW5kIGdpdmUgYW4g\\nb3V0cHV0IiIiCiAgICBzeXNfc3Rkb3V0ID0gc3lzLnN0ZG91dAogICAgaW9f\\nb3V0ID0gU3RyaW5nSU8oKQogICAgc3lzLnN0ZG91dCA9IGlvX291dAogICAg\\ndHJ5OgogICAgICAgIHlpZWxkIGlvX291dAogICAgZmluYWxseToKICAgICAg\\nICBzeXMuc3Rkb3V0ID0gc3lzX3N0ZG91dAoKCkBjb250ZXh0bGliLmNvbnRl\\neHRtYW5hZ2VyCmRlZiBwYXRjaF9sb2dnZXIobmFtZSk6CiAgICAiIiJwYXRj\\naCBsb2dnZXIgYW5kIGdpdmUgYW4gb3V0cHV0IiIiCiAgICBpb19vdXQgPSBT\\ndHJpbmdJTygpCiAgICBsb2cgPSBsb2dnaW5nLmdldExvZ2dlcihuYW1lKQog\\nICAgaGFuZGxlciA9IGxvZ2dpbmcuU3RyZWFtSGFuZGxlcihpb19vdXQpCiAg\\nICBsb2cuYWRkSGFuZGxlcihoYW5kbGVyKQogICAgdHJ5OgogICAgICAgIHlp\\nZWxkIGlvX291dAogICAgZmluYWxseToKICAgICAgICBsb2cucmVtb3ZlSGFu\\nZGxlcihoYW5kbGVyKQoKCmNsYXNzIFRlc3RUZW1wRGlyKG9iamVjdCk6CiAg\\nICBAY2xhc3NtZXRob2QKICAgIGRlZiBtYWtlX3RlbXBkaXIoY2xzKToKICAg\\nICAgICAiIiIKICAgICAgICA6cGFyYW0gZGlyOiBSb290IGRpcmVjdG9yeSBp\\nbiB3aGljaCB0byBjcmVhdGUgdGhlIHRlbXAgZGlyZWN0b3J5CiAgICAgICAg\\nIiIiCiAgICAgICAgY2xzLnRlbXBkaXIgPSB0ZW1wZmlsZS5ta2R0ZW1wKHBy\\nZWZpeD0ic3BhcmtkbF90ZXN0cyIpCgogICAgQGNsYXNzbWV0aG9kCiAgICBk\\nZWYgcmVtb3ZlX3RlbXBkaXIoY2xzKToKICAgICAgICBzaHV0aWwucm10cmVl\\nKGNscy50ZW1wZGlyKQoKCmNsYXNzIFRlc3RTcGFya0NvbnRleHQob2JqZWN0\\nKToKICAgIEBjbGFzc21ldGhvZAogICAgZGVmIHNldHVwX2VudihjbHMsIHNw\\nYXJrX2NvbmZpZyk6CiAgICAgICAgYnVpbGRlciA9IFNwYXJrU2Vzc2lvbi5i\\ndWlsZGVyLmFwcE5hbWUoInhnYm9vc3Qgc3BhcmsgcHl0aG9uIEFQSSBUZXN0\\ncyIpCiAgICAgICAgZm9yIGssIHYgaW4gc3BhcmtfY29uZmlnLml0ZW1zKCk6\\nCiAgICAgICAgICAgIGJ1aWxkZXIuY29uZmlnKGssIHYpCiAgICAgICAgc3Bh\\ncmsgPSBidWlsZGVyLmdldE9yQ3JlYXRlKCkKICAgICAgICBsb2dnaW5nLmdl\\ndExvZ2dlcigicHlzcGFyayIpLnNldExldmVsKGxvZ2dpbmcuSU5GTykKCiAg\\nICAgICAgY2xzLnNjID0gc3Bhcmsuc3BhcmtDb250ZXh0CiAgICAgICAgY2xz\\nLnNlc3Npb24gPSBzcGFyawoKICAgIEBjbGFzc21ldGhvZAogICAgZGVmIHRl\\nYXJfZG93bl9lbnYoY2xzKToKICAgICAgICBjbHMuc2Vzc2lvbi5zdG9wKCkK\\nICAgICAgICBjbHMuc2Vzc2lvbiA9IE5vbmUKICAgICAgICBjbHMuc2Muc3Rv\\ncCgpCiAgICAgICAgY2xzLnNjID0gTm9uZQoKCmNsYXNzIFNwYXJrVGVzdENh\\nc2UoVGVzdFNwYXJrQ29udGV4dCwgVGVzdFRlbXBEaXIsIHVuaXR0ZXN0LlRl\\nc3RDYXNlKToKICAgIEBjbGFzc21ldGhvZAogICAgZGVmIHNldFVwQ2xhc3Mo\\nY2xzKToKICAgICAgICBjbHMuc2V0dXBfZW52KAogICAgICAgICAgICB7CiAg\\nICAgICAgICAgICAgICAic3BhcmsubWFzdGVyIjogImxvY2FsWzRdIiwKICAg\\nICAgICAgICAgICAgICJzcGFyay5weXRob24ud29ya2VyLnJldXNlIjogImZh\\nbHNlIiwKICAgICAgICAgICAgICAgICJzcGFyay5kcml2ZXIuaG9zdCI6ICIx\\nMjcuMC4wLjEiLAogICAgICAgICAgICAgICAgInNwYXJrLnRhc2subWF4RmFp\\nbHVyZXMiOiAiMSIsCiAgICAgICAgICAgICAgICAic3Bhcmsuc3FsLmV4ZWN1\\ndGlvbi5weXNwYXJrLnVkZi5zaW1wbGlmaWVkVHJhY2ViYWNrLmVuYWJsZWQi\\nOiAiZmFsc2UiLAogICAgICAgICAgICAgICAgInNwYXJrLnNxbC5weXNwYXJr\\nLmp2bVN0YWNrdHJhY2UuZW5hYmxlZCI6ICJ0cnVlIiwKICAgICAgICAgICAg\\nfQogICAgICAgICkKICAgICAgICBjbHMubWFrZV90ZW1wZGlyKCkKCiAgICBA\\nY2xhc3NtZXRob2QKICAgIGRlZiB0ZWFyRG93bkNsYXNzKGNscyk6CiAgICAg\\nICAgY2xzLnJlbW92ZV90ZW1wZGlyKCkKICAgICAgICBjbHMudGVhcl9kb3du\\nX2VudigpCgoKY2xhc3MgU3BhcmtMb2NhbENsdXN0ZXJUZXN0Q2FzZShUZXN0\\nU3BhcmtDb250ZXh0LCBUZXN0VGVtcERpciwgdW5pdHRlc3QuVGVzdENhc2Up\\nOgogICAgQGNsYXNzbWV0aG9kCiAgICBkZWYgc2V0VXBDbGFzcyhjbHMpOgog\\nICAgICAgIGNscy5zZXR1cF9lbnYoCiAgICAgICAgICAgIHsKICAgICAgICAg\\nICAgICAgICJzcGFyay5tYXN0ZXIiOiAibG9jYWwtY2x1c3RlclsyLCAyLCAx\\nMDI0XSIsCiAgICAgICAgICAgICAgICAic3BhcmsucHl0aG9uLndvcmtlci5y\\nZXVzZSI6ICJmYWxzZSIsCiAgICAgICAgICAgICAgICAic3BhcmsuZHJpdmVy\\nLmhvc3QiOiAiMTI3LjAuMC4xIiwKICAgICAgICAgICAgICAgICJzcGFyay50\\nYXNrLm1heEZhaWx1cmVzIjogIjEiLAogICAgICAgICAgICAgICAgInNwYXJr\\nLnNxbC5leGVjdXRpb24ucHlzcGFyay51ZGYuc2ltcGxpZmllZFRyYWNlYmFj\\nay5lbmFibGVkIjogImZhbHNlIiwKICAgICAgICAgICAgICAgICJzcGFyay5z\\ncWwucHlzcGFyay5qdm1TdGFja3RyYWNlLmVuYWJsZWQiOiAidHJ1ZSIsCiAg\\nICAgICAgICAgICAgICAic3BhcmsuY29yZXMubWF4IjogIjQiLAogICAgICAg\\nICAgICAgICAgInNwYXJrLnRhc2suY3B1cyI6ICIxIiwKICAgICAgICAgICAg\\nICAgICJzcGFyay5leGVjdXRvci5jb3JlcyI6ICIyIiwKICAgICAgICAgICAg\\nfQogICAgICAgICkKICAgICAgICBjbHMubWFrZV90ZW1wZGlyKCkKICAgICAg\\nICAjIFdlIHJ1biBhIGR1bW15IGpvYiBzbyB0aGF0IHdlIGJsb2NrIHVudGls\\nIHRoZSB3b3JrZXJzIGhhdmUgY29ubmVjdGVkIHRvIHRoZSBtYXN0ZXIKICAg\\nICAgICBjbHMuc2MucGFyYWxsZWxpemUocmFuZ2UoNCksIDQpLmJhcnJpZXIo\\nKS5tYXBQYXJ0aXRpb25zKGxhbWJkYSBfOiBbXSkuY29sbGVjdCgpCgogICAg\\nQGNsYXNzbWV0aG9kCiAgICBkZWYgdGVhckRvd25DbGFzcyhjbHMpOgogICAg\\nICAgIGNscy5yZW1vdmVfdGVtcGRpcigpCiAgICAgICAgY2xzLnRlYXJfZG93\\nbl9lbnYoKQo=\\n',\n","  'encoding': 'base64',\n","  '_links': {'self': 'https://api.github.com/repos/dmlc/xgboost/contents/tests/test_distributed/test_with_spark/utils.py?ref=master',\n","   'git': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/adc6b6069ba3cc0142096b99c7380dd4436f8381',\n","   'html': 'https://github.com/dmlc/xgboost/blob/master/tests/test_distributed/test_with_spark/utils.py'}})"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["final_menu_copy = final_menu.copy()"],"metadata":{"id":"JrJ2TdWRn_PG","executionInfo":{"status":"ok","timestamp":1722403074421,"user_tz":-330,"elapsed":6,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### Exploring Files"],"metadata":{"id":"sxOkys2Gn3DS"}},{"cell_type":"code","source":["Counter([1 if 'content' in x else 0  for x in final_menu])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERlYcej6C21v","executionInfo":{"status":"ok","timestamp":1722403074421,"user_tz":-330,"elapsed":6,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"fcf009e4-2456-4e8b-a209-336e376b651f"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({1: 1339, 0: 2})"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["len(final_menu)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lzrzw9v4F0La","executionInfo":{"status":"ok","timestamp":1722403074421,"user_tz":-330,"elapsed":5,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"3bfba7ba-9b0d-40fe-ba90-3bf8ed483cd9"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1341"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["[x for x in final_menu if 'content' not in x][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"581L9KxoGKf0","executionInfo":{"status":"ok","timestamp":1722403074421,"user_tz":-330,"elapsed":5,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"5871c9d0-d3fa-4602-83a9-ad22938c17c7"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'name': 'dmlc-core',\n"," 'path': 'dmlc-core',\n"," 'sha': '13341857549852a9a86b1894b5ba84c6276ab381',\n"," 'size': 0,\n"," 'url': 'https://api.github.com/repos/dmlc/xgboost/contents/dmlc-core?ref=master',\n"," 'html_url': 'https://github.com/dmlc/dmlc-core/tree/13341857549852a9a86b1894b5ba84c6276ab381',\n"," 'git_url': 'https://api.github.com/repos/dmlc/dmlc-core/git/trees/13341857549852a9a86b1894b5ba84c6276ab381',\n"," 'download_url': None,\n"," 'type': 'submodule',\n"," 'submodule_git_url': 'https://github.com/dmlc/dmlc-core',\n"," '_links': {'self': 'https://api.github.com/repos/dmlc/xgboost/contents/dmlc-core?ref=master',\n","  'git': 'https://api.github.com/repos/dmlc/dmlc-core/git/trees/13341857549852a9a86b1894b5ba84c6276ab381',\n","  'html': 'https://github.com/dmlc/dmlc-core/tree/13341857549852a9a86b1894b5ba84c6276ab381'}}"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## Filtering Final Menu"],"metadata":{"id":"Ko98V_PzoGJY"}},{"cell_type":"code","source":["final_menu = [x for x in final_menu if 'content' in x]"],"metadata":{"id":"7FaNzjBgGujO","executionInfo":{"status":"ok","timestamp":1722403074421,"user_tz":-330,"elapsed":4,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["final_menu = [\n","    x for x in final_menu if (\n","                              (\n","                                ('.rst' in x['path'])|\n","                                ('doc/' in x['path'])|\n","                                ('demo/' in x['path'])|\n","                                ('tutorials/' in x['path'])\n","                              ) &\n","                              (\n","                                ('data' not in x['path']) |\n","                                ('data' not in x['name'])\n","                              )\n","                             )\n","]"],"metadata":{"id":"wxXFyCVhov_x","executionInfo":{"status":"ok","timestamp":1722403074421,"user_tz":-330,"elapsed":4,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["len(final_menu)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJ01a4Mgpjtr","executionInfo":{"status":"ok","timestamp":1722403074421,"user_tz":-330,"elapsed":3,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"b03fd7b5-6144-4996-9ccc-2d1ca9d939a8"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["221"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["print(*[x['path'] for x in final_menu],sep=\", \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqgDqETSUgRH","executionInfo":{"status":"ok","timestamp":1722403075474,"user_tz":-330,"elapsed":1056,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"9ca32147-e7a6-4b43-8082-9a49a41be40e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["R-package/demo/00Index, R-package/demo/README.md, R-package/demo/basic_walkthrough.R, R-package/demo/boost_from_prediction.R, R-package/demo/create_sparse_matrix.R, R-package/demo/cross_validation.R, R-package/demo/custom_objective.R, R-package/demo/early_stopping.R, R-package/demo/generalized_linear_model.R, R-package/demo/gpu_accelerated.R, R-package/demo/interaction_constraints.R, R-package/demo/poisson_regression.R, R-package/demo/predict_first_ntree.R, R-package/demo/predict_leaf_indices.R, R-package/demo/runall.R, R-package/demo/tweedie_regression.R, demo/.gitignore, demo/CLI/README.rst, demo/CLI/binary_classification/README.md, demo/CLI/binary_classification/agaricus-lepiota.fmap, demo/CLI/binary_classification/agaricus-lepiota.names, demo/CLI/binary_classification/mapfeat.py, demo/CLI/binary_classification/mknfold.py, demo/CLI/binary_classification/mushroom.conf, demo/CLI/binary_classification/runexp.sh, demo/CLI/distributed-training/README.md, demo/CLI/distributed-training/mushroom.aws.conf, demo/CLI/distributed-training/plot_model.ipynb, demo/CLI/distributed-training/run_aws.sh, demo/CLI/regression/README.md, demo/CLI/regression/machine.conf, demo/CLI/regression/machine.names, demo/CLI/regression/mapfeat.py, demo/CLI/regression/mknfold.py, demo/CLI/regression/runexp.sh, demo/CLI/yearpredMSD/README.md, demo/CLI/yearpredMSD/csv2libsvm.py, demo/CLI/yearpredMSD/runexp.sh, demo/CLI/yearpredMSD/yearpredMSD.conf, demo/README.md, demo/aft_survival/README.rst, demo/aft_survival/aft_survival_demo.py, demo/aft_survival/aft_survival_demo_with_optuna.py, demo/aft_survival/aft_survival_viz_demo.py, demo/c-api/.gitignore, demo/c-api/CMakeLists.txt, demo/c-api/basic/CMakeLists.txt, demo/c-api/basic/Makefile, demo/c-api/basic/README.md, demo/c-api/basic/c-api-demo.c, demo/c-api/external-memory/CMakeLists.txt, demo/c-api/external-memory/README.md, demo/c-api/external-memory/external_memory.c, demo/c-api/inference/CMakeLists.txt, demo/c-api/inference/inference.c, demo/dask/README.rst, demo/dask/cpu_survival.py, demo/dask/cpu_training.py, demo/dask/dask_callbacks.py, demo/dask/gpu_training.py, demo/dask/sklearn_cpu_training.py, demo/dask/sklearn_gpu_training.py, demo/data/README.md, demo/data/agaricus.txt.test, demo/data/agaricus.txt.train, demo/data/featmap.txt, demo/data/gen_autoclaims.R, demo/data/veterans_lung_cancer.csv, demo/gpu_acceleration/README.rst, demo/gpu_acceleration/cover_type.py, demo/gpu_acceleration/tree_shap.py, demo/guide-python/README.rst, demo/guide-python/basic_walkthrough.py, demo/guide-python/boost_from_prediction.py, demo/guide-python/callbacks.py, demo/guide-python/cat_in_the_dat.py, demo/guide-python/cat_pipeline.py, demo/guide-python/categorical.py, demo/guide-python/continuation.py, demo/guide-python/cross_validation.py, demo/guide-python/custom_rmsle.py, demo/guide-python/custom_softmax.py, demo/guide-python/evals_result.py, demo/guide-python/external_memory.py, demo/guide-python/feature_weights.py, demo/guide-python/gamma_regression.py, demo/guide-python/generalized_linear_model.py, demo/guide-python/individual_trees.py, demo/guide-python/learning_to_rank.py, demo/guide-python/multioutput_regression.py, demo/guide-python/predict_first_ntree.py, demo/guide-python/predict_leaf_indices.py, demo/guide-python/quantile_regression.py, demo/guide-python/sklearn_evals_result.py, demo/guide-python/sklearn_examples.py, demo/guide-python/sklearn_parallel.py, demo/guide-python/spark_estimator_examples.py, demo/guide-python/update_process.py, demo/json-model/README.md, demo/json-model/json_parser.py, demo/kaggle-higgs/README.md, demo/kaggle-higgs/higgs-cv.py, demo/kaggle-higgs/higgs-numpy.py, demo/kaggle-higgs/higgs-pred.R, demo/kaggle-higgs/higgs-pred.py, demo/kaggle-higgs/higgs-train.R, demo/kaggle-higgs/run.sh, demo/kaggle-higgs/speedtest.R, demo/kaggle-higgs/speedtest.py, demo/kaggle-otto/README.MD, demo/kaggle-otto/otto_train_pred.R, demo/kaggle-otto/understandingXGBoostModel.Rmd, demo/multiclass_classification/README.md, demo/multiclass_classification/runexp.sh, demo/multiclass_classification/train.R, demo/multiclass_classification/train.py, demo/nvflare/.gitignore, demo/nvflare/README.md, demo/nvflare/config/config_fed_client.json, demo/nvflare/config/config_fed_server.json, demo/nvflare/horizontal/README.md, demo/nvflare/horizontal/custom/controller.py, demo/nvflare/horizontal/custom/trainer.py, demo/nvflare/vertical/README.md, demo/nvflare/vertical/custom/controller.py, demo/nvflare/vertical/custom/trainer.py, demo/rank/README.md, demo/rank/mq2008.conf, demo/rank/rank.py, demo/rank/rank_sklearn.py, demo/rank/runexp.sh, demo/rmm_plugin/README.rst, demo/rmm_plugin/rmm_mgpu_with_dask.py, demo/rmm_plugin/rmm_singlegpu.py, doc/.gitignore, doc/Doxyfile.in, doc/Makefile, doc/R-package/.gitignore, doc/R-package/Makefile, doc/R-package/discoverYourData.md, doc/R-package/index.rst, doc/R-package/index_base.rst, doc/R-package/xgboostPresentation.md, doc/README, doc/_static/cn.svg, doc/_static/custom.css, doc/_static/js/auto_module_index.js, doc/_static/us.svg, doc/build.rst, doc/c++.rst, doc/c.rst, doc/changes/index.rst, doc/changes/v2.1.0.rst, doc/cli.rst, doc/conf.py, doc/contrib/ci.rst, doc/contrib/coding_guide.rst, doc/contrib/community.rst, doc/contrib/consistency.rst, doc/contrib/docs.rst, doc/contrib/donate.rst, doc/contrib/featuremap.rst, doc/contrib/git_guide.rst, doc/contrib/index.rst, doc/contrib/python_packaging.rst, doc/contrib/release.rst, doc/contrib/unit_tests.rst, doc/dump.schema, doc/faq.rst, doc/get_started.rst, doc/gpu/index.rst, doc/index.rst, doc/install.rst, doc/julia.rst, doc/jvm/api.rst, doc/jvm/index.rst, doc/jvm/java_intro.rst, doc/jvm/javadocs/index.rst, doc/jvm/scaladocs/xgboost4j-flink/index.rst, doc/jvm/scaladocs/xgboost4j-spark/index.rst, doc/jvm/scaladocs/xgboost4j/index.rst, doc/jvm/xgboost4j_spark_gpu_tutorial.rst, doc/jvm/xgboost4j_spark_tutorial.rst, doc/model.schema, doc/parameter.rst, doc/prediction.rst, doc/python/.gitignore, doc/python/callbacks.rst, doc/python/index.rst, doc/python/model.rst, doc/python/python_api.rst, doc/python/python_intro.rst, doc/python/sklearn_estimator.rst, doc/requirements.txt, doc/sphinx_util.py, doc/treemethod.rst, doc/tutorials/advanced_custom_obj.rst, doc/tutorials/aft_survival_analysis.rst, doc/tutorials/c_api_tutorial.rst, doc/tutorials/categorical.rst, doc/tutorials/custom_metric_obj.rst, doc/tutorials/dart.rst, doc/tutorials/dask.rst, doc/tutorials/external_memory.rst, doc/tutorials/feature_interaction_constraint.rst, doc/tutorials/index.rst, doc/tutorials/input_format.rst, doc/tutorials/intercept.rst, doc/tutorials/kubernetes.rst, doc/tutorials/learning_to_rank.rst, doc/tutorials/model.rst, doc/tutorials/monotonic.rst, doc/tutorials/multioutput.rst, doc/tutorials/param_tuning.rst, doc/tutorials/privacy_preserving.rst, doc/tutorials/ray.rst, doc/tutorials/rf.rst, doc/tutorials/saving_model.rst, doc/tutorials/spark_estimator.rst, doc/xgboost_doc.yml, python-package/README.rst\n"]}]},{"cell_type":"code","source":["Counter([x['type'] for x in final_menu ])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6M2z_ZQpaui","executionInfo":{"status":"ok","timestamp":1722403075474,"user_tz":-330,"elapsed":47,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"a5ed6b58-34ba-442a-9396-65f3d74a685d"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'file': 221})"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["final_menu[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iE-4ghZ9oOCk","executionInfo":{"status":"ok","timestamp":1722403075474,"user_tz":-330,"elapsed":46,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"e68cebb6-cd99-42d0-9eb6-a5fd89d94375"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'name': '00Index',\n"," 'path': 'R-package/demo/00Index',\n"," 'sha': 'fa09fa900486de72148e229d75afbdd4ba40768b',\n"," 'size': 809,\n"," 'url': 'https://api.github.com/repos/dmlc/xgboost/contents/R-package/demo/00Index?ref=master',\n"," 'html_url': 'https://github.com/dmlc/xgboost/blob/master/R-package/demo/00Index',\n"," 'git_url': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/fa09fa900486de72148e229d75afbdd4ba40768b',\n"," 'download_url': 'https://raw.githubusercontent.com/dmlc/xgboost/master/R-package/demo/00Index',\n"," 'type': 'file',\n"," 'content': 'YmFzaWNfd2Fsa3Rocm91Z2ggICAgICAgICAgICAgICBCYXNpYyBmZWF0dXJl\\nIHdhbGt0aHJvdWdoCmN1c3RvbV9vYmplY3RpdmUgICAgICAgICAgICAgICAg\\nQ3VzdG9taXplIGxvc3MgZnVuY3Rpb24sIGFuZCBldmFsdWF0aW9uIG1ldHJp\\nYwpib29zdF9mcm9tX3ByZWRpY3Rpb24gICAgICAgICAgIEJvb3N0aW5nIGZy\\nb20gZXhpc3RpbmcgcHJlZGljdGlvbgpwcmVkaWN0X2ZpcnN0X250cmVlICAg\\nICAgICAgICAgIFByZWRpY3RpbmcgdXNpbmcgZmlyc3QgbiB0cmVlcwpnZW5l\\ncmFsaXplZF9saW5lYXJfbW9kZWwgICAgICAgIEdlbmVyYWxpemVkIExpbmVh\\nciBNb2RlbApjcm9zc192YWxpZGF0aW9uICAgICAgICAgICAgICAgIENyb3Nz\\nIHZhbGlkYXRpb24KY3JlYXRlX3NwYXJzZV9tYXRyaXggICAgICAgICAgICBD\\ncmVhdGUgU3BhcnNlIE1hdHJpeApwcmVkaWN0X2xlYWZfaW5kaWNlcyAgICAg\\nICAgICAgIFByZWRpY3RpbmcgdGhlIGNvcnJlc3BvbmRpbmcgbGVhdmVzCmVh\\ncmx5X3N0b3BwaW5nICAgICAgICAgICAgICAgICAgRWFybHkgU3RvcCBpbiB0\\ncmFpbmluZwpwb2lzc29uX3JlZ3Jlc3Npb24gICAgICAgICAgICAgIFBvaXNz\\nb24gcmVncmVzc2lvbiBvbiBjb3VudCBkYXRhCnR3ZWVkaWVfcmVncmVzc2lv\\nbiAgICAgICAgICAgICAgVHdlZWRpZSByZWdyZXNzaW9uCmdwdV9hY2NlbGVy\\nYXRlZCAgICAgICAgICAgICAgICAgR1BVLWFjY2VsZXJhdGVkIHRyZWUgYnVp\\nbGRpbmcgYWxnb3JpdGhtcwppbnRlcmFjdGlvbl9jb25zdHJhaW50cyAgICAg\\nICAgIEludGVyYWN0aW9uIGNvbnN0cmFpbnRzIGFtb25nIGZlYXR1cmVzCgo=\\n',\n"," 'encoding': 'base64',\n"," '_links': {'self': 'https://api.github.com/repos/dmlc/xgboost/contents/R-package/demo/00Index?ref=master',\n","  'git': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/fa09fa900486de72148e229d75afbdd4ba40768b',\n","  'html': 'https://github.com/dmlc/xgboost/blob/master/R-package/demo/00Index'}}"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## Chunking of Documents"],"metadata":{"id":"bGCN4v4Zg7BF"}},{"cell_type":"code","source":["def get_file_content(content_encoded) :\n","  return base64.b64decode(content_encoded).decode(\"utf-8\")\n","\n","import re\n","\n","def parse_content(text, debug=False):\n","\n","    # Regular expressions for the headings\n","    heading_patterns = [\n","        r\"(?P<heading0>^[A-Za-z0-9 \\-]+)\\n(?P<underline0>\\#+)$\",\n","        r\"(?P<heading1>^[A-Za-z0-9 \\-]+)\\n(?P<underline1>=+)$\",\n","        r\"(?P<heading2A>^[A-Za-z0-9 \\-]+)\\n(?P<underline2>-+)$\",\n","        r\"(?P<heading2B>^[A-Za-z0-9 \\-]+)\\n(?P<underline3>\\*+)$\",\n","        r\"(?P<heading3>\\* ``[A-Za-z0-9_\\-]+``)\" #experimental\n","    ]\n","\n","    # Combine all patterns into one\n","    combined_pattern = \"|\".join(heading_patterns)\n","\n","\n","    matches = re.finditer(combined_pattern, text, re.MULTILINE)\n","\n","    headings = []\n","    last_index = 0\n","\n","    for match in matches:\n","        if debug :\n","            print(match)\n","\n","        start, end = match.span()\n","\n","        if last_index < start:\n","            content = text[last_index:start].strip()\n","            if all([content, headings]):\n","                headings[-1]['content'].append(content)\n","\n","        if match.group('heading0'):\n","            headings.append({'heading': match.group('heading0').strip(), 'level': 0, 'content': []})\n","        elif match.group('heading1'):\n","            headings.append({'heading': match.group('heading1').strip(), 'level': 1, 'content': []})\n","        elif match.group('heading2A'):\n","            headings.append({'heading': match.group('heading2A').strip(), 'level': 2, 'content': []})\n","        elif match.group('heading2B'):\n","            headings.append({'heading': match.group('heading2B').strip(), 'level': 2, 'content': []})\n","        elif match.group('heading3'):\n","            headings.append({'heading': match.group('heading3').strip(), 'level': 3, 'content': []})\n","\n","        last_index = end\n","\n","    # Capture the remaining content after the last match\n","    if last_index < len(text):\n","        content = text[last_index:].strip()\n","        if all([content, headings]):\n","            headings[-1]['content'].append(content)\n","        else :\n","            headings.append({'heading':'' , 'level': 0, 'content': content})\n","\n","\n","    return headings"],"metadata":{"id":"wsTqGHYUH1lF","executionInfo":{"status":"ok","timestamp":1722403075474,"user_tz":-330,"elapsed":44,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["param_page = [x for x in final_menu if x['path'] == 'doc/parameter.rst'][0]\n","param_page"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mE1_EHc-Sf2","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":45,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"f4fa00a5-eb8c-4a18-fdfe-c9df71a3303e"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'name': 'parameter.rst',\n"," 'path': 'doc/parameter.rst',\n"," 'sha': 'a776559223f4ab9f1684e872bbf9bd335389d821',\n"," 'size': 34032,\n"," 'url': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master',\n"," 'html_url': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst',\n"," 'git_url': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/a776559223f4ab9f1684e872bbf9bd335389d821',\n"," 'download_url': 'https://raw.githubusercontent.com/dmlc/xgboost/master/doc/parameter.rst',\n"," 'type': 'file',\n"," 'content': 'IyMjIyMjIyMjIyMjIyMjIyMjClhHQm9vc3QgUGFyYW1ldGVycwojIyMjIyMj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\\ncHJvYmFiaWxpdHkK\\n',\n"," 'encoding': 'base64',\n"," '_links': {'self': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master',\n","  'git': 'https://api.github.com/repos/dmlc/xgboost/git/blobs/a776559223f4ab9f1684e872bbf9bd335389d821',\n","  'html': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst'}}"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# print(get_file_content(param_page['content']))"],"metadata":{"collapsed":true,"id":"LOG4MkddvgKU","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":41,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["[(x['heading'],x['level']) for x in parse_content(get_file_content(param_page['content']))]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49vdpi3t_RjD","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":41,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"2953df98-b667-45b4-83b8-dc5dd0a50f2a"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('XGBoost Parameters', 0),\n"," ('Global Configuration', 2),\n"," ('* ``verbosity``', 3),\n"," ('* ``use_rmm``', 3),\n"," ('General Parameters', 2),\n"," ('* ``booster``', 3),\n"," ('* ``device``', 3),\n"," ('* ``verbosity``', 3),\n"," ('* ``validate_parameters``', 3),\n"," ('* ``nthread``', 3),\n"," ('* ``disable_default_eval_metric``', 3),\n"," ('Parameters for Tree Booster', 1),\n"," ('* ``eta``', 3),\n"," ('* ``gamma``', 3),\n"," ('* ``max_depth``', 3),\n"," ('* ``min_child_weight``', 3),\n"," ('* ``max_delta_step``', 3),\n"," ('* ``subsample``', 3),\n"," ('* ``sampling_method``', 3),\n"," ('* ``colsample_bytree``', 3),\n"," ('* ``lambda``', 3),\n"," ('* ``alpha``', 3),\n"," ('* ``tree_method``', 3),\n"," ('* ``scale_pos_weight``', 3),\n"," ('* ``updater``', 3),\n"," ('* ``refresh_leaf``', 3),\n"," ('* ``process_type``', 3),\n"," ('* ``grow_policy``', 3),\n"," ('* ``max_leaves``', 3),\n"," ('* ``max_bin``', 3),\n"," ('* ``num_parallel_tree``', 3),\n"," ('* ``monotone_constraints``', 3),\n"," ('* ``interaction_constraints``', 3),\n"," ('* ``multi_strategy``', 3),\n"," ('* ``max_cached_hist_node``', 3),\n"," ('Parameters for Categorical Feature', 1),\n"," ('* ``max_cat_to_onehot``', 3),\n"," ('* ``max_cat_threshold``', 3),\n"," ('* ``sample_type``', 3),\n"," ('* ``normalize_type``', 3),\n"," ('* ``rate_drop``', 3),\n"," ('* ``one_drop``', 3),\n"," ('* ``skip_drop``', 3),\n"," ('* ``lambda``', 3),\n"," ('* ``alpha``', 3),\n"," ('* ``updater``', 3),\n"," ('* ``feature_selector``', 3),\n"," ('* ``cyclic``', 3),\n"," ('* ``shuffle``', 3),\n"," ('* ``random``', 3),\n"," ('* ``greedy``', 3),\n"," ('* ``thrifty``', 3),\n"," ('* ``top_k``', 3),\n"," ('Learning Task Parameters', 2),\n"," ('* ``objective``', 3),\n"," ('* ``base_score``', 3),\n"," ('* ``eval_metric``', 3),\n"," ('* ``seed``', 3),\n"," ('* ``seed_per_iteration``', 3),\n"," ('* ``tweedie_variance_power``', 3),\n"," ('* ``huber_slope``', 3),\n"," ('* ``quantile_alpha``', 3),\n"," ('* ``aft_loss_distribution``', 3),\n"," ('* ``lambdarank_pair_method``', 3),\n"," ('* ``lambdarank_num_pair_per_sample``', 3),\n"," ('* ``lambdarank_normalization``', 3),\n"," ('* ``lambdarank_bias_norm``', 3),\n"," ('* ``ndcg_exp_gain``', 3),\n"," ('Command Line Parameters', 2),\n"," ('* ``num_round``', 3),\n"," ('* ``data``', 3),\n"," ('* ``save_period``', 3),\n"," ('* ``task``', 3),\n"," ('* ``model_in``', 3),\n"," ('* ``model_out``', 3),\n"," ('* ``model_dir``', 3),\n"," ('* ``fmap``', 3),\n"," ('* ``dump_format``', 3),\n"," ('* ``name_dump``', 3),\n"," ('* ``name_pred``', 3),\n"," ('* ``pred_margin``', 3)]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["get_file_content(param_page['content'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"7VxsAuwkDxpN","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":32,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"a5709e0c-c317-45d7-d8b0-989d64a0f1f9"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'##################\\nXGBoost Parameters\\n##################\\nBefore running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************\\nGlobal Configuration\\n********************\\nThe following parameters can be set in the global scope, using :py:func:`xgboost.config_context()` (Python) or ``xgb.set.config()`` (R).\\n\\n* ``verbosity``: Verbosity of printing messages. Valid values of 0 (silent), 1 (warning), 2 (info), and 3 (debug).\\n\\n* ``use_rmm``: Whether to use RAPIDS Memory Manager (RMM) to allocate cache GPU\\n  memory. The primary memory is always allocated on the RMM pool when XGBoost is built\\n  (compiled) with the RMM plugin enabled. Valid values are ``true`` and ``false``. See\\n  :doc:`/python/rmm-examples/index` for details.\\n\\n******************\\nGeneral Parameters\\n******************\\n* ``booster`` [default= ``gbtree`` ]\\n\\n  - Which booster to use. Can be ``gbtree``, ``gblinear`` or ``dart``; ``gbtree`` and ``dart`` use tree based models while ``gblinear`` uses linear functions.\\n\\n* ``device`` [default= ``cpu``]\\n\\n  .. versionadded:: 2.0.0\\n\\n  - Device for XGBoost to run. User can set it to one of the following values:\\n\\n    + ``cpu``: Use CPU.\\n    + ``cuda``: Use a GPU (CUDA device).\\n    + ``cuda:<ordinal>``: ``<ordinal>`` is an integer that specifies the ordinal of the GPU (which GPU do you want to use if you have more than one devices).\\n    + ``gpu``: Default GPU device selection from the list of available and supported devices. Only ``cuda`` devices are supported currently.\\n    + ``gpu:<ordinal>``: Default GPU device selection from the list of available and supported devices. Only ``cuda`` devices are supported currently.\\n\\n    For more information about GPU acceleration, see :doc:`/gpu/index`. In distributed environments, ordinal selection is handled by distributed frameworks instead of XGBoost. As a result, using ``cuda:<ordinal>`` will result in an error. Use ``cuda`` instead.\\n\\n* ``verbosity`` [default=1]\\n\\n  - Verbosity of printing messages.  Valid values are 0 (silent), 1 (warning), 2 (info), 3\\n    (debug).  Sometimes XGBoost tries to change configurations based on heuristics, which\\n    is displayed as warning message.  If there\\'s unexpected behaviour, please try to\\n    increase value of verbosity.\\n\\n* ``validate_parameters`` [default to ``false``, except for Python, R and CLI interface]\\n\\n  - When set to True, XGBoost will perform validation of input parameters to check whether\\n    a parameter is used or not. A warning is emitted when there\\'s unknown parameter.\\n\\n* ``nthread`` [default to maximum number of threads available if not set]\\n\\n  - Number of parallel threads used to run XGBoost.  When choosing it, please keep thread\\n    contention and hyperthreading in mind.\\n\\n* ``disable_default_eval_metric`` [default= ``false``]\\n\\n  - Flag to disable default metric. Set to 1 or ``true`` to disable.\\n\\nParameters for Tree Booster\\n===========================\\n* ``eta`` [default=0.3, alias: ``learning_rate``]\\n\\n  - Step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new features, and ``eta`` shrinks the feature weights to make the boosting process more conservative.\\n  - range: [0,1]\\n\\n* ``gamma`` [default=0, alias: ``min_split_loss``]\\n\\n  - Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger ``gamma`` is, the more conservative the algorithm will be. Note that a tree where no splits were made might still contain a single terminal node with a non-zero score.\\n  - range: [0,∞]\\n\\n* ``max_depth`` [default=6]\\n\\n  - Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. ``exact`` tree method requires non-zero value.\\n  - range: [0,∞]\\n\\n* ``min_child_weight`` [default=1]\\n\\n  - Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than ``min_child_weight``, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger ``min_child_weight`` is, the more conservative the algorithm will be.\\n  - range: [0,∞]\\n\\n* ``max_delta_step`` [default=0]\\n\\n  - Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.\\n  - range: [0,∞]\\n\\n* ``subsample`` [default=1]\\n\\n  - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\\n  - range: (0,1]\\n\\n* ``sampling_method`` [default= ``uniform``]\\n\\n  - The method to use to sample the training instances.\\n  - ``uniform``: each training instance has an equal probability of being selected. Typically set\\n    ``subsample`` >= 0.5 for good results.\\n  - ``gradient_based``: the selection probability for each training instance is proportional to the\\n    *regularized absolute value* of gradients (more specifically, :math:`\\\\sqrt{g^2+\\\\lambda h^2}`).\\n    ``subsample`` may be set to as low as 0.1 without loss of model accuracy. Note that this\\n    sampling method is only supported when ``tree_method`` is set to ``hist`` and the device is ``cuda``; other tree\\n    methods only support ``uniform`` sampling.\\n\\n* ``colsample_bytree``, ``colsample_bylevel``, ``colsample_bynode`` [default=1]\\n\\n  - This is a family of parameters for subsampling of columns.\\n  - All ``colsample_by*`` parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\\n  - ``colsample_bytree`` is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\\n  - ``colsample_bylevel`` is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\\n  - ``colsample_bynode`` is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level. This is not supported by the exact tree method.\\n  - ``colsample_by*`` parameters work cumulatively. For instance,\\n    the combination ``{\\'colsample_bytree\\':0.5, \\'colsample_bylevel\\':0.5,\\n    \\'colsample_bynode\\':0.5}`` with 64 features will leave 8 features to choose from at\\n    each split.\\n\\n    Using the Python or the R package, one can set the ``feature_weights`` for DMatrix to\\n    define the probability of each feature being selected when using column sampling.\\n    There\\'s a similar parameter for ``fit`` method in sklearn interface.\\n\\n* ``lambda`` [default=1, alias: ``reg_lambda``]\\n\\n  - L2 regularization term on weights. Increasing this value will make model more conservative.\\n  - range: [0, :math:`\\\\infty`]\\n\\n* ``alpha`` [default=0, alias: ``reg_alpha``]\\n\\n  - L1 regularization term on weights. Increasing this value will make model more conservative.\\n  - range: [0, :math:`\\\\infty`]\\n\\n* ``tree_method`` string [default= ``auto``]\\n\\n  - The tree construction algorithm used in XGBoost. See description in the `reference paper <http://arxiv.org/abs/1603.02754>`_ and :doc:`treemethod`.\\n\\n  - Choices: ``auto``, ``exact``, ``approx``, ``hist``, this is a combination of commonly\\n    used updaters.  For other updaters like ``refresh``, set the parameter ``updater``\\n    directly.\\n\\n    - ``auto``: Same as the ``hist`` tree method.\\n    - ``exact``: Exact greedy algorithm.  Enumerates all split candidates.\\n    - ``approx``: Approximate greedy algorithm using quantile sketch and gradient histogram.\\n    - ``hist``: Faster histogram optimized approximate greedy algorithm.\\n\\n* ``scale_pos_weight`` [default=1]\\n\\n  - Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: ``sum(negative instances) / sum(positive instances)``. See :doc:`Parameters Tuning </tutorials/param_tuning>` for more discussion. Also, see Higgs Kaggle competition demo for examples: `R <https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-train.R>`_, `py1 <https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-numpy.py>`_, `py2 <https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-cv.py>`_, `py3 <https://github.com/dmlc/xgboost/blob/master/demo/guide-python/cross_validation.py>`_.\\n\\n* ``updater``\\n\\n  - A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters. However, it could be also set explicitly by a user. The following updaters exist:\\n\\n    - ``grow_colmaker``: non-distributed column-based construction of trees.\\n    - ``grow_histmaker``: distributed tree construction with row-based data splitting based on global proposal of histogram counting.\\n    - ``grow_quantile_histmaker``: Grow tree using quantized histogram.\\n    - ``grow_gpu_hist``:  Enabled when ``tree_method`` is set to ``hist`` along with ``device=cuda``.\\n    - ``grow_gpu_approx``: Enabled when ``tree_method`` is set to ``approx`` along with ``device=cuda``.\\n    - ``sync``: synchronizes trees in all distributed nodes.\\n    - ``refresh``: refreshes tree\\'s statistics and/or leaf values based on the current data. Note that no random subsampling of data rows is performed.\\n    - ``prune``: prunes the splits where loss < min_split_loss (or gamma) and nodes that have depth greater than ``max_depth``.\\n\\n* ``refresh_leaf`` [default=1]\\n\\n  - This is a parameter of the ``refresh`` updater. When this flag is 1, tree leafs as well as tree nodes\\' stats are updated. When it is 0, only node stats are updated.\\n\\n* ``process_type`` [default= ``default``]\\n\\n  - A type of boosting process to run.\\n  - Choices: ``default``, ``update``\\n\\n    - ``default``: The normal boosting process which creates new trees.\\n    - ``update``: Starts from an existing model and only updates its trees. In each boosting iteration, a tree from the initial model is taken, a specified sequence of updaters is run for that tree, and a modified tree is added to the new model. The new model would have either the same or smaller number of trees, depending on the number of boosting iterations performed. Currently, the following built-in updaters could be meaningfully used with this process type: ``refresh``, ``prune``. With ``process_type=update``, one cannot use updaters that create new trees.\\n\\n* ``grow_policy`` [default= ``depthwise``]\\n\\n  - Controls a way new nodes are added to the tree.\\n  - Currently supported only if ``tree_method`` is set to ``hist`` or ``approx``.\\n  - Choices: ``depthwise``, ``lossguide``\\n\\n    - ``depthwise``: split at nodes closest to the root.\\n    - ``lossguide``: split at nodes with highest loss change.\\n\\n* ``max_leaves`` [default=0]\\n\\n  - Maximum number of nodes to be added.  Not used by ``exact`` tree method.\\n\\n* ``max_bin``, [default=256]\\n\\n  - Only used if ``tree_method`` is set to ``hist`` or ``approx``.\\n  - Maximum number of discrete bins to bucket continuous features.\\n  - Increasing this number improves the optimality of splits at the cost of higher computation time.\\n\\n* ``num_parallel_tree``, [default=1]\\n\\n  - Number of parallel trees constructed during each iteration. This option is used to support boosted random forest.\\n\\n* ``monotone_constraints``\\n\\n  - Constraint of variable monotonicity.  See :doc:`/tutorials/monotonic` for more information.\\n\\n* ``interaction_constraints``\\n\\n  - Constraints for interaction representing permitted interactions.  The constraints must\\n    be specified in the form of a nest list, e.g. ``[[0, 1], [2, 3, 4]]``, where each inner\\n    list is a group of indices of features that are allowed to interact with each other.\\n    See :doc:`/tutorials/feature_interaction_constraint` for more information.\\n\\n* ``multi_strategy``, [default = ``one_output_per_tree``]\\n\\n  .. versionadded:: 2.0.0\\n\\n  .. note:: This parameter is working-in-progress.\\n\\n  - The strategy used for training multi-target models, including multi-target regression\\n    and multi-class classification. See :doc:`/tutorials/multioutput` for more information.\\n\\n    - ``one_output_per_tree``: One model for each target.\\n    - ``multi_output_tree``:  Use multi-target trees.\\n\\n* ``max_cached_hist_node``, [default = 65536]\\n\\n  Maximum number of cached nodes for CPU histogram.\\n\\n  .. versionadded:: 2.0.0\\n\\n  - For most of the cases this parameter should not be set except for growing deep trees\\n    on CPU.\\n\\n.. _cat-param:\\n\\nParameters for Categorical Feature\\n==================================\\n\\nThese parameters are only used for training with categorical data. See\\n:doc:`/tutorials/categorical` for more information.\\n\\n.. note:: These parameters are experimental. ``exact`` tree method is not yet supported.\\n\\n\\n* ``max_cat_to_onehot``\\n\\n  .. versionadded:: 1.6.0\\n\\n  - A threshold for deciding whether XGBoost should use one-hot encoding based split for\\n    categorical data.  When number of categories is lesser than the threshold then one-hot\\n    encoding is chosen, otherwise the categories will be partitioned into children nodes.\\n\\n* ``max_cat_threshold``\\n\\n  .. versionadded:: 1.7.0\\n\\n  - Maximum number of categories considered for each split. Used only by partition-based\\n    splits for preventing over-fitting.\\n\\nAdditional parameters for Dart Booster (``booster=dart``)\\n=========================================================\\n\\n.. note:: Using ``predict()`` with DART booster\\n\\n  If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n  some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n  not the training data. To obtain correct results on test sets, set ``iteration_range`` to\\n  a nonzero value, e.g.\\n\\n  .. code-block:: python\\n\\n    preds = bst.predict(dtest, iteration_range=(0, num_round))\\n\\n* ``sample_type`` [default= ``uniform``]\\n\\n  - Type of sampling algorithm.\\n\\n    - ``uniform``: dropped trees are selected uniformly.\\n    - ``weighted``: dropped trees are selected in proportion to weight.\\n\\n* ``normalize_type`` [default= ``tree``]\\n\\n  - Type of normalization algorithm.\\n\\n    - ``tree``: new trees have the same weight of each of dropped trees.\\n\\n      - Weight of new trees are ``1 / (k + learning_rate)``.\\n      - Dropped trees are scaled by a factor of ``k / (k + learning_rate)``.\\n\\n    - ``forest``: new trees have the same weight of sum of dropped trees (forest).\\n\\n      - Weight of new trees are ``1 / (1 + learning_rate)``.\\n      - Dropped trees are scaled by a factor of ``1 / (1 + learning_rate)``.\\n\\n* ``rate_drop`` [default=0.0]\\n\\n  - Dropout rate (a fraction of previous trees to drop during the dropout).\\n  - range: [0.0, 1.0]\\n\\n* ``one_drop`` [default=0]\\n\\n  - When this flag is enabled, at least one tree is always dropped during the dropout (allows Binomial-plus-one or epsilon-dropout from the original DART paper).\\n\\n* ``skip_drop`` [default=0.0]\\n\\n  - Probability of skipping the dropout procedure during a boosting iteration.\\n\\n    - If a dropout is skipped, new trees are added in the same manner as ``gbtree``.\\n    - Note that non-zero ``skip_drop`` has higher priority than ``rate_drop`` or ``one_drop``.\\n\\n  - range: [0.0, 1.0]\\n\\nParameters for Linear Booster (``booster=gblinear``)\\n====================================================\\n* ``lambda`` [default=0, alias: ``reg_lambda``]\\n\\n  - L2 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples.\\n\\n* ``alpha`` [default=0, alias: ``reg_alpha``]\\n\\n  - L1 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples.\\n\\n* ``updater`` [default= ``shotgun``]\\n\\n  - Choice of algorithm to fit linear model\\n\\n    - ``shotgun``: Parallel coordinate descent algorithm based on shotgun algorithm. Uses \\'hogwild\\' parallelism and therefore produces a nondeterministic solution on each run.\\n    - ``coord_descent``: Ordinary coordinate descent algorithm. Also multithreaded but still produces a deterministic solution. When the ``device`` parameter is set to ``cuda`` or ``gpu``, a GPU variant would be used.\\n\\n* ``feature_selector`` [default= ``cyclic``]\\n\\n  - Feature selection and ordering method\\n\\n    * ``cyclic``: Deterministic selection by cycling through features one at a time.\\n    * ``shuffle``: Similar to ``cyclic`` but with random feature shuffling prior to each update.\\n    * ``random``: A random (with replacement) coordinate selector.\\n    * ``greedy``: Select coordinate with the greatest gradient magnitude.  It has ``O(num_feature^2)`` complexity. It is fully deterministic. It allows restricting the selection to ``top_k`` features per group with the largest magnitude of univariate weight change, by setting the ``top_k`` parameter. Doing so would reduce the complexity to ``O(num_feature*top_k)``.\\n    * ``thrifty``: Thrifty, approximately-greedy feature selector. Prior to cyclic updates, reorders features in descending magnitude of their univariate weight changes. This operation is multithreaded and is a linear complexity approximation of the quadratic greedy selection. It allows restricting the selection to ``top_k`` features per group with the largest magnitude of univariate weight change, by setting the ``top_k`` parameter.\\n\\n* ``top_k`` [default=0]\\n\\n  - The number of top features to select in ``greedy`` and ``thrifty`` feature selector. The value of 0 means using all the features.\\n\\n************************\\nLearning Task Parameters\\n************************\\nSpecify the learning task and the corresponding learning objective. The objective options are below:\\n\\n* ``objective`` [default=reg:squarederror]\\n\\n  - ``reg:squarederror``: regression with squared loss.\\n  - ``reg:squaredlogerror``: regression with squared log loss :math:`\\\\frac{1}{2}[log(pred + 1) - log(label + 1)]^2`.  All input labels are required to be greater than -1.  Also, see metric ``rmsle`` for possible issue  with this objective.\\n  - ``reg:logistic``: logistic regression, output probability\\n  - ``reg:pseudohubererror``: regression with Pseudo Huber loss, a twice differentiable alternative to absolute loss.\\n  - ``reg:absoluteerror``: Regression with L1 error. When tree model is used, leaf value is refreshed after tree construction. If used in distributed training, the leaf value is calculated as the mean value from all workers, which is not guaranteed to be optimal.\\n\\n    .. versionadded:: 1.7.0\\n\\n  - ``reg:quantileerror``: Quantile loss, also known as ``pinball loss``. See later sections for its parameter and :ref:`sphx_glr_python_examples_quantile_regression.py` for a worked example.\\n\\n    .. versionadded:: 2.0.0\\n\\n  - ``binary:logistic``: logistic regression for binary classification, output probability\\n  - ``binary:logitraw``: logistic regression for binary classification, output score before logistic transformation\\n  - ``binary:hinge``: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\\n  - ``count:poisson``: Poisson regression for count data, output mean of Poisson distribution.\\n\\n    + ``max_delta_step`` is set to 0.7 by default in Poisson regression (used to safeguard optimization)\\n\\n  - ``survival:cox``: Cox regression for right censored survival time data (negative values are considered right censored).\\n    Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function ``h(t) = h0(t) * HR``).\\n  - ``survival:aft``: Accelerated failure time model for censored survival time data.\\n    See :doc:`/tutorials/aft_survival_analysis` for details.\\n  - ``multi:softmax``: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\\n  - ``multi:softprob``: same as softmax, but output a vector of ``ndata * nclass``, which can be further reshaped to ``ndata * nclass`` matrix. The result contains predicted probability of each data point belonging to each class.\\n  - ``rank:ndcg``: Use LambdaMART to perform pair-wise ranking where `Normalized Discounted Cumulative Gain (NDCG) <http://en.wikipedia.org/wiki/NDCG>`_ is maximized. This objective supports position debiasing for click data.\\n  - ``rank:map``: Use LambdaMART to perform pair-wise ranking where `Mean Average Precision (MAP) <http://en.wikipedia.org/wiki/Mean_average_precision#Mean_average_precision>`_ is maximized\\n  - ``rank:pairwise``: Use LambdaRank to perform pair-wise ranking using the `ranknet` objective.\\n  - ``reg:gamma``: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be `gamma-distributed <https://en.wikipedia.org/wiki/Gamma_distribution#Occurrence_and_applications>`_.\\n  - ``reg:tweedie``: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be `Tweedie-distributed <https://en.wikipedia.org/wiki/Tweedie_distribution#Occurrence_and_applications>`_.\\n\\n* ``base_score``\\n\\n  - The initial prediction score of all instances, global bias\\n  - The parameter is automatically estimated for selected objectives before training. To\\n    disable the estimation, specify a real number argument.\\n  - If ``base_margin`` is supplied, ``base_score`` will not be added.\\n  - For sufficient number of iterations, changing this value will not have too much effect.\\n\\n  See :doc:`/tutorials/intercept` for more info.\\n\\n* ``eval_metric`` [default according to objective]\\n\\n  - Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and logloss for classification, `mean average precision` for ``rank:map``, etc.)\\n  - User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter ``eval_metric`` won\\'t override previous ones\\n\\n  - The choices are listed below:\\n\\n    - ``rmse``: `root mean square error <http://en.wikipedia.org/wiki/Root_mean_square_error>`_\\n    - ``rmsle``: root mean square log error: :math:`\\\\sqrt{\\\\frac{1}{N}[log(pred + 1) - log(label + 1)]^2}`. Default metric of ``reg:squaredlogerror`` objective. This metric reduces errors generated by outliers in dataset.  But because ``log`` function is employed, ``rmsle`` might output ``nan`` when prediction value is less than -1.  See ``reg:squaredlogerror`` for other requirements.\\n    - ``mae``: `mean absolute error <https://en.wikipedia.org/wiki/Mean_absolute_error>`_\\n    - ``mape``: `mean absolute percentage error <https://en.wikipedia.org/wiki/Mean_absolute_percentage_error>`_\\n    - ``mphe``: `mean Pseudo Huber error <https://en.wikipedia.org/wiki/Huber_loss>`_. Default metric of ``reg:pseudohubererror`` objective.\\n    - ``logloss``: `negative log-likelihood <http://en.wikipedia.org/wiki/Log-likelihood>`_\\n    - ``error``: Binary classification error rate. It is calculated as ``#(wrong cases)/#(all cases)``. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\\n    - ``error@t``: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through \\'t\\'.\\n    - ``merror``: Multiclass classification error rate. It is calculated as ``#(wrong cases)/#(all cases)``.\\n    - ``mlogloss``: `Multiclass logloss <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html>`_.\\n    - ``auc``: `Receiver Operating Characteristic Area under the Curve <https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve>`_.\\n      Available for classification and learning-to-rank tasks.\\n\\n      - When used with binary classification, the objective should be ``binary:logistic`` or similar functions that work on probability.\\n      - When used with multi-class classification, objective should be ``multi:softprob`` instead of ``multi:softmax``, as the latter doesn\\'t output probability.  Also the AUC is calculated by 1-vs-rest with reference class weighted by class prevalence.\\n      - When used with LTR task, the AUC is computed by comparing pairs of documents to count correctly sorted pairs.  This corresponds to pairwise learning to rank.  The implementation has some issues with average AUC around groups and distributed workers not being well-defined.\\n      - On a single machine the AUC calculation is exact. In a distributed environment the AUC is a weighted average over the AUC of training rows on each node - therefore, distributed AUC is an approximation sensitive to the distribution of data across workers. Use another metric in distributed environments if precision and reproducibility are important.\\n      - When input dataset contains only negative or positive samples, the output is `NaN`.  The behavior is implementation defined, for instance, ``scikit-learn`` returns :math:`0.5` instead.\\n\\n    - ``aucpr``: `Area under the PR curve <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\\n      Available for classification and learning-to-rank tasks.\\n\\n      After XGBoost 1.6, both of the requirements and restrictions for using ``aucpr`` in classification problem are similar to ``auc``.  For ranking task, only binary relevance label :math:`y \\\\in [0, 1]` is supported.  Different from ``map (mean average precision)``, ``aucpr`` calculates the *interpolated* area under precision recall curve using continuous interpolation.\\n\\n    - ``pre``: Precision at :math:`k`. Supports only learning to rank task.\\n    - ``ndcg``: `Normalized Discounted Cumulative Gain <http://en.wikipedia.org/wiki/NDCG>`_\\n    - ``map``: `Mean Average Precision <http://en.wikipedia.org/wiki/Mean_average_precision#Mean_average_precision>`_\\n\\n      The `average precision` is defined as:\\n\\n      .. math::\\n\\n         AP@l = \\\\frac{1}{min{(l, N)}}\\\\sum^l_{k=1}P@k \\\\cdot I_{(k)}\\n\\n      where :math:`I_{(k)}` is an indicator function that equals to :math:`1` when the document at :math:`k` is relevant and :math:`0` otherwise. The :math:`P@k` is the precision at :math:`k`, and :math:`N` is the total number of relevant documents. Lastly, the `mean average precision` is defined as the weighted average across all queries.\\n\\n    - ``ndcg@n``, ``map@n``, ``pre@n``: :math:`n` can be assigned as an integer to cut off the top positions in the lists for evaluation.\\n    - ``ndcg-``, ``map-``, ``ndcg@n-``, ``map@n-``: In XGBoost, the NDCG and MAP evaluate the score of a list without any positive samples as :math:`1`. By appending \"-\" to the evaluation metric name, we can ask XGBoost to evaluate these scores as :math:`0` to be consistent under some conditions.\\n    - ``poisson-nloglik``: negative log-likelihood for Poisson regression\\n    - ``gamma-nloglik``: negative log-likelihood for gamma regression\\n    - ``cox-nloglik``: negative partial log-likelihood for Cox proportional hazards regression\\n    - ``gamma-deviance``: residual deviance for gamma regression\\n    - ``tweedie-nloglik``: negative log-likelihood for Tweedie regression (at a specified value of the ``tweedie_variance_power`` parameter)\\n    - ``aft-nloglik``: Negative log likelihood of Accelerated Failure Time model.\\n      See :doc:`/tutorials/aft_survival_analysis` for details.\\n    - ``interval-regression-accuracy``: Fraction of data points whose predicted labels fall in the interval-censored labels.\\n      Only applicable for interval-censored data.  See :doc:`/tutorials/aft_survival_analysis` for details.\\n\\n* ``seed`` [default=0]\\n\\n  - Random number seed.  In the R package, if not specified, instead of defaulting to seed \\'zero\\', will take a random seed through R\\'s own RNG engine.\\n\\n* ``seed_per_iteration`` [default= ``false``]\\n\\n  - Seed PRNG determnisticly via iterator number.\\n\\nParameters for Tweedie Regression (``objective=reg:tweedie``)\\n=============================================================\\n* ``tweedie_variance_power`` [default=1.5]\\n\\n  - Parameter that controls the variance of the Tweedie distribution ``var(y) ~ E(y)^tweedie_variance_power``\\n  - range: (1,2)\\n  - Set closer to 2 to shift towards a gamma distribution\\n  - Set closer to 1 to shift towards a Poisson distribution.\\n\\nParameter for using Pseudo-Huber (``reg:pseudohubererror``)\\n===========================================================\\n\\n* ``huber_slope`` : A parameter used for Pseudo-Huber loss to define the :math:`\\\\delta` term. [default = 1.0]\\n\\nParameter for using Quantile Loss (``reg:quantileerror``)\\n=========================================================\\n\\n* ``quantile_alpha``: A scalar or a list of targeted quantiles.\\n\\n    .. versionadded:: 2.0.0\\n\\nParameter for using AFT Survival Loss (``survival:aft``) and Negative Log Likelihood of AFT metric (``aft-nloglik``)\\n====================================================================================================================\\n\\n* ``aft_loss_distribution``: Probability Density Function, ``normal``, ``logistic``, or ``extreme``.\\n\\n.. _ltr-param:\\n\\nParameters for learning to rank (``rank:ndcg``, ``rank:map``, ``rank:pairwise``)\\n================================================================================\\n\\nThese are parameters specific to learning to rank task. See :doc:`Learning to Rank </tutorials/learning_to_rank>` for an in-depth explanation.\\n\\n* ``lambdarank_pair_method`` [default = ``topk``]\\n\\n  How to construct pairs for pair-wise learning.\\n\\n  - ``mean``: Sample ``lambdarank_num_pair_per_sample`` pairs for each document in the query list.\\n  - ``topk``: Focus on top-``lambdarank_num_pair_per_sample`` documents. Construct :math:`|query|` pairs for each document at the top-``lambdarank_num_pair_per_sample`` ranked by the model.\\n\\n* ``lambdarank_num_pair_per_sample`` [range = :math:`[1, \\\\infty]`]\\n\\n  It specifies the number of pairs sampled for each document when pair method is ``mean``, or the truncation level for queries when the pair method is ``topk``. For example, to train with ``ndcg@6``, set ``lambdarank_num_pair_per_sample`` to :math:`6` and ``lambdarank_pair_method`` to ``topk``.\\n\\n* ``lambdarank_normalization`` [default = ``true``]\\n\\n  .. versionadded:: 2.1.0\\n\\n  Whether to normalize the leaf value by lambda gradient. This can sometimes stagnate the training progress.\\n\\n*  ``lambdarank_unbiased`` [default = ``false``]\\n\\n  Specify whether do we need to debias input click data.\\n\\n* ``lambdarank_bias_norm`` [default = 2.0]\\n\\n  :math:`L_p` normalization for position debiasing, default is :math:`L_2`. Only relevant when ``lambdarank_unbiased`` is set to true.\\n\\n* ``ndcg_exp_gain`` [default = ``true``]\\n\\n  Whether we should use exponential gain function for ``NDCG``. There are two forms of gain function for ``NDCG``, one is using relevance value directly while the other is using :math:`2^{rel} - 1` to emphasize on retrieving relevant documents. When ``ndcg_exp_gain`` is true (the default), relevance degree cannot be greater than 31.\\n\\n***********************\\nCommand Line Parameters\\n***********************\\nThe following parameters are only used in the console version of XGBoost\\n\\n* ``num_round``\\n\\n  - The number of rounds for boosting\\n\\n* ``data``\\n\\n  - The path of training data\\n\\n* ``test:data``\\n\\n  - The path of test data to do prediction\\n\\n* ``save_period`` [default=0]\\n\\n  - The period to save the model. Setting ``save_period=10`` means that for every 10 rounds XGBoost will save the model. Setting it to 0 means not saving any model during the training.\\n\\n* ``task`` [default= ``train``] options: ``train``, ``pred``, ``eval``, ``dump``\\n\\n  - ``train``: training using data\\n  - ``pred``: making prediction for test:data\\n  - ``eval``: for evaluating statistics specified by ``eval[name]=filename``\\n  - ``dump``: for dump the learned model into text format\\n\\n* ``model_in`` [default=NULL]\\n\\n  - Path to input model, needed for ``test``, ``eval``, ``dump`` tasks. If it is specified in training, XGBoost will continue training from the input model.\\n\\n* ``model_out`` [default=NULL]\\n\\n  - Path to output model after training finishes. If not specified, XGBoost will output files with such names as ``0003.model`` where ``0003`` is number of boosting rounds.\\n\\n* ``model_dir`` [default= ``models/``]\\n\\n  - The output directory of the saved models during training\\n\\n* ``fmap``\\n\\n  - Feature map, used for dumping model\\n\\n* ``dump_format`` [default= ``text``] options: ``text``, ``json``\\n\\n  - Format of model dump file\\n\\n* ``name_dump`` [default= ``dump.txt``]\\n\\n  - Name of model dump file\\n\\n* ``name_pred`` [default= ``pred.txt``]\\n\\n  - Name of prediction file, used in pred mode\\n\\n* ``pred_margin`` [default=0]\\n\\n  - Predict margin instead of transformed probability\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["parse_content(get_file_content(param_page['content']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sp5MTdTjvnb6","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":31,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"34e60198-3fe8-496a-ab4a-91fbbb83b6bb"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'heading': 'XGBoost Parameters',\n","  'level': 0,\n","  'content': ['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']},\n"," {'heading': 'Global Configuration',\n","  'level': 2,\n","  'content': ['The following parameters can be set in the global scope, using :py:func:`xgboost.config_context()` (Python) or ``xgb.set.config()`` (R).']},\n"," {'heading': '* ``verbosity``',\n","  'level': 3,\n","  'content': [': Verbosity of printing messages. Valid values of 0 (silent), 1 (warning), 2 (info), and 3 (debug).']},\n"," {'heading': '* ``use_rmm``',\n","  'level': 3,\n","  'content': [': Whether to use RAPIDS Memory Manager (RMM) to allocate cache GPU\\n  memory. The primary memory is always allocated on the RMM pool when XGBoost is built\\n  (compiled) with the RMM plugin enabled. Valid values are ``true`` and ``false``. See\\n  :doc:`/python/rmm-examples/index` for details.\\n\\n******************']},\n"," {'heading': 'General Parameters', 'level': 2, 'content': []},\n"," {'heading': '* ``booster``',\n","  'level': 3,\n","  'content': ['[default= ``gbtree`` ]\\n\\n  - Which booster to use. Can be ``gbtree``, ``gblinear`` or ``dart``; ``gbtree`` and ``dart`` use tree based models while ``gblinear`` uses linear functions.']},\n"," {'heading': '* ``device``',\n","  'level': 3,\n","  'content': ['[default= ``cpu``]\\n\\n  .. versionadded:: 2.0.0\\n\\n  - Device for XGBoost to run. User can set it to one of the following values:\\n\\n    + ``cpu``: Use CPU.\\n    + ``cuda``: Use a GPU (CUDA device).\\n    + ``cuda:<ordinal>``: ``<ordinal>`` is an integer that specifies the ordinal of the GPU (which GPU do you want to use if you have more than one devices).\\n    + ``gpu``: Default GPU device selection from the list of available and supported devices. Only ``cuda`` devices are supported currently.\\n    + ``gpu:<ordinal>``: Default GPU device selection from the list of available and supported devices. Only ``cuda`` devices are supported currently.\\n\\n    For more information about GPU acceleration, see :doc:`/gpu/index`. In distributed environments, ordinal selection is handled by distributed frameworks instead of XGBoost. As a result, using ``cuda:<ordinal>`` will result in an error. Use ``cuda`` instead.']},\n"," {'heading': '* ``verbosity``',\n","  'level': 3,\n","  'content': [\"[default=1]\\n\\n  - Verbosity of printing messages.  Valid values are 0 (silent), 1 (warning), 2 (info), 3\\n    (debug).  Sometimes XGBoost tries to change configurations based on heuristics, which\\n    is displayed as warning message.  If there's unexpected behaviour, please try to\\n    increase value of verbosity.\"]},\n"," {'heading': '* ``validate_parameters``',\n","  'level': 3,\n","  'content': [\"[default to ``false``, except for Python, R and CLI interface]\\n\\n  - When set to True, XGBoost will perform validation of input parameters to check whether\\n    a parameter is used or not. A warning is emitted when there's unknown parameter.\"]},\n"," {'heading': '* ``nthread``',\n","  'level': 3,\n","  'content': ['[default to maximum number of threads available if not set]\\n\\n  - Number of parallel threads used to run XGBoost.  When choosing it, please keep thread\\n    contention and hyperthreading in mind.']},\n"," {'heading': '* ``disable_default_eval_metric``',\n","  'level': 3,\n","  'content': ['[default= ``false``]\\n\\n  - Flag to disable default metric. Set to 1 or ``true`` to disable.']},\n"," {'heading': 'Parameters for Tree Booster', 'level': 1, 'content': []},\n"," {'heading': '* ``eta``',\n","  'level': 3,\n","  'content': ['[default=0.3, alias: ``learning_rate``]\\n\\n  - Step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new features, and ``eta`` shrinks the feature weights to make the boosting process more conservative.\\n  - range: [0,1]']},\n"," {'heading': '* ``gamma``',\n","  'level': 3,\n","  'content': ['[default=0, alias: ``min_split_loss``]\\n\\n  - Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger ``gamma`` is, the more conservative the algorithm will be. Note that a tree where no splits were made might still contain a single terminal node with a non-zero score.\\n  - range: [0,∞]']},\n"," {'heading': '* ``max_depth``',\n","  'level': 3,\n","  'content': ['[default=6]\\n\\n  - Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. ``exact`` tree method requires non-zero value.\\n  - range: [0,∞]']},\n"," {'heading': '* ``min_child_weight``',\n","  'level': 3,\n","  'content': ['[default=1]\\n\\n  - Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than ``min_child_weight``, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger ``min_child_weight`` is, the more conservative the algorithm will be.\\n  - range: [0,∞]']},\n"," {'heading': '* ``max_delta_step``',\n","  'level': 3,\n","  'content': ['[default=0]\\n\\n  - Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.\\n  - range: [0,∞]']},\n"," {'heading': '* ``subsample``',\n","  'level': 3,\n","  'content': ['[default=1]\\n\\n  - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\\n  - range: (0,1]']},\n"," {'heading': '* ``sampling_method``',\n","  'level': 3,\n","  'content': ['[default= ``uniform``]\\n\\n  - The method to use to sample the training instances.\\n  - ``uniform``: each training instance has an equal probability of being selected. Typically set\\n    ``subsample`` >= 0.5 for good results.\\n  - ``gradient_based``: the selection probability for each training instance is proportional to the\\n    *regularized absolute value* of gradients (more specifically, :math:`\\\\sqrt{g^2+\\\\lambda h^2}`).\\n    ``subsample`` may be set to as low as 0.1 without loss of model accuracy. Note that this\\n    sampling method is only supported when ``tree_method`` is set to ``hist`` and the device is ``cuda``; other tree\\n    methods only support ``uniform`` sampling.']},\n"," {'heading': '* ``colsample_bytree``',\n","  'level': 3,\n","  'content': [\", ``colsample_bylevel``, ``colsample_bynode`` [default=1]\\n\\n  - This is a family of parameters for subsampling of columns.\\n  - All ``colsample_by*`` parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\\n  - ``colsample_bytree`` is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\\n  - ``colsample_bylevel`` is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\\n  - ``colsample_bynode`` is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level. This is not supported by the exact tree method.\\n  - ``colsample_by*`` parameters work cumulatively. For instance,\\n    the combination ``{'colsample_bytree':0.5, 'colsample_bylevel':0.5,\\n    'colsample_bynode':0.5}`` with 64 features will leave 8 features to choose from at\\n    each split.\\n\\n    Using the Python or the R package, one can set the ``feature_weights`` for DMatrix to\\n    define the probability of each feature being selected when using column sampling.\\n    There's a similar parameter for ``fit`` method in sklearn interface.\"]},\n"," {'heading': '* ``lambda``',\n","  'level': 3,\n","  'content': ['[default=1, alias: ``reg_lambda``]\\n\\n  - L2 regularization term on weights. Increasing this value will make model more conservative.\\n  - range: [0, :math:`\\\\infty`]']},\n"," {'heading': '* ``alpha``',\n","  'level': 3,\n","  'content': ['[default=0, alias: ``reg_alpha``]\\n\\n  - L1 regularization term on weights. Increasing this value will make model more conservative.\\n  - range: [0, :math:`\\\\infty`]']},\n"," {'heading': '* ``tree_method``',\n","  'level': 3,\n","  'content': ['string [default= ``auto``]\\n\\n  - The tree construction algorithm used in XGBoost. See description in the `reference paper <http://arxiv.org/abs/1603.02754>`_ and :doc:`treemethod`.\\n\\n  - Choices: ``auto``, ``exact``, ``approx``, ``hist``, this is a combination of commonly\\n    used updaters.  For other updaters like ``refresh``, set the parameter ``updater``\\n    directly.\\n\\n    - ``auto``: Same as the ``hist`` tree method.\\n    - ``exact``: Exact greedy algorithm.  Enumerates all split candidates.\\n    - ``approx``: Approximate greedy algorithm using quantile sketch and gradient histogram.\\n    - ``hist``: Faster histogram optimized approximate greedy algorithm.']},\n"," {'heading': '* ``scale_pos_weight``',\n","  'level': 3,\n","  'content': ['[default=1]\\n\\n  - Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: ``sum(negative instances) / sum(positive instances)``. See :doc:`Parameters Tuning </tutorials/param_tuning>` for more discussion. Also, see Higgs Kaggle competition demo for examples: `R <https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-train.R>`_, `py1 <https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-numpy.py>`_, `py2 <https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-cv.py>`_, `py3 <https://github.com/dmlc/xgboost/blob/master/demo/guide-python/cross_validation.py>`_.']},\n"," {'heading': '* ``updater``',\n","  'level': 3,\n","  'content': [\"- A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters. However, it could be also set explicitly by a user. The following updaters exist:\\n\\n    - ``grow_colmaker``: non-distributed column-based construction of trees.\\n    - ``grow_histmaker``: distributed tree construction with row-based data splitting based on global proposal of histogram counting.\\n    - ``grow_quantile_histmaker``: Grow tree using quantized histogram.\\n    - ``grow_gpu_hist``:  Enabled when ``tree_method`` is set to ``hist`` along with ``device=cuda``.\\n    - ``grow_gpu_approx``: Enabled when ``tree_method`` is set to ``approx`` along with ``device=cuda``.\\n    - ``sync``: synchronizes trees in all distributed nodes.\\n    - ``refresh``: refreshes tree's statistics and/or leaf values based on the current data. Note that no random subsampling of data rows is performed.\\n    - ``prune``: prunes the splits where loss < min_split_loss (or gamma) and nodes that have depth greater than ``max_depth``.\"]},\n"," {'heading': '* ``refresh_leaf``',\n","  'level': 3,\n","  'content': [\"[default=1]\\n\\n  - This is a parameter of the ``refresh`` updater. When this flag is 1, tree leafs as well as tree nodes' stats are updated. When it is 0, only node stats are updated.\"]},\n"," {'heading': '* ``process_type``',\n","  'level': 3,\n","  'content': ['[default= ``default``]\\n\\n  - A type of boosting process to run.\\n  - Choices: ``default``, ``update``\\n\\n    - ``default``: The normal boosting process which creates new trees.\\n    - ``update``: Starts from an existing model and only updates its trees. In each boosting iteration, a tree from the initial model is taken, a specified sequence of updaters is run for that tree, and a modified tree is added to the new model. The new model would have either the same or smaller number of trees, depending on the number of boosting iterations performed. Currently, the following built-in updaters could be meaningfully used with this process type: ``refresh``, ``prune``. With ``process_type=update``, one cannot use updaters that create new trees.']},\n"," {'heading': '* ``grow_policy``',\n","  'level': 3,\n","  'content': ['[default= ``depthwise``]\\n\\n  - Controls a way new nodes are added to the tree.\\n  - Currently supported only if ``tree_method`` is set to ``hist`` or ``approx``.\\n  - Choices: ``depthwise``, ``lossguide``\\n\\n    - ``depthwise``: split at nodes closest to the root.\\n    - ``lossguide``: split at nodes with highest loss change.']},\n"," {'heading': '* ``max_leaves``',\n","  'level': 3,\n","  'content': ['[default=0]\\n\\n  - Maximum number of nodes to be added.  Not used by ``exact`` tree method.']},\n"," {'heading': '* ``max_bin``',\n","  'level': 3,\n","  'content': [', [default=256]\\n\\n  - Only used if ``tree_method`` is set to ``hist`` or ``approx``.\\n  - Maximum number of discrete bins to bucket continuous features.\\n  - Increasing this number improves the optimality of splits at the cost of higher computation time.']},\n"," {'heading': '* ``num_parallel_tree``',\n","  'level': 3,\n","  'content': [', [default=1]\\n\\n  - Number of parallel trees constructed during each iteration. This option is used to support boosted random forest.']},\n"," {'heading': '* ``monotone_constraints``',\n","  'level': 3,\n","  'content': ['- Constraint of variable monotonicity.  See :doc:`/tutorials/monotonic` for more information.']},\n"," {'heading': '* ``interaction_constraints``',\n","  'level': 3,\n","  'content': ['- Constraints for interaction representing permitted interactions.  The constraints must\\n    be specified in the form of a nest list, e.g. ``[[0, 1], [2, 3, 4]]``, where each inner\\n    list is a group of indices of features that are allowed to interact with each other.\\n    See :doc:`/tutorials/feature_interaction_constraint` for more information.']},\n"," {'heading': '* ``multi_strategy``',\n","  'level': 3,\n","  'content': [', [default = ``one_output_per_tree``]\\n\\n  .. versionadded:: 2.0.0\\n\\n  .. note:: This parameter is working-in-progress.\\n\\n  - The strategy used for training multi-target models, including multi-target regression\\n    and multi-class classification. See :doc:`/tutorials/multioutput` for more information.\\n\\n    - ``one_output_per_tree``: One model for each target.\\n    - ``multi_output_tree``:  Use multi-target trees.']},\n"," {'heading': '* ``max_cached_hist_node``',\n","  'level': 3,\n","  'content': [', [default = 65536]\\n\\n  Maximum number of cached nodes for CPU histogram.\\n\\n  .. versionadded:: 2.0.0\\n\\n  - For most of the cases this parameter should not be set except for growing deep trees\\n    on CPU.\\n\\n.. _cat-param:']},\n"," {'heading': 'Parameters for Categorical Feature',\n","  'level': 1,\n","  'content': ['These parameters are only used for training with categorical data. See\\n:doc:`/tutorials/categorical` for more information.\\n\\n.. note:: These parameters are experimental. ``exact`` tree method is not yet supported.']},\n"," {'heading': '* ``max_cat_to_onehot``',\n","  'level': 3,\n","  'content': ['.. versionadded:: 1.6.0\\n\\n  - A threshold for deciding whether XGBoost should use one-hot encoding based split for\\n    categorical data.  When number of categories is lesser than the threshold then one-hot\\n    encoding is chosen, otherwise the categories will be partitioned into children nodes.']},\n"," {'heading': '* ``max_cat_threshold``',\n","  'level': 3,\n","  'content': ['.. versionadded:: 1.7.0\\n\\n  - Maximum number of categories considered for each split. Used only by partition-based\\n    splits for preventing over-fitting.\\n\\nAdditional parameters for Dart Booster (``booster=dart``)\\n=========================================================\\n\\n.. note:: Using ``predict()`` with DART booster\\n\\n  If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n  some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n  not the training data. To obtain correct results on test sets, set ``iteration_range`` to\\n  a nonzero value, e.g.\\n\\n  .. code-block:: python\\n\\n    preds = bst.predict(dtest, iteration_range=(0, num_round))']},\n"," {'heading': '* ``sample_type``',\n","  'level': 3,\n","  'content': ['[default= ``uniform``]\\n\\n  - Type of sampling algorithm.\\n\\n    - ``uniform``: dropped trees are selected uniformly.\\n    - ``weighted``: dropped trees are selected in proportion to weight.']},\n"," {'heading': '* ``normalize_type``',\n","  'level': 3,\n","  'content': ['[default= ``tree``]\\n\\n  - Type of normalization algorithm.\\n\\n    - ``tree``: new trees have the same weight of each of dropped trees.\\n\\n      - Weight of new trees are ``1 / (k + learning_rate)``.\\n      - Dropped trees are scaled by a factor of ``k / (k + learning_rate)``.\\n\\n    - ``forest``: new trees have the same weight of sum of dropped trees (forest).\\n\\n      - Weight of new trees are ``1 / (1 + learning_rate)``.\\n      - Dropped trees are scaled by a factor of ``1 / (1 + learning_rate)``.']},\n"," {'heading': '* ``rate_drop``',\n","  'level': 3,\n","  'content': ['[default=0.0]\\n\\n  - Dropout rate (a fraction of previous trees to drop during the dropout).\\n  - range: [0.0, 1.0]']},\n"," {'heading': '* ``one_drop``',\n","  'level': 3,\n","  'content': ['[default=0]\\n\\n  - When this flag is enabled, at least one tree is always dropped during the dropout (allows Binomial-plus-one or epsilon-dropout from the original DART paper).']},\n"," {'heading': '* ``skip_drop``',\n","  'level': 3,\n","  'content': ['[default=0.0]\\n\\n  - Probability of skipping the dropout procedure during a boosting iteration.\\n\\n    - If a dropout is skipped, new trees are added in the same manner as ``gbtree``.\\n    - Note that non-zero ``skip_drop`` has higher priority than ``rate_drop`` or ``one_drop``.\\n\\n  - range: [0.0, 1.0]\\n\\nParameters for Linear Booster (``booster=gblinear``)\\n====================================================']},\n"," {'heading': '* ``lambda``',\n","  'level': 3,\n","  'content': ['[default=0, alias: ``reg_lambda``]\\n\\n  - L2 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples.']},\n"," {'heading': '* ``alpha``',\n","  'level': 3,\n","  'content': ['[default=0, alias: ``reg_alpha``]\\n\\n  - L1 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples.']},\n"," {'heading': '* ``updater``',\n","  'level': 3,\n","  'content': [\"[default= ``shotgun``]\\n\\n  - Choice of algorithm to fit linear model\\n\\n    - ``shotgun``: Parallel coordinate descent algorithm based on shotgun algorithm. Uses 'hogwild' parallelism and therefore produces a nondeterministic solution on each run.\\n    - ``coord_descent``: Ordinary coordinate descent algorithm. Also multithreaded but still produces a deterministic solution. When the ``device`` parameter is set to ``cuda`` or ``gpu``, a GPU variant would be used.\"]},\n"," {'heading': '* ``feature_selector``',\n","  'level': 3,\n","  'content': ['[default= ``cyclic``]\\n\\n  - Feature selection and ordering method']},\n"," {'heading': '* ``cyclic``',\n","  'level': 3,\n","  'content': [': Deterministic selection by cycling through features one at a time.']},\n"," {'heading': '* ``shuffle``',\n","  'level': 3,\n","  'content': [': Similar to ``cyclic`` but with random feature shuffling prior to each update.']},\n"," {'heading': '* ``random``',\n","  'level': 3,\n","  'content': [': A random (with replacement) coordinate selector.']},\n"," {'heading': '* ``greedy``',\n","  'level': 3,\n","  'content': [': Select coordinate with the greatest gradient magnitude.  It has ``O(num_feature^2)`` complexity. It is fully deterministic. It allows restricting the selection to ``top_k`` features per group with the largest magnitude of univariate weight change, by setting the ``top_k`` parameter. Doing so would reduce the complexity to ``O(num_feature*top_k)``.']},\n"," {'heading': '* ``thrifty``',\n","  'level': 3,\n","  'content': [': Thrifty, approximately-greedy feature selector. Prior to cyclic updates, reorders features in descending magnitude of their univariate weight changes. This operation is multithreaded and is a linear complexity approximation of the quadratic greedy selection. It allows restricting the selection to ``top_k`` features per group with the largest magnitude of univariate weight change, by setting the ``top_k`` parameter.']},\n"," {'heading': '* ``top_k``',\n","  'level': 3,\n","  'content': ['[default=0]\\n\\n  - The number of top features to select in ``greedy`` and ``thrifty`` feature selector. The value of 0 means using all the features.\\n\\n************************']},\n"," {'heading': 'Learning Task Parameters',\n","  'level': 2,\n","  'content': ['Specify the learning task and the corresponding learning objective. The objective options are below:']},\n"," {'heading': '* ``objective``',\n","  'level': 3,\n","  'content': ['[default=reg:squarederror]\\n\\n  - ``reg:squarederror``: regression with squared loss.\\n  - ``reg:squaredlogerror``: regression with squared log loss :math:`\\\\frac{1}{2}[log(pred + 1) - log(label + 1)]^2`.  All input labels are required to be greater than -1.  Also, see metric ``rmsle`` for possible issue  with this objective.\\n  - ``reg:logistic``: logistic regression, output probability\\n  - ``reg:pseudohubererror``: regression with Pseudo Huber loss, a twice differentiable alternative to absolute loss.\\n  - ``reg:absoluteerror``: Regression with L1 error. When tree model is used, leaf value is refreshed after tree construction. If used in distributed training, the leaf value is calculated as the mean value from all workers, which is not guaranteed to be optimal.\\n\\n    .. versionadded:: 1.7.0\\n\\n  - ``reg:quantileerror``: Quantile loss, also known as ``pinball loss``. See later sections for its parameter and :ref:`sphx_glr_python_examples_quantile_regression.py` for a worked example.\\n\\n    .. versionadded:: 2.0.0\\n\\n  - ``binary:logistic``: logistic regression for binary classification, output probability\\n  - ``binary:logitraw``: logistic regression for binary classification, output score before logistic transformation\\n  - ``binary:hinge``: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\\n  - ``count:poisson``: Poisson regression for count data, output mean of Poisson distribution.\\n\\n    + ``max_delta_step`` is set to 0.7 by default in Poisson regression (used to safeguard optimization)\\n\\n  - ``survival:cox``: Cox regression for right censored survival time data (negative values are considered right censored).\\n    Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function ``h(t) = h0(t) * HR``).\\n  - ``survival:aft``: Accelerated failure time model for censored survival time data.\\n    See :doc:`/tutorials/aft_survival_analysis` for details.\\n  - ``multi:softmax``: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\\n  - ``multi:softprob``: same as softmax, but output a vector of ``ndata * nclass``, which can be further reshaped to ``ndata * nclass`` matrix. The result contains predicted probability of each data point belonging to each class.\\n  - ``rank:ndcg``: Use LambdaMART to perform pair-wise ranking where `Normalized Discounted Cumulative Gain (NDCG) <http://en.wikipedia.org/wiki/NDCG>`_ is maximized. This objective supports position debiasing for click data.\\n  - ``rank:map``: Use LambdaMART to perform pair-wise ranking where `Mean Average Precision (MAP) <http://en.wikipedia.org/wiki/Mean_average_precision#Mean_average_precision>`_ is maximized\\n  - ``rank:pairwise``: Use LambdaRank to perform pair-wise ranking using the `ranknet` objective.\\n  - ``reg:gamma``: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be `gamma-distributed <https://en.wikipedia.org/wiki/Gamma_distribution#Occurrence_and_applications>`_.\\n  - ``reg:tweedie``: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be `Tweedie-distributed <https://en.wikipedia.org/wiki/Tweedie_distribution#Occurrence_and_applications>`_.']},\n"," {'heading': '* ``base_score``',\n","  'level': 3,\n","  'content': ['- The initial prediction score of all instances, global bias\\n  - The parameter is automatically estimated for selected objectives before training. To\\n    disable the estimation, specify a real number argument.\\n  - If ``base_margin`` is supplied, ``base_score`` will not be added.\\n  - For sufficient number of iterations, changing this value will not have too much effect.\\n\\n  See :doc:`/tutorials/intercept` for more info.']},\n"," {'heading': '* ``eval_metric``',\n","  'level': 3,\n","  'content': ['[default according to objective]\\n\\n  - Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and logloss for classification, `mean average precision` for ``rank:map``, etc.)\\n  - User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter ``eval_metric`` won\\'t override previous ones\\n\\n  - The choices are listed below:\\n\\n    - ``rmse``: `root mean square error <http://en.wikipedia.org/wiki/Root_mean_square_error>`_\\n    - ``rmsle``: root mean square log error: :math:`\\\\sqrt{\\\\frac{1}{N}[log(pred + 1) - log(label + 1)]^2}`. Default metric of ``reg:squaredlogerror`` objective. This metric reduces errors generated by outliers in dataset.  But because ``log`` function is employed, ``rmsle`` might output ``nan`` when prediction value is less than -1.  See ``reg:squaredlogerror`` for other requirements.\\n    - ``mae``: `mean absolute error <https://en.wikipedia.org/wiki/Mean_absolute_error>`_\\n    - ``mape``: `mean absolute percentage error <https://en.wikipedia.org/wiki/Mean_absolute_percentage_error>`_\\n    - ``mphe``: `mean Pseudo Huber error <https://en.wikipedia.org/wiki/Huber_loss>`_. Default metric of ``reg:pseudohubererror`` objective.\\n    - ``logloss``: `negative log-likelihood <http://en.wikipedia.org/wiki/Log-likelihood>`_\\n    - ``error``: Binary classification error rate. It is calculated as ``#(wrong cases)/#(all cases)``. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\\n    - ``error@t``: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through \\'t\\'.\\n    - ``merror``: Multiclass classification error rate. It is calculated as ``#(wrong cases)/#(all cases)``.\\n    - ``mlogloss``: `Multiclass logloss <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html>`_.\\n    - ``auc``: `Receiver Operating Characteristic Area under the Curve <https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve>`_.\\n      Available for classification and learning-to-rank tasks.\\n\\n      - When used with binary classification, the objective should be ``binary:logistic`` or similar functions that work on probability.\\n      - When used with multi-class classification, objective should be ``multi:softprob`` instead of ``multi:softmax``, as the latter doesn\\'t output probability.  Also the AUC is calculated by 1-vs-rest with reference class weighted by class prevalence.\\n      - When used with LTR task, the AUC is computed by comparing pairs of documents to count correctly sorted pairs.  This corresponds to pairwise learning to rank.  The implementation has some issues with average AUC around groups and distributed workers not being well-defined.\\n      - On a single machine the AUC calculation is exact. In a distributed environment the AUC is a weighted average over the AUC of training rows on each node - therefore, distributed AUC is an approximation sensitive to the distribution of data across workers. Use another metric in distributed environments if precision and reproducibility are important.\\n      - When input dataset contains only negative or positive samples, the output is `NaN`.  The behavior is implementation defined, for instance, ``scikit-learn`` returns :math:`0.5` instead.\\n\\n    - ``aucpr``: `Area under the PR curve <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\\n      Available for classification and learning-to-rank tasks.\\n\\n      After XGBoost 1.6, both of the requirements and restrictions for using ``aucpr`` in classification problem are similar to ``auc``.  For ranking task, only binary relevance label :math:`y \\\\in [0, 1]` is supported.  Different from ``map (mean average precision)``, ``aucpr`` calculates the *interpolated* area under precision recall curve using continuous interpolation.\\n\\n    - ``pre``: Precision at :math:`k`. Supports only learning to rank task.\\n    - ``ndcg``: `Normalized Discounted Cumulative Gain <http://en.wikipedia.org/wiki/NDCG>`_\\n    - ``map``: `Mean Average Precision <http://en.wikipedia.org/wiki/Mean_average_precision#Mean_average_precision>`_\\n\\n      The `average precision` is defined as:\\n\\n      .. math::\\n\\n         AP@l = \\\\frac{1}{min{(l, N)}}\\\\sum^l_{k=1}P@k \\\\cdot I_{(k)}\\n\\n      where :math:`I_{(k)}` is an indicator function that equals to :math:`1` when the document at :math:`k` is relevant and :math:`0` otherwise. The :math:`P@k` is the precision at :math:`k`, and :math:`N` is the total number of relevant documents. Lastly, the `mean average precision` is defined as the weighted average across all queries.\\n\\n    - ``ndcg@n``, ``map@n``, ``pre@n``: :math:`n` can be assigned as an integer to cut off the top positions in the lists for evaluation.\\n    - ``ndcg-``, ``map-``, ``ndcg@n-``, ``map@n-``: In XGBoost, the NDCG and MAP evaluate the score of a list without any positive samples as :math:`1`. By appending \"-\" to the evaluation metric name, we can ask XGBoost to evaluate these scores as :math:`0` to be consistent under some conditions.\\n    - ``poisson-nloglik``: negative log-likelihood for Poisson regression\\n    - ``gamma-nloglik``: negative log-likelihood for gamma regression\\n    - ``cox-nloglik``: negative partial log-likelihood for Cox proportional hazards regression\\n    - ``gamma-deviance``: residual deviance for gamma regression\\n    - ``tweedie-nloglik``: negative log-likelihood for Tweedie regression (at a specified value of the ``tweedie_variance_power`` parameter)\\n    - ``aft-nloglik``: Negative log likelihood of Accelerated Failure Time model.\\n      See :doc:`/tutorials/aft_survival_analysis` for details.\\n    - ``interval-regression-accuracy``: Fraction of data points whose predicted labels fall in the interval-censored labels.\\n      Only applicable for interval-censored data.  See :doc:`/tutorials/aft_survival_analysis` for details.']},\n"," {'heading': '* ``seed``',\n","  'level': 3,\n","  'content': [\"[default=0]\\n\\n  - Random number seed.  In the R package, if not specified, instead of defaulting to seed 'zero', will take a random seed through R's own RNG engine.\"]},\n"," {'heading': '* ``seed_per_iteration``',\n","  'level': 3,\n","  'content': ['[default= ``false``]\\n\\n  - Seed PRNG determnisticly via iterator number.\\n\\nParameters for Tweedie Regression (``objective=reg:tweedie``)\\n=============================================================']},\n"," {'heading': '* ``tweedie_variance_power``',\n","  'level': 3,\n","  'content': ['[default=1.5]\\n\\n  - Parameter that controls the variance of the Tweedie distribution ``var(y) ~ E(y)^tweedie_variance_power``\\n  - range: (1,2)\\n  - Set closer to 2 to shift towards a gamma distribution\\n  - Set closer to 1 to shift towards a Poisson distribution.\\n\\nParameter for using Pseudo-Huber (``reg:pseudohubererror``)\\n===========================================================']},\n"," {'heading': '* ``huber_slope``',\n","  'level': 3,\n","  'content': [': A parameter used for Pseudo-Huber loss to define the :math:`\\\\delta` term. [default = 1.0]\\n\\nParameter for using Quantile Loss (``reg:quantileerror``)\\n=========================================================']},\n"," {'heading': '* ``quantile_alpha``',\n","  'level': 3,\n","  'content': [': A scalar or a list of targeted quantiles.\\n\\n    .. versionadded:: 2.0.0\\n\\nParameter for using AFT Survival Loss (``survival:aft``) and Negative Log Likelihood of AFT metric (``aft-nloglik``)\\n====================================================================================================================']},\n"," {'heading': '* ``aft_loss_distribution``',\n","  'level': 3,\n","  'content': [': Probability Density Function, ``normal``, ``logistic``, or ``extreme``.\\n\\n.. _ltr-param:\\n\\nParameters for learning to rank (``rank:ndcg``, ``rank:map``, ``rank:pairwise``)\\n================================================================================\\n\\nThese are parameters specific to learning to rank task. See :doc:`Learning to Rank </tutorials/learning_to_rank>` for an in-depth explanation.']},\n"," {'heading': '* ``lambdarank_pair_method``',\n","  'level': 3,\n","  'content': ['[default = ``topk``]\\n\\n  How to construct pairs for pair-wise learning.\\n\\n  - ``mean``: Sample ``lambdarank_num_pair_per_sample`` pairs for each document in the query list.\\n  - ``topk``: Focus on top-``lambdarank_num_pair_per_sample`` documents. Construct :math:`|query|` pairs for each document at the top-``lambdarank_num_pair_per_sample`` ranked by the model.']},\n"," {'heading': '* ``lambdarank_num_pair_per_sample``',\n","  'level': 3,\n","  'content': ['[range = :math:`[1, \\\\infty]`]\\n\\n  It specifies the number of pairs sampled for each document when pair method is ``mean``, or the truncation level for queries when the pair method is ``topk``. For example, to train with ``ndcg@6``, set ``lambdarank_num_pair_per_sample`` to :math:`6` and ``lambdarank_pair_method`` to ``topk``.']},\n"," {'heading': '* ``lambdarank_normalization``',\n","  'level': 3,\n","  'content': ['[default = ``true``]\\n\\n  .. versionadded:: 2.1.0\\n\\n  Whether to normalize the leaf value by lambda gradient. This can sometimes stagnate the training progress.\\n\\n*  ``lambdarank_unbiased`` [default = ``false``]\\n\\n  Specify whether do we need to debias input click data.']},\n"," {'heading': '* ``lambdarank_bias_norm``',\n","  'level': 3,\n","  'content': ['[default = 2.0]\\n\\n  :math:`L_p` normalization for position debiasing, default is :math:`L_2`. Only relevant when ``lambdarank_unbiased`` is set to true.']},\n"," {'heading': '* ``ndcg_exp_gain``',\n","  'level': 3,\n","  'content': ['[default = ``true``]\\n\\n  Whether we should use exponential gain function for ``NDCG``. There are two forms of gain function for ``NDCG``, one is using relevance value directly while the other is using :math:`2^{rel} - 1` to emphasize on retrieving relevant documents. When ``ndcg_exp_gain`` is true (the default), relevance degree cannot be greater than 31.\\n\\n***********************']},\n"," {'heading': 'Command Line Parameters',\n","  'level': 2,\n","  'content': ['The following parameters are only used in the console version of XGBoost']},\n"," {'heading': '* ``num_round``',\n","  'level': 3,\n","  'content': ['- The number of rounds for boosting']},\n"," {'heading': '* ``data``',\n","  'level': 3,\n","  'content': ['- The path of training data\\n\\n* ``test:data``\\n\\n  - The path of test data to do prediction']},\n"," {'heading': '* ``save_period``',\n","  'level': 3,\n","  'content': ['[default=0]\\n\\n  - The period to save the model. Setting ``save_period=10`` means that for every 10 rounds XGBoost will save the model. Setting it to 0 means not saving any model during the training.']},\n"," {'heading': '* ``task``',\n","  'level': 3,\n","  'content': ['[default= ``train``] options: ``train``, ``pred``, ``eval``, ``dump``\\n\\n  - ``train``: training using data\\n  - ``pred``: making prediction for test:data\\n  - ``eval``: for evaluating statistics specified by ``eval[name]=filename``\\n  - ``dump``: for dump the learned model into text format']},\n"," {'heading': '* ``model_in``',\n","  'level': 3,\n","  'content': ['[default=NULL]\\n\\n  - Path to input model, needed for ``test``, ``eval``, ``dump`` tasks. If it is specified in training, XGBoost will continue training from the input model.']},\n"," {'heading': '* ``model_out``',\n","  'level': 3,\n","  'content': ['[default=NULL]\\n\\n  - Path to output model after training finishes. If not specified, XGBoost will output files with such names as ``0003.model`` where ``0003`` is number of boosting rounds.']},\n"," {'heading': '* ``model_dir``',\n","  'level': 3,\n","  'content': ['[default= ``models/``]\\n\\n  - The output directory of the saved models during training']},\n"," {'heading': '* ``fmap``',\n","  'level': 3,\n","  'content': ['- Feature map, used for dumping model']},\n"," {'heading': '* ``dump_format``',\n","  'level': 3,\n","  'content': ['[default= ``text``] options: ``text``, ``json``\\n\\n  - Format of model dump file']},\n"," {'heading': '* ``name_dump``',\n","  'level': 3,\n","  'content': ['[default= ``dump.txt``]\\n\\n  - Name of model dump file']},\n"," {'heading': '* ``name_pred``',\n","  'level': 3,\n","  'content': ['[default= ``pred.txt``]\\n\\n  - Name of prediction file, used in pred mode']},\n"," {'heading': '* ``pred_margin``',\n","  'level': 3,\n","  'content': ['[default=0]\\n\\n  - Predict margin instead of transformed probability']}]"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["def lazy_load(files):\n","  for file in files:\n","      content = get_file_content(file[\"content\"])\n","      parsed_content = parse_content(content)\n","\n","      metadata = {\n","          \"name\"    : file[\"name\"],\n","          \"path\"    : file[\"path\"],\n","          \"source\"  : file[\"html_url\"],\n","          \"doc_api\" : file[\"url\"]\n","      }\n","\n","      prevHeading0, prevHeading1, prevHeading2, prevHeading3 = \"\", \"\", \"\", \"\"\n","      prevContent0, prevContent1, prevContent2, prevContent3 = \"\", \"\", \"\", \"\"\n","\n","      for doc_dict in parsed_content :\n","\n","        try :\n","\n","          final_heading = \"\"\n","\n","          if doc_dict['level'] == 0:\n","            prevHeading0 = doc_dict['heading']\n","            prevContent0 = doc_dict['content']\n","            final_heading = doc_dict['heading']\n","            final_content = doc_dict['content']\n","\n","          elif doc_dict['level'] == 1:\n","            prevHeading1 = doc_dict['heading']\n","            prevContent1 = doc_dict['content']\n","            final_heading = f\"{prevHeading0} - {doc_dict['heading']}\"\n","            final_content = doc_dict['content']\n","\n","          elif doc_dict['level'] == 2:\n","            prevHeading2 = doc_dict['heading']\n","            prevContent2 = doc_dict['content']\n","            final_heading = f\"{prevHeading0} - {prevHeading1} - {doc_dict['heading']}\"\n","            final_content = doc_dict['content']\n","\n","          elif doc_dict['level'] == 3:\n","            prevHeading3 = doc_dict['heading']\n","            final_heading = f\"{prevHeading0} - {prevHeading1} - {prevHeading2}\"\n","            final_content = str(prevContent0) + \"\\n\" + str(doc_dict['heading']) + \":\" + str(doc_dict['content'])\n","\n","          if isinstance(final_content, str)==False:\n","            final_content = str(final_content)\n","\n","          final_content = final_heading.upper() + \"\\n\\n\" + final_content\n","\n","          yield Document(page_content=final_content, metadata=metadata)\n","\n","        except Exception as e :\n","          print(e)\n","\n","          yield Document(page_content=\"404 Error\", metadata=metadata)\n","\n","\n","def load(files):\n","  return [x for x in lazy_load(files)]"],"metadata":{"id":"hk1W4eRvQSkg","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":22,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# final_menu_docs = lazy_load(final_menu)\n","Documents = load(final_menu)"],"metadata":{"id":"7lKhIpWAjVmU","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":21,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["**Count of Chunks**"],"metadata":{"id":"hqHZ6Zvsjrce"}},{"cell_type":"code","source":["len(Documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbOIxKHQjqTB","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":20,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"b98cb41d-7942-4343-b6f7-8d5b2d2e01bf"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["681"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["len([x for x in Documents if x.metadata['path'] == 'doc/parameter.rst'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aY5up5pS5fFv","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":12,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"9d8754c1-fe6c-49dd-b26e-6924234448ab"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["81"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["for chunk in [x for x in Documents if x.metadata['path'] == 'doc/parameter.rst'][0:3] :\n","  print(chunk.metadata)\n","  print(chunk.page_content)\n","\n","  print(\"\\n\\n\\n\",\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_Lj1FZZ5lN8","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":8,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"d23d2464-a4b1-479e-b986-5e364961f7e5"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'parameter.rst', 'path': 'doc/parameter.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst', 'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master'}\n","XGBOOST PARAMETERS\n","\n","['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']\n","\n","\n","\n"," --------------------------------------------------\n","{'name': 'parameter.rst', 'path': 'doc/parameter.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst', 'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master'}\n","XGBOOST PARAMETERS -  - GLOBAL CONFIGURATION\n","\n","['The following parameters can be set in the global scope, using :py:func:`xgboost.config_context()` (Python) or ``xgb.set.config()`` (R).']\n","\n","\n","\n"," --------------------------------------------------\n","{'name': 'parameter.rst', 'path': 'doc/parameter.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst', 'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master'}\n","XGBOOST PARAMETERS -  - GLOBAL CONFIGURATION\n","\n","['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']\n","* ``verbosity``:[': Verbosity of printing messages. Valid values of 0 (silent), 1 (warning), 2 (info), and 3 (debug).']\n","\n","\n","\n"," --------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## Improving Document Quality"],"metadata":{"id":"MfglvjEMPh-J"}},{"cell_type":"code","source":["Documents[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"wPMDkxduPrAL","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":6,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"f6466ec7-62ef-49c2-c618-47f855ed5f24"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nbasic_walkthrough               Basic feature walkthrough\\ncustom_objective                Customize loss function, and evaluation metric\\nboost_from_prediction           Boosting from existing prediction\\npredict_first_ntree             Predicting using first n trees\\ngeneralized_linear_model        Generalized Linear Model\\ncross_validation                Cross validation\\ncreate_sparse_matrix            Create Sparse Matrix\\npredict_leaf_indices            Predicting the corresponding leaves\\nearly_stopping                  Early Stop in training\\npoisson_regression              Poisson regression on count data\\ntweedie_regression              Tweedie regression\\ngpu_accelerated                 GPU-accelerated tree building algorithms\\ninteraction_constraints         Interaction constraints among features'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","template = \"\"\"\n","You are an expert at cleaning and formatting documents.\n","Write a summary of the document, before re-writing it in cleaner format. Do dot write extra information, while re-writing.\n","\n","### Original Document :\n","{text}\n","###\n","\"\"\"\n","\n","doc_quality_improving_prompt = PromptTemplate(\n","    input_variables=[\"text\"],\n","    template=template,\n",")"],"metadata":{"id":"XZiiIx5_PsgM","executionInfo":{"status":"ok","timestamp":1722403075475,"user_tz":-330,"elapsed":5,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1722403077092,"user_tz":-330,"elapsed":1622,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"id":"llBXQvdyV0pY"},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint\n","\n","repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","# repo_id = 'dunzhang/stella_en_1.5B_v5'\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id=repo_id,\n","    task=\"text-generation\",\n","    max_new_tokens=15000,\n","    do_sample=False,\n","    repetition_penalty=1.03,\n","    temperature = 0.6\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722403078132,"user_tz":-330,"elapsed":1041,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"2d12c2cc-63ec-482e-ab53-3bd88638799f","id":"P4q9smiuV0pZ"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["chain_improve_quality = doc_quality_improving_prompt | llm"],"metadata":{"id":"MHqjud_tV6Pt","executionInfo":{"status":"ok","timestamp":1722403078133,"user_tz":-330,"elapsed":3,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["for i in range(5):\n","  print(\"\")\n","  print(\"<== Original  ==>\")\n","  print(Documents[i].page_content)\n","  print()\n","  print(\"<== Improved ==>\")\n","  print(chain_improve_quality.invoke({'text':Documents[i].page_content}))\n","  print(\"-\"*100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGokuBBYW17f","executionInfo":{"status":"ok","timestamp":1722403094034,"user_tz":-330,"elapsed":15904,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"5ac67165-942b-4ca1-a5ed-90ef9cbddb8a"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","<== Original  ==>\n","\n","\n","basic_walkthrough               Basic feature walkthrough\n","custom_objective                Customize loss function, and evaluation metric\n","boost_from_prediction           Boosting from existing prediction\n","predict_first_ntree             Predicting using first n trees\n","generalized_linear_model        Generalized Linear Model\n","cross_validation                Cross validation\n","create_sparse_matrix            Create Sparse Matrix\n","predict_leaf_indices            Predicting the corresponding leaves\n","early_stopping                  Early Stop in training\n","poisson_regression              Poisson regression on count data\n","tweedie_regression              Tweedie regression\n","gpu_accelerated                 GPU-accelerated tree building algorithms\n","interaction_constraints         Interaction constraints among features\n","\n","<== Improved ==>\n","\n","### Summary:\n","This document outlines various features of the machine learning model, including customizing the loss function and evaluation metric, boosting from existing predictions, predicting using the first n trees, generalized linear models, cross validation, creating sparse matrices, predicting leaf indices, early stopping, poisson regression, tweedie regression, GPU-accelerated tree building algorithms, and interaction constraints among features.\n","\n","### Cleaned Format:\n","This document details the following features of the machine learning model:\n","\n","- Customizable loss function and evaluation metric\n","- Boosting from existing predictions\n","- Predicting using the first n trees\n","- Generalized Linear Models\n","- Cross validation\n","- Creating sparse matrices\n","- Predicting leaf indices\n","- Early stopping\n","- Poisson regression\n","- Tweedie regression\n","- GPU-accelerated tree building algorithms\n","- Interaction constraints among features.\n","----------------------------------------------------------------------------------------------------\n","\n","<== Original  ==>\n"," - XGBOOST R FEATURE WALKTHROUGH\n","\n","['* [Basic walkthrough of wrappers](basic_walkthrough.R)\\n* [Customize loss function, and evaluation metric](custom_objective.R)\\n* [Boosting from existing prediction](boost_from_prediction.R)\\n* [Predicting using first n trees](predict_first_ntree.R)\\n* [Generalized Linear Model](generalized_linear_model.R)\\n* [Cross validation](cross_validation.R)\\n* [Create a sparse matrix from a dense one](create_sparse_matrix.R)\\n* [Use GPU-accelerated tree building algorithms](gpu_accelerated.R)']\n","\n","<== Improved ==>\n","Summary:\n","This document outlines various XGBoost functions in R, including basic walkthroughs, customizing loss functions and evaluation metrics, boosting from existing predictions, predicting using the first n trees, implementing Generalized Linear Models, conducting cross-validation, creating a sparse matrix from a dense one, and using GPU-accelerated tree building algorithms.\n","\n","### Cleaned Format:\n","#### XGBoost Functions in R\n","\n","1. **Basic walkthroughs**\n","   - *[basic_walkthrough.R]*\n","2. **Customizing loss functions and evaluation metrics**\n","   - *[custom_objective.R]*\n","3. **Boosting from existing predictions**\n","   - *[boost_from_prediction.R]*\n","4. **Predicting using the first n trees**\n","   - *[predict_first_ntree.R]*\n","5. **Generalized Linear Models**\n","   - *[generalized_linear_model.R]*\n","6. **Cross validation**\n","   - *[cross_validation.R]*\n","7. **Creating a sparse matrix from a dense one**\n","   - *[create_sparse_matrix.R]*\n","8. **Using GPU-accelerated tree building algorithms**\n","   - *[gpu_accelerated.R]*\n","----------------------------------------------------------------------------------------------------\n","\n","<== Original  ==>\n"," - BENCHMARKS\n","\n","['* [Starter script for Kaggle Higgs Boson](../../demo/kaggle-higgs)']\n","\n","<== Improved ==>\n","['* [Starter script for Kaggle Titanic](../../demo/kaggle-titanic)']\n","###\n","['* [Starter script for Kaggle Wine](../../demo/kaggle-wine)']\n","###\n","['* [Starter script for Kaggle MNIST](../../demo/kaggle-mnist)']\n","###\n","['* [Starter script for Kaggle CIFAR10](../../demo/kaggle-cifar10)']\n","###\n","['* [Starter script for Kaggle Traffic Prediction](../../demo/kaggle-traffic)']\n","###\n","['* [Starter script for Kaggle House Prices](../../demo/kaggle-house-prices)']\n","###\n","['* [Starter script for Kaggle Santas vs. Rudolph](../../demo/kaggle-santas)']\n","###\n","['* [Starter script for Kaggle Adult Income](../../demo/kaggle-adult-income)']\n","\n","This document lists various starter scripts for different Kaggle competitions.\n","\n","### Cleaned Format:\n","The following is a list of starter scripts for several Kaggle competitions:\n","\n","- [Kaggle Higgs Boson](../../demo/kaggle-higgs)\n","- [Kaggle Titanic](../../demo/kaggle-titanic)\n","- [Kaggle Wine](../../demo/kaggle-wine)\n","- [Kaggle MNIST](../../demo/kaggle-mnist)\n","- [Kaggle CIFAR10](../../demo/kaggle-cifar10)\n","- [Kaggle Traffic Prediction](../../demo/kaggle-traffic)\n","- [Kaggle House Prices](../../demo/kaggle-house-prices)\n","- [Kaggle Santas vs. Rudolph](../../demo/kaggle-santas)\n","- [Kaggle Adult Income](../../demo/kaggle-adult-income)\n","----------------------------------------------------------------------------------------------------\n","\n","<== Original  ==>\n"," - NOTES\n","\n","['* Contribution of examples, benchmarks is more than welcomed!\\n* If you like to share how you use xgboost to solve your problem, send a pull request :)']\n","\n","<== Improved ==>\n","Summary:\n","The document includes a note encouraging contributions of examples and benchmarks for using xgboost, as well as an invitation for users to share their experiences and methods for solving problems with xgboost by sending a pull request.\n","\n","### Cleaned Document:\n","This document welcomes contributions of examples and benchmarks for using xgboost. Users who have applied xgboost to solve a problem are invited to share their experiences by submitting a pull request.\n","----------------------------------------------------------------------------------------------------\n","\n","<== Original  ==>\n","\n","\n","require(xgboost)\n","require(methods)\n","\n","# we load in the agaricus dataset\n","# In this example, we are aiming to predict whether a mushroom is edible\n","data(agaricus.train, package = 'xgboost')\n","data(agaricus.test, package = 'xgboost')\n","train <- agaricus.train\n","test <- agaricus.test\n","# the loaded data is stored in sparseMatrix, and label is a numeric vector in {0,1}\n","class(train$label)\n","class(train$data)\n","\n","#-------------Basic Training using XGBoost-----------------\n","# this is the basic usage of xgboost you can put matrix in data field\n","# note: we are putting in sparse matrix here, xgboost naturally handles sparse input\n","# use sparse matrix when your feature is sparse(e.g. when you are using one-hot encoding vector)\n","print(\"Training xgboost with sparseMatrix\")\n","bst <- xgboost(x = train$data, y = factor(train$label, c(0, 1)),\n","               params = list(max_depth = 2, eta = 1),\n","               nrounds = 2, nthread = 2)\n","# alternatively, you can put in dense matrix, i.e. basic R-matrix\n","print(\"Training xgboost with Matrix\")\n","bst <- xgboost(x = as.matrix(train$data), y = factor(train$label, c(0, 1)),\n","               params = list(max_depth = 2, eta = 1),\n","               nrounds = 2, nthread = 2)\n","\n","# you can also put in xgb.DMatrix object, which stores label, data and other meta datas needed for advanced features\n","print(\"Training xgboost with xgb.DMatrix\")\n","dtrain <- xgb.DMatrix(data = train$data, label = train$label)\n","params <- list(max_depth = 2, eta = 1, nthread = 2, objective = \"binary:logistic\")\n","bst <- xgb.train(data = dtrain, params = params, nrounds = 2)\n","\n","# Verbose = 0,1,2\n","print(\"Train xgboost with verbose 0, no message\")\n","bst <- xgb.train(data = dtrain, params = params, nrounds = 2, verbose = 0)\n","print(\"Train xgboost with verbose 1, print evaluation metric\")\n","bst <- xgb.train(data = dtrain, params = params, nrounds = 2, verbose = 1)\n","print(\"Train xgboost with verbose 2, also print information about tree\")\n","bst <- xgb.train(data = dtrain, params = params, nrounds = 2, verbose = 2)\n","\n","# you can also specify data as file path to a LIBSVM format input\n","# since we do not have this file with us, the following line is just for illustration\n","# bst <- xgboost(data = 'agaricus.train.svm', max_depth = 2, eta = 1, nrounds = 2,objective = \"binary:logistic\")\n","\n","#--------------------basic prediction using xgboost--------------\n","# you can do prediction using the following line\n","# you can put in Matrix, sparseMatrix, or xgb.DMatrix\n","pred <- predict(bst, test$data)\n","err <- mean(as.numeric(pred > 0.5) != test$label)\n","print(paste(\"test-error=\", err))\n","\n","#-------------------save and load models-------------------------\n","# save model to binary local file\n","xgb.save(bst, \"xgboost.model\")\n","# load binary model to R\n","# Function doesn't take 'nthreads', but can be set like this:\n","RhpcBLASctl::omp_set_num_threads(1)\n","bst2 <- xgb.load(\"xgboost.model\")\n","pred2 <- predict(bst2, test$data)\n","# pred2 should be identical to pred\n","print(paste(\"sum(abs(pred2-pred))=\", sum(abs(pred2 - pred))))\n","\n","# save model to R's raw vector\n","raw <- xgb.save.raw(bst)\n","# load binary model to R\n","bst3 <- xgb.load.raw(raw)\n","pred3 <- predict(bst3, test$data)\n","# pred3 should be identical to pred\n","print(paste(\"sum(abs(pred3-pred))=\", sum(abs(pred3 - pred))))\n","\n","#----------------Advanced features --------------\n","# to use advanced features, we need to put data in xgb.DMatrix\n","dtrain <- xgb.DMatrix(data = train$data, label = train$label)\n","dtest <- xgb.DMatrix(data = test$data, label = test$label)\n","#---------------Using an evaluation set----------------\n","# 'evals' is a list of xgb.DMatrix, each of them is tagged with name\n","evals <- list(train = dtrain, test = dtest)\n","# to train with an evaluation set, use xgb.train, which contains more advanced features\n","# 'evals' argument allows us to monitor the evaluation result on all data in the list\n","print(\"Train xgboost using xgb.train with evaluation data\")\n","bst <- xgb.train(data = dtrain, max_depth = 2, eta = 1, nrounds = 2, evals = evals,\n","                 nthread = 2, objective = \"binary:logistic\")\n","# we can change evaluation metrics, or use multiple evaluation metrics\n","print(\"train xgboost using xgb.train with evaluation data, watch logloss and error\")\n","bst <- xgb.train(data = dtrain, max_depth = 2, eta = 1, nrounds = 2, evals = evals,\n","                 eval_metric = \"error\", eval_metric = \"logloss\",\n","                 nthread = 2, objective = \"binary:logistic\")\n","\n","# xgb.DMatrix can also be saved using xgb.DMatrix.save\n","xgb.DMatrix.save(dtrain, \"dtrain.buffer\")\n","# to load it in, simply call xgb.DMatrix\n","dtrain2 <- xgb.DMatrix(\"dtrain.buffer\")\n","bst <- xgb.train(data = dtrain2, max_depth = 2, eta = 1, nrounds = 2, evals = evals,\n","                 nthread = 2, objective = \"binary:logistic\")\n","# information can be extracted from xgb.DMatrix using getinfo\n","label <- getinfo(dtest, \"label\")\n","pred <- predict(bst, dtest)\n","err <- as.numeric(sum(as.integer(pred > 0.5) != label)) / length(label)\n","print(paste(\"test-error=\", err))\n","\n","# You can dump the tree you learned using xgb.dump into a text file\n","dump_path <- file.path(tempdir(), 'dump.raw.txt')\n","xgb.dump(bst, dump_path, with_stats = TRUE)\n","\n","# Finally, you can check which features are the most important.\n","print(\"Most important features (look at column Gain):\")\n","imp_matrix <- xgb.importance(feature_names = colnames(train$data), model = bst)\n","print(imp_matrix)\n","\n","# Feature importance bar plot by gain\n","print(\"Feature importance Plot : \")\n","print(xgb.plot.importance(importance_matrix = imp_matrix))\n","\n","<== Improved ==>\n","\n","### Summary :\n","\n","This document explains how to use the xgboost package in R for binary classification tasks. It covers the following topics:\n","\n","1. Loading the agaricus dataset.\n","2. Basic training using xgboost with sparse matrices, matrices, and xgb.DMatrix objects.\n","3. Prediction using xgboost.\n","4. Saving and loading models.\n","5. Advanced features using xgb.DMatrix.\n","\n","The document provides examples for each topic and demonstrates how to change various parameters such as the maximum depth, learning rate, number of rounds, and number of threads. It also shows how to specify data as a file path to a LIBSVM format input. The document concludes by explaining how to extract information from xgb.DMatrix using getinfo and how to dump the tree using xgb.dump.\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# prompt: create a function and use try, ecxeption Documents2 = [chain_improve_quality.invoke({'text':x.page_content}) for x in Documents if len(x.page_content)>200]\n","\n","def improve_docs(docs):\n","  Documents2 = []\n","  for x in docs:\n","    if len(x.page_content)>200:\n","      try:\n","        improved_doc = chain_improve_quality.invoke({'text':x.page_content})\n","        Documents2.append(Document(page_content=improved_doc, metadata=x.metadata))\n","      except Exception as e:\n","        print(f\"Error processing document: {e}\")\n","  return Documents2\n","\n","Documents2 = improve_docs(Documents)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIqFCEuYeV2h","executionInfo":{"status":"ok","timestamp":1722405056924,"user_tz":-330,"elapsed":708434,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"a8922edc-c052-420b-eaa0-23a4cf1a9a42"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Error processing document: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: EicOoFYHF2lLaVBAWXJi-)\n","\n","Input validation error: `inputs` tokens + `max_new_tokens` must be <= 32768. Given: 183665 `inputs` tokens and 15000 `max_new_tokens`\n","Make sure 'text-generation' task is supported by the model.\n","Error processing document: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: NwzHyc3xNrI-uciFB5TG1)\n","\n","Input validation error: `inputs` tokens + `max_new_tokens` must be <= 32768. Given: 742311 `inputs` tokens and 15000 `max_new_tokens`\n","Make sure 'text-generation' task is supported by the model.\n","Error processing document: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: a383cc68-228a-48dd-97c8-5a6a54f55629)')\n","Error processing document: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: HHscOYBqNZEO1EXcVmJau)\n","\n","Input validation error: `inputs` tokens + `max_new_tokens` must be <= 32768. Given: 32833 `inputs` tokens and 15000 `max_new_tokens`\n","Make sure 'text-generation' task is supported by the model.\n","Error processing document: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xIpt5HcEzzW_7ODB7VWMN)\n","\n","Input validation error: `inputs` tokens + `max_new_tokens` must be <= 32768. Given: 19247 `inputs` tokens and 15000 `max_new_tokens`\n","Make sure 'text-generation' task is supported by the model.\n","Error processing document: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: 068e2b7e-033c-496b-852f-435555e65a2e)')\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ax85zePU9eDyc4lojjrrE)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ZRTNHJXoVC7FWIWCIbInK)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 7Mv7mc-WLjvyRBZfmk6Fo)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: BqNd52QGHs8jOoHDx6axe)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: JcSO3wboD2C6pkrKSPJjo)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 8d1vBttMXl2LudO7zD1kh)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: u7SXZVHjkWFSu3tX_W6C4)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: K2WcuDpaJyhHbH0VIfxE_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: flEWN_R58xsq41hCWTuK3)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 1gijy70k7nk40-nGN0cQC)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: WVtJqz0TA6650Adk6p8H-)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 6mZbaQ7aNoVVdzIJJon7U)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Gm7SLgyyaPasevjVmiD2_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 8Ml2nrCopgqWxwH-yA6Eh)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: DO0hKbxEJeBYeWXCXsdNm)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ePrki1JbmLtIO-HMvAK08)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 838ywvIuikL1JAGBn74B0)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: rOq8HnAQC-rqSpALaZWGG)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: fN8Dgs4S65e5j3UmCoFvt)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: lEF-tcKjsVSPX9DCqeJbX)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: CW8YIPnY-I4HSkeziA5yJ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: RaWttMU7krOUc6AK-koGP)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 05rBz6ZbAf-SsN5XbQE-d)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: iQ2xg5cD6MKtt4RcWD9Z_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: i_zM3kua-XpM3_8zCcSr2)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: tn9b35ta2j0_uamw634Vt)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: BaWylXOaTBzqCeyZ9EpKM)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: LlCYLhVqHUy9uL-BzIDZz)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 4toTZ7UWQe8ZbIBWlBMqg)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: W8xYbE55LjPngIjyKWTxA)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Gf4KXEuQ9cR-ucsPzPq9O)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: JGxugy1vJ2_ML7AGw3dIL)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Acnsm3hEo3qO3nh75nAfM)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: eUBMRYLkawkdbRa7MLz9Y)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: YfRQqCbSaqtu5bkY8FFUr)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: SxviFev48fLgZgovCRj7p)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: kVRioGHGyZV0gDWNB85XF)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: U8IFjsamvlzP1TP3xmFM1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: a0hXa1CDtTh38BQ62XGOu)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: s_aAmCNCiEnsm2UjgbZov)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: QjZ2FsZxS34S_K5mruqoC)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: lH4P0_deFs2vuMyaInxIO)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: QOvUtH5aP_D1sadvcV2eI)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: wzkUnMhfH0D1HOb8nPmuj)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 9tdGCTeeG-uAl99_un841)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: uRY_8btjDqi9n1aqBLuFQ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: TQXUAvKRu_vx6FVbiYfrl)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xiu54-WZ1YPCYzOu16e5F)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: jpwz04Nf3O6-FRVlRRVf4)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: TjJvOcSLXktJu7Ysqsx4y)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 45PSG50KLmhfXzAeRf1zp)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: pL7wF11oAfvnQy4hgZNQA)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: LLZz-zaoKeBsmX_oOt685)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Z_oiFxyQR_4c3ye9Aq9Ap)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: niARj19qPRRsdD4worR30)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 2cd6cvyZ2VniJSfCxXhQf)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 3jTZM01bf77vLP0kkBsX_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: kW7GHk6iHzCO_9ynWgOuB)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: GgGJ46JijR3hSsOme6Amq)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: GysKr3EJIqwXWl694gr7Q)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: W0tF2Zhn2OLK-PZJzOh0Z)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: m2qZZax-Cxk3dZIBfGzx8)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: dQ0g5V4tmwKEENJuxbRRV)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: pgUh2fnUcATsOY_OOe2V1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 4Uv5x5y8HsAxJVjm0dMcS)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: L9LuMJGahDwATtKMkdGCg)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: bjaSdepHB-p3jIQcVHuyf)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: r0vd0B47INCJaXmgcgA7Q)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: B-CFybyU6qyr2p8Alm7Tf)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: AjxHw5Hx7sIYARYtqRDPK)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: z8fbLJvCNXwsEpN5A5RRH)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: DM4bRWlToGsKkrIspbZ5v)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: dmf1VAmT7ZRJH24tmcKTZ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: _jPVzJJAs9kVL2nnbrHSj)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: yprfkdxtnPGmNBqCQt_pn)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 3pOaeATv9FG-eTveRQIFt)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: dFECSusPPZapObo7x64jo)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 2f8VkxcPE5S45F77xQV2k)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ZLuSUd7Ff3RlGqdebJUt6)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: yEDEAzVaTMg58rgnHmYHY)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: cJooNeqJwEUNUqmgJcrbD)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: sz_RwfqqJmtkMNcYdh-wp)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: -4DUeShD2PimNilfwHfkm)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: WPUefz0tFC-zalOVHlvyb)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: SQwD3yeObrupecEQQTYw5)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: b5LzYBxSCV3R5Yfoj9tj6)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: hTr8ep73caD-0HGz2Fh_F)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Cg2JovQtr5W1fakNbB3v1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 7iC9Rmi4l_fDD0AivWfO1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 6d2lcqYL9_d4gFGBv7K5r)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: MoF0oFTvhoOaxSshEL1_1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ANeZee7XpeBT2MaOp9lHW)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xDM7nE9XMw-HVFHGX7mjz)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: pGS1n3YTXncfa4zfnwA3D)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: _M55YSnJl-lmjBkxZX5NO)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: eLLQAyH-52NXmPfxGCPC9)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: yZSIvWspITAF6uod1tZEN)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: EfWkAEociMlr09voH7p7L)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: RFkuGHkKUuzD2_Ekd2SI_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: LdA0nTjfrlrBo771leOkA)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: wS4xWBP9dhHLbqrJqCxVu)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: SrKQutDMHbtw2MA8UuGPm)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ab3wTw6_iQ3vSZzCuVwu9)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Tt3RdRfmGG717iZIPJ15V)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: CAxlISRYveEdkMiZ-LrmC)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: -3OwzHMRsWJiqScG0vYCI)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: OfiSYVLqamOAVGv1n3o4V)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: LPZ6ims_AnqmOh5RwVmtb)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xRVgEio3SMQwA8cAciglH)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: WAvmXQq9WDLVt8JVJSuai)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 2Dc45bfZM7zxHb3femVZi)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: mqhbtCY3nA8uYK--Pm48Q)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: pBrWL_B998BSN262lig4r)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: MysbZRqNts763vqKb4A2C)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: EL7r4OGfexsB1Q67EAsZA)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: r96gZ9BIpGV6hzbhX47C1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 49C4Blld_x7UwwLV9Y2-7)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 9oIt6mnKx_wzrb-hPOXFk)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: cnD_B8jTXKaQtZdSBiuKB)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 6dcSj2hdtxImjgFH0nobx)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: mNSSIgKGyStaghEaf4qGY)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: _q6FFZW_Oesehqsi4861S)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: KTvvVfjcj6JPHoICPlytD)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: uozAtbS533ogGUkeK80Zk)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: nWArHgAUQ0mTO9JbFt0O_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: _D1vBC7sadWOTL_Ae1oll)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: OI7HiCS-yteVKEy-5UnSd)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 21UHsQFhSO4SoHar-FA5N)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: MvZpEwvyN_fljdw5yDJAq)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: D8U_9QbczjjMcUcMkMPN4)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: di72bH0m5y9L4ykH227HM)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: EUfnaDnWvLJuo64WE7zz5)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 8ywyhAa-qGW4_d3UGQ2HX)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: V-K954vqym7lhb4_N10dd)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Bz6h13KzKYMYgzXAPHL8I)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: lJZxDW_tU1GMcQPtj711P)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: _1qKib70NLvVIvhuFlCx7)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: y6J6y6E0mdX7BRWRMVNw_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: vfp6Kq7E8UjhL38WG-LT1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: H40T41vY3ADnmoS7Q1DBh)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: W_A4XJe8DbuziFOsNEvRn)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Ns6VBW9n0UhGu0vYy8zbi)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: CkxZL1JQglEawjOjSskJC)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: HdbLLlIZKgb5QyOuomrU2)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: wPNsK7X7OEGLL0EmsiE04)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: NCLxmDnfN84AqfR2pqvAp)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 90Ox9bLWngi4LKWQZLerb)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: AhmEZ5QPerBtxHZgRyksT)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: zOVMCSEdVva2xdwlELaJB)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: FVEWze05LJX5WMelDo4BD)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: qmWt0w45wwtjGPgQBBh7G)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: mpXNpS8CmmzAbePugoKEu)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: r_okIMEMFXs5QBcgEhU0P)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: prpRO1olJqgjRS0SQP0OZ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 4XML39DNgzFaZLPtr2DeV)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: FL5YhVP7_zR6Wgp-7XMpS)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: E6ZLZPJ_6Zcq16YqeWtwd)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: vysSkChgp61TcAUwVt8Vz)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: D023rP5ADZ-XxwnrTg7nY)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: RwQ7CUQZ1GWNUcKPpN3Yz)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: jYESheyNfmPh63--GCIgN)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: QkOIWe_-gCqru0JfahLAL)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: frSZQOZQ2Ur9Dg1LdptBa)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: vkA0QY9pukoFvfGWelQH5)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: zkQhBwm8qF6JQpWIAip72)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Z5ged7ks7GECU27q-jGTt)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: tiwG_K_8wxoMK_6GALBtx)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 5LwGJoMHm_HYsDtxzc8zk)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: j5OnP0fTAXTcFJkOvmaJN)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: cFFImn-Ny2lm8prFlegBO)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: MZw4CupcpnOU47i1ZsfGA)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 8pIYmtKZXIlqN_hkjTALo)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: CiyhkUAz5W1GHHyLB4Js2)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: iqUPLW3LmZWKvF6sbmnRX)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 7xIVCb36lG6iP12-N517B)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: LDzEi-LOI9e5BMVUEOiz-)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 2c0k5J659PLvDi_fZjXXe)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 2_ueYg1OUpAPfixdFmNt8)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ugnOre6xzSHhFftJNBshv)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: plDjNRfJQLyrHPK_vzEo8)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 1S-T3ljvqC0WbEUWyFJ3A)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xNhuZL2LLlYH5oIwiisMN)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: AkN3fwAtHnUdHI6ATCtBJ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: l2ibsVo9DVsonweTs_k9Z)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Nou4d7v5CSlnWYbsK9Ndn)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ryrDugthFjlFVPyIjGN_B)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: N7UvQA0G6ZcZ4aUD1wgYL)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 8OLXiSTyYjKHTc2G9DtjQ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: bYHJmOrngaGpNgzTtshO-)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: UmreZTItpS1sn9b6Dug-P)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: tF0cz_kDpZGvi4WfhQ16I)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: nf5Fjr0c6TfeOdTtsL7sW)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: WuEJawRZQ_vhfXnOe2wZw)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: w1LvijR2Or7CYuLzAWpsR)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 9bWD-96rZwrIwvDdQksmi)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 50Ll7neSnsoN1ShpU-_Aq)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: _SVZxutzTZXC0En56ixBE)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: t6tQGoEHlT4n5WGNRLGCa)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: -EP6YGk12IhRKHFMbAckV)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: qtiqQppPpiHHvjccaGfax)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xkOGMA65ILQ_1eUiC-yx0)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 18jIxuSk3_baX7TvvkYp9)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: YNE7WO0O4je92phsga0h6)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Nr__NrXoNM3KGDzWjGVvD)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: QTfAwquSz6pNDm25S9XeK)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: YScrj2kReu40B9CudKyy2)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 5zDuc3fvWL3kVl1Xsh8vB)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: j9prVYesa6ZU42ZQB4Sbh)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: spgivGQd-mBQwRvuFhEjV)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Z2D6OnisEHlTQfBg3Uf-r)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: V-iB_AMQqYZKhepS7nvUX)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: izUZLDw1G_g4xwLjOl0ev)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: oINLlwkpnReGpt9aoP3Wh)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: BiK8LZdy-G8LTcPDQz2dz)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: RPdkHw8B-vy1cZRg4msn7)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 1yGe_Zt0ggjEgWpBK2c4f)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 0XuNRxlJXEEy-RFT5QO8P)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: rqnFwX2VM1nOZHZKzxzk_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: sEZ0x1u7fq_yPcUxfFehZ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: XzBnk6eURWDh_exqTa537)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xZ5UgH_15iv9PTSQwWUu1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: eXWVuDPx4oof_asX_z58a)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: hOwcxTWwMzoviL3sFmfqG)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 14Ja23hSdgS3tZUP3i_dh)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: mTDdzbaU8-9-L02ikzo2C)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: L0fi0-r4ZJOzCm-N_Duz3)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Jn2Td3tTcMPexB88K4R4y)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: xeTuiBjcJlNtEo3KAudmO)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: gagPsyyBIVLcU9zwZiMIl)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: cHhWqeQsfcZ7sOJ1uRCmi)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: idb7egYmT6GIhhR44ATs3)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: SqtUOMQcCVb4BNZ1ihrhs)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: GovLlkUwUxvTVMHNdh7iv)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: sV0VYVy8u75pUo14SrRmw)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: i4ZHExRKIHxs7AzY7TyAy)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: g9CpgpIgyH94aWq3KRZCu)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: EC6KcurHkkyQtu8mJEEev)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: it5hD6cyRb1itoK_RcFDD)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: VFjnSo8ydfY10V21ojNBs)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 8bibt0ioyg6XNSwvUzP9H)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: sW1-NwU9KBn-L3SCsFDjn)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: FeA1Ltdh1y4JKpuiITpr8)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 94VXe_kVkaga3ivLa5a3K)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: rD2i27UNj0mB8APiv9q5p)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: kB22GFtYc1xhVvv1mxicR)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: QcMMeBHSanhfEvDpkTRqY)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: TRNGFnD0ER6U83dKcGz_5)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: dKQi3H0be1Immr8yiRCte)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: WXZqnrEqde2Pxzw24-SXs)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: eh99kEUrksGWLuwUW4Emf)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: vPBQZ2P-i1fAgyNyE6IhT)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: O__SSjdQx1-3qbn-f4Q_w)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ra1yzeMU5t5tm0ru_NFGc)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 5aeX4zWA4ZkXe35jBA2dC)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: aZeJxlgQH2StNERNWV73L)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: TPVGWQCkTvyX7jbJfveOo)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: LmQ7xDrz03913Hjw_QqHb)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: L1FvzAZMy0gdk5ZaRum1n)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: c9E-MZbIlpZIK8zU9SLNG)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: SVE30jw_R8AlD1KQloHY1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: L1gVFN3FDJB9PzJyeFFJi)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: VVFDsSBE4FhHgH9QyYrcm)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: WuuOMY94rVZj1gTdisbtL)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ijDXmWdeieqo05OPyJ9dO)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ceW6qT4C08erjxwPVJMOA)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: WLLL_5vZ57BZJtmxwx0Cj)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: M8HZr6wjMheUVLc9Indib)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: zJ29Ta1rWZHW-AJbLXrdZ)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: E7D71jlQvelh329qyA0L3)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 1sD3xWnwKCxiH1Ey4PFOf)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: HCeVCPqOBO6gNnKgWi4lU)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: hwfBU7zWTJ8VC6dCmPJ6-)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: syMAn8wI9fNLjPhvRmYbf)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: rqIgFJKRb8oZATbFPSs25)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 8cSoVOHzwp47KRcMsTCA-)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: vA8XOLbwsi5zT9x4jYGhg)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: EMhYE64AV9t-LB7u2pEmn)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: AUpDCdQpbXp8nzWkP3yb5)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: vl5HCiqHaowTbWElK86ss)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: lr9Zn3MQYmAQYU-qrRmLw)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: l9rYVsIn253AnaLa3wZXX)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: ZRlaXpcT5Lb2vjJGLy4OY)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 4Mopj1H5NAyojZNdRlcoA)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: C2SA_nv1XXNR1VQyZPxkF)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: DIpZy3DMPebaMggfo5MKg)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: gfozTQMO_-TM6ar4A8VMS)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: L538eyIryiB53rJgXkot4)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: hDZZ2fD55KPh8NQUGJLns)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: cvN83oa5y6IxT-dKO0I9_)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: MFWyh19TMgerJJY9y-Kid)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: HCUEV2KeOjiJq_PWAKUyX)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 0MViWwGtr0m9c2NAGSMAK)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: wW-mJeCSOTbm4iNhAJED1)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: lzCO0eGX20AJvUyB7b3z9)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: -FfVV_dre7LuLm8fUW-TK)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: MQRth7pwDSWK7VLxdET6M)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: fHCySZjaJCaLVsqAtbM9d)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n","Error processing document: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: cOjZUBHYuscIkRzfnWzAx)\n","\n","Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n"]}]},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"54ronUJilR67","executionInfo":{"status":"ok","timestamp":1722405056924,"user_tz":-330,"elapsed":3,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["with open(\"Course 5 - SemanticSpotter /Final_Project/Documents2.pkl\", \"wb\") as file:\n","  pickle.dump(Documents2, file)"],"metadata":{"id":"Mn-LUu-zlrqB","executionInfo":{"status":"ok","timestamp":1722405056924,"user_tz":-330,"elapsed":2,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["with open(\"Course 5 - SemanticSpotter /Final_Project/Documents2.pkl\", \"rb\") as file:\n","  x=pickle.load(file)"],"metadata":{"id":"K8hVMlQllz5b","executionInfo":{"status":"ok","timestamp":1722405056924,"user_tz":-330,"elapsed":2,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["len(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQdNBpD_l8wq","executionInfo":{"status":"ok","timestamp":1722405056924,"user_tz":-330,"elapsed":2,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"c8542b07-2faf-4475-98d6-afecaf8d9123"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["289"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["## Embedding"],"metadata":{"id":"vDjgUe_hhhPI"}},{"cell_type":"code","source":["from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","\n","model_name = \"BAAI/bge-large-en-v1.5\"\n","model_kwargs = {\"device\": \"cpu\"}\n","encode_kwargs = {\"normalize_embeddings\": True}\n","\n","hf = HuggingFaceBgeEmbeddings(\n","    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",")"],"metadata":{"id":"iDBnYsnxnfpW","executionInfo":{"status":"ok","timestamp":1722405107840,"user_tz":-330,"elapsed":50917,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"colab":{"base_uri":"https://localhost:8080/","height":455,"referenced_widgets":["260dc09cfd084e39b0353cf0bfb46d54","4b3382c482734a87b8f2ec62fd9431d8","9ffc4e42326844c9b1f1563122ad738d","b175e995e8694a448325ad2fc1e9d410","a758f340857840dd96e59bb9d51cb3a9","c6a0102bc6e741c8bd6bd03c8a9f0583","fc14a9fe94cc446d8e96d3adb7a4ad16","8529f352807a48a799c8dc28ccd934e9","7e9005ddc5a14ad09b37a5211440c099","37730e1dc7784b85b00f3283d45f5739","0a7d895ba997491a9b4c7fbd814e58b0","4764a572c0c641cf800a11d73dfb43b7","c40b6effa729441e8e5a679409583d1f","41c75f7079594fa0b8e29ac31a43e528","66d8efdf67f447a7844a35cb7955af4d","ada382641da64902bbd18856e4afd13f","94cf118b3ce9479a9aaa518903b9999f","6360b54750034c75b7be9bdde3cac7f9","5f8733cb021f414b9594f94db35ca5a4","c4beff1346b64caa911a3938d06385f3","f5d59a85687d479b9bbaec70784b48a3","63102f11f33e46efa3e5787ea4e1a27a","83d7f8bbbea5484ba47004b5f20c8742","76b1e496af32491bbdd5272dc683c90f","b1629993b0504943ab706e0ddf9063aa","2190b4135853473b9a5a5610d6185201","998fd66e0e834d4096b2caf93b726b7e","c2759f30be5a4e3a9fea9b84fe2eaf1b","6b7aa99201f644dc958fbf599ec1b119","7ecf358ade1b495f85b466e3b87d5e27","6ed47004f08742bf8f25faae9669c4b3","59a4118732914fb2ae0332aed4130b62","b10e9d9dcb544a57a2be4075ef6a7b2f","50c45f92fea3470ebc514a4b76018816","12ab644a3fef4e5e8db9bac78e7f3e32","f68d1325fd1f4f4e8746705306b738bd","90230940e2ce432b839b8418aaccc3c4","92ed089d23f743769ed41133f8cdd29e","ad52d9d85f7843368de767cc93b37c19","26f0b0e8227349c09d5515fd32aad459","9f83b00560914fa2ad13e8b9851fc1c9","a09dbf1a6f794865843d0d1ae25945f9","7bd580c1e8fa44c297c1db5c38d8856d","6d65c27b1f80452baddc117a4b370af6","c257ac7cbad641e880f2b87055ea79d0","c63521309b07460bba58c740700c1c73","245d3a63d0144a688c94211fb0a04576","6371a60f697a42cfbd2c4d0b60512e60","037dc1196793444baad9e9c989d45aa9","c377718d6e29406387c434660f3b7c07","81b37f5c3fe04f79bed75fdec9faa994","18a416573f864e82b5dbfb7285c07888","1894cd22fb104ce599c994da440e8ff5","08cfd79236734b2eaced2ccca7c3096c","d419839d49714a40b507b2e2fa43b841","817fb1800c07446bb3719260eac2d9d1","adaceb2fede5403895132b373bfec869","705e70006b80435c83998b9c8f5fb8d4","120635bb691441ef987312eb166cfa7e","b0d8dc5e1f3c46f5844197fff82f12ad","e2367bdb1c384985be915ed87669967d","3bcb5855ba604431823c4ebd56510ba1","409a5629168649f78d5981b84266a069","801f2981c0b34685bb0d70f957ec9b15","9e38725c33a0494e9a4228c148c37d0f","7f542eb9d7bb42258927c5d4f8c62205","ef3474be4aff4966b7d9687125cbe9a6","856144115b894206aacc319bef8adfe9","dba9e1397a2249559824ef4ddc59330b","f651eb710dff49acae05ed3b4a4d451e","a06ec23c749f40f3b42024375d6a8c8a","569c8a05973847569d42865196b50415","841d8e2ddb3e4d5d8b7310b2933dfc3e","ef1a6a04f63e4d52935af2d89d83c28d","cd5acce5364f4f79818ab622076db35b","072ed81db91a4a46bcdb38ac770c3f6c","64348bf2d8b54bd8b6b43dbe0d5a74d4","6be3a732a9374f228ae65dc598b35178","d84c87949edc493bb512da6c01c06fe4","50e2782867e548e89debee2a53df0fea","90bbf2262c28462a974292f963b66202","884f677ecadd4763987b9d2185cb9ddb","0feacd333ccb444994b4f8ec843982f5","697a186e2e3d487d9496eaae4ede6921","3c56d127dc044f2cb9a78fcb0d974764","3ba2ca6da7494402a0f8adf6c92a6d23","289272d831da43a88809b5fb0d043949","ad9a0b185b4e400b92028fd4e81d721f","7957c5419a8844cba0533dd809ecb540","ddbea77cd12f4936a495b6468c974ad8","ed9ba31fdf434da898c3d604392335dc","537bd3f74a364379854c6f67b051db85","0736212120634877b559ce692ec916db","e7ec5f2dea6a4384b945faaed42a0c5c","6ad59bfb7f004a73bcbffca0f6bacc16","da27d2bc72124d4084b50e3b2788b48b","3880470ecc09417785703bf60071441b","564e5906d30d48cbb3af8502392ae6c2","40c961078fd144229b5a2e2a29de64cc","dac5e2b0f4a34507a45936bc641b9ba1","59dad249ea89493a821d29e6c3c1cc9d","b6fe2c8235fb40e0a7771a828314ef78","412cfcb8e7684c08a589607856492cd2","06fa8b66009d49579a3a8103ce4cfa7f","0aa2d52ef22a40ec8124080b293f97c5","60d4c129877f47d2a6f1355b4d877121","186f76ba58b9450d96ef587e95ff11c9","24d5cd0f9a4b4a83b164a1cf438d10ea","7f80e08154c44276b528e8665108840c","e3fecd3dfbb9431f9d99d5d23b8bcbda","8069b56d25be467c89a44de9c1c35489","4447af7b80884ff884bf7e00bcd334f7","ecce0e898cbc4fde817518ae6d06d307","f7ee2a38bdda426bb4967eb54c18359c","5456f87ff727437e942d7aad6a07d8e0","7405576c1f854ba7b5e150bd393b49fc","9250909f1df64cfb9506c519309a7f13","98d4ba58225049e395575abfe3e537e3","36c04320bb084e8d991f377c84a84136","8fd790d7c69142b2b0f725ecd6dd2fa5","34280fb96a84410eaffbf3728fe15d48"]},"outputId":"09b2a332-2175-4fb6-95fc-3d32c9e7f141"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260dc09cfd084e39b0353cf0bfb46d54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4764a572c0c641cf800a11d73dfb43b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d7f8bbbea5484ba47004b5f20c8742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c45f92fea3470ebc514a4b76018816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c257ac7cbad641e880f2b87055ea79d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"817fb1800c07446bb3719260eac2d9d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3474be4aff4966b7d9687125cbe9a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6be3a732a9374f228ae65dc598b35178"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7957c5419a8844cba0533dd809ecb540"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac5e2b0f4a34507a45936bc641b9ba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8069b56d25be467c89a44de9c1c35489"}},"metadata":{}}]},{"cell_type":"code","source":["embedding = hf.embed_query(Documents[3].page_content)\n","len(embedding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uu6qvZ6OpZTF","executionInfo":{"status":"ok","timestamp":1722405109256,"user_tz":-330,"elapsed":1416,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"06b88d8c-abb7-413e-9ccb-ab7bb0171aba"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["## Saving in VectorDB"],"metadata":{"id":"uP5qtsYqtGLJ"}},{"cell_type":"code","source":["from langchain_chroma import Chroma"],"metadata":{"id":"uZVXNPvTtKcA","executionInfo":{"status":"ok","timestamp":1722405109687,"user_tz":-330,"elapsed":432,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["os.listdir('Course 5 - SemanticSpotter /Final_Project')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCfivcKBWBQO","executionInfo":{"status":"ok","timestamp":1722405109687,"user_tz":-330,"elapsed":2,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"bd514926-067a-4c68-a6b8-b1d8e0d89d1e"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['all_files.pkl', '.ipynb_checkpoints', 'chroma_db', 'Documents2.pkl']"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# import shutil\n","# shutil.rmtree('Course 5 - SemanticSpotter /Final_Project/chroma_db')"],"metadata":{"id":"U0P516zZZGXS","executionInfo":{"status":"ok","timestamp":1722405109687,"user_tz":-330,"elapsed":0,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# os.rmdir('Course 5 - SemanticSpotter /Final_Project/chroma_db')"],"metadata":{"id":"CWfIqyEUZNC4","executionInfo":{"status":"ok","timestamp":1722405109687,"user_tz":-330,"elapsed":0,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["update_chroma_db = True"],"metadata":{"id":"6zbNnFC3ktxK","executionInfo":{"status":"ok","timestamp":1722405109687,"user_tz":-330,"elapsed":0,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["if update_chroma_db :\n","  db = Chroma.from_documents(\n","      Documents2,\n","      embedding_function=hf,\n","      persist_directory=\"Course 5 - SemanticSpotter /Final_Project/chroma_db_new\"\n","  )\n","else :\n","  # load from disk\n","  db = Chroma(\n","      persist_directory=\"Course 5 - SemanticSpotter /Final_Project/chroma_db_new\",\n","      embedding_function=hf\n","  )"],"metadata":{"id":"jmje63EJtLuI","executionInfo":{"status":"error","timestamp":1722405110184,"user_tz":-330,"elapsed":497,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"5488267b-8cd7-4986-98f2-4343d7ab5acc"},"execution_count":55,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"langchain_chroma.vectorstores.Chroma() got multiple values for keyword argument 'embedding_function'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-79160787fb08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupdate_chroma_db\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   db = Chroma.from_documents(\n\u001b[0m\u001b[1;32m      3\u001b[0m       \u001b[0mDocuments2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Course 5 - SemanticSpotter /Final_Project/chroma_db_new\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChroma\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \"\"\"\n\u001b[0;32m--> 854\u001b[0;31m         chroma_collection = cls(\n\u001b[0m\u001b[1;32m    855\u001b[0m             \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: langchain_chroma.vectorstores.Chroma() got multiple values for keyword argument 'embedding_function'"]}]},{"cell_type":"code","source":["query = \"What is the prameter boosters?\""],"metadata":{"id":"A6pOuyaflgHB","executionInfo":{"status":"aborted","timestamp":1722405110184,"user_tz":-330,"elapsed":1,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = db.similarity_search(query,k=3)"],"metadata":{"id":"s_g_ouIHq0yT","executionInfo":{"status":"aborted","timestamp":1722405110184,"user_tz":-330,"elapsed":1,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretty_print_docs(docs)"],"metadata":{"id":"qL7psA3JyHTd","executionInfo":{"status":"aborted","timestamp":1722405110185,"user_tz":-330,"elapsed":1,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Retrivers"],"metadata":{"id":"-oGAQBwzwwC4"}},{"cell_type":"code","source":["from google.colab import userdata\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"],"metadata":{"id":"y6t-D57Pnz1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint\n","\n","repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","# repo_id = 'dunzhang/stella_en_1.5B_v5'\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id=repo_id,\n","    task=\"text-generation\",\n","    max_new_tokens=512,\n","    do_sample=False,\n","    repetition_penalty=1.03,\n","    temperature = 0.8\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzQk9xbWoJW-","executionInfo":{"status":"ok","timestamp":1722347254369,"user_tz":-330,"elapsed":2,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"68c4e536-e036-4d1f-b213-135c6d8ea006"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["llm.invoke(\"Howdy?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"cnBe_BTExdBy","executionInfo":{"status":"ok","timestamp":1722347260853,"user_tz":-330,"elapsed":6486,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"c6619d6c-cca8-4469-be0a-52a32965031f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" I'm a software developer based in Berlin, Germany.\\n\\nI have been developing software for more than 20 years now, mostly in the area of CAD and CAE applications for various industries. In my most recent project, I was working as a CTO at CamptoCamp, where I was leading the development of a cloud-based 3D modelling application for the outdoor industry. We recently finished the beta phase and have just launched our application to the public. I am currently taking a break from the daily work life and am trying out some new projects.\\n\\nI enjoy learning about new technologies, programming paradigms and algorithms. In my spare time, I like to spend some time on my hobby projects. I've also written some articles on the web and have presented at conferences.\\n\\nI've developed software for various platforms and languages, among them C++, Java, JavaScript, Python, PHP, Ruby, Rust and Go. I am always happy to learn new things and to explore new technologies.\\n\\nIf you have any questions or want to discuss a project, feel free to contact me.\\n\\n## Skills\\n\\nI have worked extensively with the following technologies and frameworks:\\n\\n* C++ (11 and later)\\n* CMake\\n* Java\\n* Spring (MVC, Boot)\\n* JavaScript\\n* Angular, React, Redux\\n* CSS and Sass\\n* HTML5/HTML4\\n* Ruby on Rails\\n* PHP and Laravel\\n* Python\\n* SQL and NoSQL databases (MySQL, PostgreSQL, MongoDB)\\n* Git\\n* Docker, Kubernetes, AWS, Azure\\n* HTML5 Canvas\\n* WebGL, Three.js\\n\\nI'm also familiar with various design patterns, data structures and algorithms.\\n\\n## Projects\\n\\nHere's a list of some projects I've done over the last few years:\\n\\n### CamptoCamp\\n\\nCamptoCamp is a cloud-based 3D modeling application for the outdoor industry. It allows users to create and edit campgrounds, caravan sites, hiking trails, bike routes, and other outdoor recreational areas. Users can search for locations, import and export GPX files, share their creations, and use an integrated photo editor to add pictures to their designs. The application is written in a mix of Ruby, Python and JavaScript\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["from langchain.retrievers.multi_query import MultiQueryRetriever\n","\n","question = \"What is the parameter booster do in XGBOOST?\"\n","\n","retriever_from_llm = MultiQueryRetriever.from_llm(\n","    retriever=db.as_retriever(), llm=llm\n",")"],"metadata":{"id":"qYo9NcRzsvn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever_from_llm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2tboK6jtDJn","executionInfo":{"status":"ok","timestamp":1722347261534,"user_tz":-330,"elapsed":4,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"71976971-5cf7-4c56-9eab-03d31cc8e436"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiQueryRetriever(retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7944880b0910>), llm_chain=PromptTemplate(input_variables=['question'], template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}')\n","| HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', repetition_penalty=1.03, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, task='text-generation')\n","| LineListOutputParser())"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# Set logging for the queries\n","import logging\n","\n","logging.basicConfig()\n","logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"],"metadata":{"id":"WIuJ5GFKtg9y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrived_docs = retriever_from_llm.invoke(question)\n","len(retrived_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Izz2NDOgtQ--","executionInfo":{"status":"ok","timestamp":1722347263181,"user_tz":-330,"elapsed":1650,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"27ccc738-c241-4142-b98f-126ae1b8d7dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['Version 1:', 'Can you explain the role of the booster parameter in XGBoost?', 'Version 2:', \"How does XGBoost's booster algorithm function, and what impact does it have on the model?\", 'Version 3:', \"In XGBoost, what is the significance of the booster component, and how does it contribute to the model's performance?\"]\n"]},{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["pretty_print_docs(retrived_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WG2JPuiPsbjx","executionInfo":{"status":"ok","timestamp":1722347263181,"user_tz":-330,"elapsed":10,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"65e7eaf0-e762-4ab9-bf51-620c29a9852a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Document 1:\n","\n","\n","\n","0\tcap-shape=bell\ti\n","1\tcap-shape=conical\ti\n","2\tcap-shape=convex\ti\n","3\tcap-shape=flat\ti\n","4\tcap-shape=knobbed\ti\n","5\tcap-shape=sunken\ti\n","6\tcap-surface=fibrous\ti\n","7\tcap-surface=grooves\ti\n","8\tcap-surface=scaly\ti\n","9\tcap-surface=smooth\ti\n","10\tcap-color=brown\ti\n","11\tcap-color=buff\ti\n","12\tcap-color=cinnamon\ti\n","13\tcap-color=gray\ti\n","14\tcap-color=green\ti\n","15\tcap-color=pink\ti\n","16\tcap-color=purple\ti\n","17\tcap-color=red\ti\n","18\tcap-color=white\ti\n","19\tcap-color=yellow\ti\n","20\tbruises?=bruises\ti\n","21\tbruises?=no\ti\n","22\todor=almond\ti\n","23\todor=anise\ti\n","24\todor=creosote\ti\n","25\todor=fishy\ti\n","26\todor=foul\ti\n","27\todor=musty\ti\n","28\todor=none\ti\n","29\todor=pungent\ti\n","30\todor=spicy\ti\n","31\tgill-attachment=attached\ti\n","32\tgill-attachment=descending\ti\n","33\tgill-attachment=free\ti\n","34\tgill-attachment=notched\ti\n","35\tgill-spacing=close\ti\n","36\tgill-spacing=crowded\ti\n","37\tgill-spacing=distant\ti\n","38\tgill-size=broad\ti\n","39\tgill-size=narrow\ti\n","40\tgill-color=black\ti\n","41\tgill-color=brown\ti\n","42\tgill-color=buff\ti\n","43\tgill-color=chocolate\ti\n","44\tgill-color=gray\ti\n","45\tgill-color=green\ti\n","46\tgill-color=orange\ti\n","47\tgill-color=pink\ti\n","48\tgill-color=purple\ti\n","49\tgill-color=red\ti\n","50\tgill-color=white\ti\n","51\tgill-color=yellow\ti\n","52\tstalk-shape=enlarging\ti\n","53\tstalk-shape=tapering\ti\n","54\tstalk-root=bulbous\ti\n","55\tstalk-root=club\ti\n","56\tstalk-root=cup\ti\n","57\tstalk-root=equal\ti\n","58\tstalk-root=rhizomorphs\ti\n","59\tstalk-root=rooted\ti\n","60\tstalk-root=missing\ti\n","61\tstalk-surface-above-ring=fibrous\ti\n","62\tstalk-surface-above-ring=scaly\ti\n","63\tstalk-surface-above-ring=silky\ti\n","64\tstalk-surface-above-ring=smooth\ti\n","65\tstalk-surface-below-ring=fibrous\ti\n","66\tstalk-surface-below-ring=scaly\ti\n","67\tstalk-surface-below-ring=silky\ti\n","68\tstalk-surface-below-ring=smooth\ti\n","69\tstalk-color-above-ring=brown\ti\n","70\tstalk-color-above-ring=buff\ti\n","71\tstalk-color-above-ring=cinnamon\ti\n","72\tstalk-color-above-ring=gray\ti\n","73\tstalk-color-above-ring=orange\ti\n","74\tstalk-color-above-ring=pink\ti\n","75\tstalk-color-above-ring=red\ti\n","76\tstalk-color-above-ring=white\ti\n","77\tstalk-color-above-ring=yellow\ti\n","78\tstalk-color-below-ring=brown\ti\n","79\tstalk-color-below-ring=buff\ti\n","80\tstalk-color-below-ring=cinnamon\ti\n","81\tstalk-color-below-ring=gray\ti\n","82\tstalk-color-below-ring=orange\ti\n","83\tstalk-color-below-ring=pink\ti\n","84\tstalk-color-below-ring=red\ti\n","85\tstalk-color-below-ring=white\ti\n","86\tstalk-color-below-ring=yellow\ti\n","87\tveil-type=partial\ti\n","88\tveil-type=universal\ti\n","89\tveil-color=brown\ti\n","90\tveil-color=orange\ti\n","91\tveil-color=white\ti\n","92\tveil-color=yellow\ti\n","93\tring-number=none\ti\n","94\tring-number=one\ti\n","95\tring-number=two\ti\n","96\tring-type=cobwebby\ti\n","97\tring-type=evanescent\ti\n","98\tring-type=flaring\ti\n","99\tring-type=large\ti\n","100\tring-type=none\ti\n","101\tring-type=pendant\ti\n","102\tring-type=sheathing\ti\n","103\tring-type=zone\ti\n","104\tspore-print-color=black\ti\n","105\tspore-print-color=brown\ti\n","106\tspore-print-color=buff\ti\n","107\tspore-print-color=chocolate\ti\n","108\tspore-print-color=green\ti\n","109\tspore-print-color=orange\ti\n","110\tspore-print-color=purple\ti\n","111\tspore-print-color=white\ti\n","112\tspore-print-color=yellow\ti\n","113\tpopulation=abundant\ti\n","114\tpopulation=clustered\ti\n","115\tpopulation=numerous\ti\n","116\tpopulation=scattered\ti\n","117\tpopulation=several\ti\n","118\tpopulation=solitary\ti\n","119\thabitat=grasses\ti\n","120\thabitat=leaves\ti\n","121\thabitat=meadows\ti\n","122\thabitat=paths\ti\n","123\thabitat=urban\ti\n","124\thabitat=waste\ti\n","125\thabitat=woods\ti\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/demo/data/featmap.txt?ref=master', 'name': 'featmap.txt', 'path': 'demo/data/featmap.txt', 'source': 'https://github.com/dmlc/xgboost/blob/master/demo/data/featmap.txt'}\n","----------------------------------------------------------------------------------------------------\n","Document 2:\n","\n","\n","\n","Survival_label_lower_bound,Survival_label_upper_bound,Age_in_years,Karnofsky_score,Months_from_Diagnosis,Celltype=adeno,Celltype=large,Celltype=smallcell,Celltype=squamous,Prior_therapy=no,Prior_therapy=yes,Treatment=standard,Treatment=test\n","72.0,72.0,69.0,60.0,7.0,0,0,0,1,1,0,1,0\n","411.0,411.0,64.0,70.0,5.0,0,0,0,1,0,1,1,0\n","228.0,228.0,38.0,60.0,3.0,0,0,0,1,1,0,1,0\n","126.0,126.0,63.0,60.0,9.0,0,0,0,1,0,1,1,0\n","118.0,118.0,65.0,70.0,11.0,0,0,0,1,0,1,1,0\n","10.0,10.0,49.0,20.0,5.0,0,0,0,1,1,0,1,0\n","82.0,82.0,69.0,40.0,10.0,0,0,0,1,0,1,1,0\n","110.0,110.0,68.0,80.0,29.0,0,0,0,1,1,0,1,0\n","314.0,314.0,43.0,50.0,18.0,0,0,0,1,1,0,1,0\n","100.0,inf,70.0,70.0,6.0,0,0,0,1,1,0,1,0\n","42.0,42.0,81.0,60.0,4.0,0,0,0,1,1,0,1,0\n","8.0,8.0,63.0,40.0,58.0,0,0,0,1,0,1,1,0\n","144.0,144.0,63.0,30.0,4.0,0,0,0,1,1,0,1,0\n","25.0,inf,52.0,80.0,9.0,0,0,0,1,0,1,1,0\n","11.0,11.0,48.0,70.0,11.0,0,0,0,1,0,1,1,0\n","30.0,30.0,61.0,60.0,3.0,0,0,1,0,1,0,1,0\n","384.0,384.0,42.0,60.0,9.0,0,0,1,0,1,0,1,0\n","4.0,4.0,35.0,40.0,2.0,0,0,1,0,1,0,1,0\n","54.0,54.0,63.0,80.0,4.0,0,0,1,0,0,1,1,0\n","13.0,13.0,56.0,60.0,4.0,0,0,1,0,1,0,1,0\n","123.0,inf,55.0,40.0,3.0,0,0,1,0,1,0,1,0\n","97.0,inf,67.0,60.0,5.0,0,0,1,0,1,0,1,0\n","153.0,153.0,63.0,60.0,14.0,0,0,1,0,0,1,1,0\n","59.0,59.0,65.0,30.0,2.0,0,0,1,0,1,0,1,0\n","117.0,117.0,46.0,80.0,3.0,0,0,1,0,1,0,1,0\n","16.0,16.0,53.0,30.0,4.0,0,0,1,0,0,1,1,0\n","151.0,151.0,69.0,50.0,12.0,0,0,1,0,1,0,1,0\n","22.0,22.0,68.0,60.0,4.0,0,0,1,0,1,0,1,0\n","56.0,56.0,43.0,80.0,12.0,0,0,1,0,0,1,1,0\n","21.0,21.0,55.0,40.0,2.0,0,0,1,0,0,1,1,0\n","18.0,18.0,42.0,20.0,15.0,0,0,1,0,1,0,1,0\n","139.0,139.0,64.0,80.0,2.0,0,0,1,0,1,0,1,0\n","20.0,20.0,65.0,30.0,5.0,0,0,1,0,1,0,1,0\n","31.0,31.0,65.0,75.0,3.0,0,0,1,0,1,0,1,0\n","52.0,52.0,55.0,70.0,2.0,0,0,1,0,1,0,1,0\n","287.0,287.0,66.0,60.0,25.0,0,0,1,0,0,1,1,0\n","18.0,18.0,60.0,30.0,4.0,0,0,1,0,1,0,1,0\n","51.0,51.0,67.0,60.0,1.0,0,0,1,0,1,0,1,0\n","122.0,122.0,53.0,80.0,28.0,0,0,1,0,1,0,1,0\n","27.0,27.0,62.0,60.0,8.0,0,0,1,0,1,0,1,0\n","54.0,54.0,67.0,70.0,1.0,0,0,1,0,1,0,1,0\n","7.0,7.0,72.0,50.0,7.0,0,0,1,0,1,0,1,0\n","63.0,63.0,48.0,50.0,11.0,0,0,1,0,1,0,1,0\n","392.0,392.0,68.0,40.0,4.0,0,0,1,0,1,0,1,0\n","10.0,10.0,67.0,40.0,23.0,0,0,1,0,0,1,1,0\n","8.0,8.0,61.0,20.0,19.0,1,0,0,0,0,1,1,0\n","92.0,92.0,60.0,70.0,10.0,1,0,0,0,1,0,1,0\n","35.0,35.0,62.0,40.0,6.0,1,0,0,0,1,0,1,0\n","117.0,117.0,38.0,80.0,2.0,1,0,0,0,1,0,1,0\n","132.0,132.0,50.0,80.0,5.0,1,0,0,0,1,0,1,0\n","12.0,12.0,63.0,50.0,4.0,1,0,0,0,0,1,1,0\n","162.0,162.0,64.0,80.0,5.0,1,0,0,0,1,0,1,0\n","3.0,3.0,43.0,30.0,3.0,1,0,0,0,1,0,1,0\n","95.0,95.0,34.0,80.0,4.0,1,0,0,0,1,0,1,0\n","177.0,177.0,66.0,50.0,16.0,0,1,0,0,0,1,1,0\n","162.0,162.0,62.0,80.0,5.0,0,1,0,0,1,0,1,0\n","216.0,216.0,52.0,50.0,15.0,0,1,0,0,1,0,1,0\n","553.0,553.0,47.0,70.0,2.0,0,1,0,0,1,0,1,0\n","278.0,278.0,63.0,60.0,12.0,0,1,0,0,1,0,1,0\n","12.0,12.0,68.0,40.0,12.0,0,1,0,0,0,1,1,0\n","260.0,260.0,45.0,80.0,5.0,0,1,0,0,1,0,1,0\n","200.0,200.0,41.0,80.0,12.0,0,1,0,0,0,1,1,0\n","156.0,156.0,66.0,70.0,2.0,0,1,0,0,1,0,1,0\n","182.0,inf,62.0,90.0,2.0,0,1,0,0,1,0,1,0\n","143.0,143.0,60.0,90.0,8.0,0,1,0,0,1,0,1,0\n","105.0,105.0,66.0,80.0,11.0,0,1,0,0,1,0,1,0\n","103.0,103.0,38.0,80.0,5.0,0,1,0,0,1,0,1,0\n","250.0,250.0,53.0,70.0,8.0,0,1,0,0,0,1,1,0\n","100.0,100.0,37.0,60.0,13.0,0,1,0,0,0,1,1,0\n","999.0,999.0,54.0,90.0,12.0,0,0,0,1,0,1,0,1\n","112.0,112.0,60.0,80.0,6.0,0,0,0,1,1,0,0,1\n","87.0,inf,48.0,80.0,3.0,0,0,0,1,1,0,0,1\n","231.0,inf,52.0,50.0,8.0,0,0,0,1,0,1,0,1\n","242.0,242.0,70.0,50.0,1.0,0,0,0,1,1,0,0,1\n","991.0,991.0,50.0,70.0,7.0,0,0,0,1,0,1,0,1\n","111.0,111.0,62.0,70.0,3.0,0,0,0,1,1,0,0,1\n","1.0,1.0,65.0,20.0,21.0,0,0,0,1,0,1,0,1\n","587.0,587.0,58.0,60.0,3.0,0,0,0,1,1,0,0,1\n","389.0,389.0,62.0,90.0,2.0,0,0,0,1,1,0,0,1\n","33.0,33.0,64.0,30.0,6.0,0,0,0,1,1,0,0,1\n","25.0,25.0,63.0,20.0,36.0,0,0,0,1,1,0,0,1\n","357.0,357.0,58.0,70.0,13.0,0,0,0,1,1,0,0,1\n","467.0,467.0,64.0,90.0,2.0,0,0,0,1,1,0,0,1\n","201.0,201.0,52.0,80.0,28.0,0,0,0,1,0,1,0,1\n","1.0,1.0,35.0,50.0,7.0,0,0,0,1,1,0,0,1\n","30.0,30.0,63.0,70.0,11.0,0,0,0,1,1,0,0,1\n","44.0,44.0,70.0,60.0,13.0,0,0,0,1,0,1,0,1\n","283.0,283.0,51.0,90.0,2.0,0,0,0,1,1,0,0,1\n","15.0,15.0,40.0,50.0,13.0,0,0,0,1,0,1,0,1\n","25.0,25.0,69.0,30.0,2.0,0,0,1,0,1,0,0,1\n","103.0,inf,36.0,70.0,22.0,0,0,1,0,0,1,0,1\n","21.0,21.0,71.0,20.0,4.0,0,0,1,0,1,0,0,1\n","13.0,13.0,62.0,30.0,2.0,0,0,1,0,1,0,0,1\n","87.0,87.0,60.0,60.0,2.0,0,0,1,0,1,0,0,1\n","2.0,2.0,44.0,40.0,36.0,0,0,1,0,0,1,0,1\n","20.0,20.0,54.0,30.0,9.0,0,0,1,0,0,1,0,1\n","7.0,7.0,66.0,20.0,11.0,0,0,1,0,1,0,0,1\n","24.0,24.0,49.0,60.0,8.0,0,0,1,0,1,0,0,1\n","99.0,99.0,72.0,70.0,3.0,0,0,1,0,1,0,0,1\n","8.0,8.0,68.0,80.0,2.0,0,0,1,0,1,0,0,1\n","99.0,99.0,62.0,85.0,4.0,0,0,1,0,1,0,0,1\n","61.0,61.0,71.0,70.0,2.0,0,0,1,0,1,0,0,1\n","25.0,25.0,70.0,70.0,2.0,0,0,1,0,1,0,0,1\n","95.0,95.0,61.0,70.0,1.0,0,0,1,0,1,0,0,1\n","80.0,80.0,71.0,50.0,17.0,0,0,1,0,1,0,0,1\n","51.0,51.0,59.0,30.0,87.0,0,0,1,0,0,1,0,1\n","29.0,29.0,67.0,40.0,8.0,0,0,1,0,1,0,0,1\n","24.0,24.0,60.0,40.0,2.0,1,0,0,0,1,0,0,1\n","18.0,18.0,69.0,40.0,5.0,1,0,0,0,0,1,0,1\n","83.0,inf,57.0,99.0,3.0,1,0,0,0,1,0,0,1\n","31.0,31.0,39.0,80.0,3.0,1,0,0,0,1,0,0,1\n","51.0,51.0,62.0,60.0,5.0,1,0,0,0,1,0,0,1\n","90.0,90.0,50.0,60.0,22.0,1,0,0,0,0,1,0,1\n","52.0,52.0,43.0,60.0,3.0,1,0,0,0,1,0,0,1\n","73.0,73.0,70.0,60.0,3.0,1,0,0,0,1,0,0,1\n","8.0,8.0,66.0,50.0,5.0,1,0,0,0,1,0,0,1\n","36.0,36.0,61.0,70.0,8.0,1,0,0,0,1,0,0,1\n","48.0,48.0,81.0,10.0,4.0,1,0,0,0,1,0,0,1\n","7.0,7.0,58.0,40.0,4.0,1,0,0,0,1,0,0,1\n","140.0,140.0,63.0,70.0,3.0,1,0,0,0,1,0,0,1\n","186.0,186.0,60.0,90.0,3.0,1,0,0,0,1,0,0,1\n","84.0,84.0,62.0,80.0,4.0,1,0,0,0,0,1,0,1\n","19.0,19.0,42.0,50.0,10.0,1,0,0,0,1,0,0,1\n","45.0,45.0,69.0,40.0,3.0,1,0,0,0,1,0,0,1\n","80.0,80.0,63.0,40.0,4.0,1,0,0,0,1,0,0,1\n","52.0,52.0,45.0,60.0,4.0,0,1,0,0,1,0,0,1\n","164.0,164.0,68.0,70.0,15.0,0,1,0,0,0,1,0,1\n","19.0,19.0,39.0,30.0,4.0,0,1,0,0,0,1,0,1\n","53.0,53.0,66.0,60.0,12.0,0,1,0,0,1,0,0,1\n","15.0,15.0,63.0,30.0,5.0,0,1,0,0,1,0,0,1\n","43.0,43.0,49.0,60.0,11.0,0,1,0,0,0,1,0,1\n","340.0,340.0,64.0,80.0,10.0,0,1,0,0,0,1,0,1\n","133.0,133.0,65.0,75.0,1.0,0,1,0,0,1,0,0,1\n","111.0,111.0,64.0,60.0,5.0,0,1,0,0,1,0,0,1\n","231.0,231.0,67.0,70.0,18.0,0,1,0,0,0,1,0,1\n","378.0,378.0,65.0,80.0,4.0,0,1,0,0,1,0,0,1\n","49.0,49.0,37.0,30.0,3.0,0,1,0,0,1,0,0,1\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/demo/data/veterans_lung_cancer.csv?ref=master', 'name': 'veterans_lung_cancer.csv', 'path': 'demo/data/veterans_lung_cancer.csv', 'source': 'https://github.com/dmlc/xgboost/blob/master/demo/data/veterans_lung_cancer.csv'}\n","----------------------------------------------------------------------------------------------------\n","Document 3:\n","\n","\n","\n","1. cap-shape:                bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s\n","     2. cap-surface:              fibrous=f,grooves=g,scaly=y,smooth=s\n","     3. cap-color:                brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n","     4. bruises?:                 bruises=t,no=f\n","     5. odor:                     almond=a,anise=l,creosote=c,fishy=y,foul=f,\n","                                  musty=m,none=n,pungent=p,spicy=s\n","     6. gill-attachment:          attached=a,descending=d,free=f,notched=n\n","     7. gill-spacing:             close=c,crowded=w,distant=d\n","     8. gill-size:                broad=b,narrow=n\n","     9. gill-color:               black=k,brown=n,buff=b,chocolate=h,gray=g,\n","                                  green=r,orange=o,pink=p,purple=u,red=e,\n","                                  white=w,yellow=y\n","    10. stalk-shape:              enlarging=e,tapering=t\n","    11. stalk-root:               bulbous=b,club=c,cup=u,equal=e,\n","                                  rhizomorphs=z,rooted=r,missing=?\n","    12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n","    13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n","    14. stalk-color-above-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,\n","                                  pink=p,red=e,white=w,yellow=y\n","    15. stalk-color-below-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,\n","                                  pink=p,red=e,white=w,yellow=y\n","    16. veil-type:                partial=p,universal=u\n","    17. veil-color:               brown=n,orange=o,white=w,yellow=y\n","    18. ring-number:              none=n,one=o,two=t\n","    19. ring-type:                cobwebby=c,evanescent=e,flaring=f,large=l,\n","                                  none=n,pendant=p,sheathing=s,zone=z\n","    20. spore-print-color:        black=k,brown=n,buff=b,chocolate=h,green=r,\n","                                  orange=o,purple=u,white=w,yellow=y\n","    21. population:               abundant=a,clustered=c,numerous=n,\n","                                  scattered=s,several=v,solitary=y\n","    22. habitat:                  grasses=g,leaves=l,meadows=m,paths=p,\n","                                  urban=u,waste=w,woods=d\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/demo/CLI/binary_classification/agaricus-lepiota.fmap?ref=master', 'name': 'agaricus-lepiota.fmap', 'path': 'demo/CLI/binary_classification/agaricus-lepiota.fmap', 'source': 'https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/agaricus-lepiota.fmap'}\n","----------------------------------------------------------------------------------------------------\n","Document 4:\n","\n","\n","\n","\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/jvm/javadocs/index.rst?ref=master', 'name': 'index.rst', 'path': 'doc/jvm/javadocs/index.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/jvm/javadocs/index.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 5:\n","\n","XGBOOST PARAMETERS - PARAMETERS FOR TREE BOOSTER\n","\n","[]\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master', 'name': 'parameter.rst', 'path': 'doc/parameter.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 6:\n","\n","XGBOOST PARAMETERS - PARAMETERS FOR TREE BOOSTER - GENERAL PARAMETERS\n","\n","['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']\n","* ``max_depth``:['[default=6]\\n\\n  - Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. ``exact`` tree method requires non-zero value.\\n  - range: [0,∞]']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master', 'name': 'parameter.rst', 'path': 'doc/parameter.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 7:\n","\n","XGBOOST PARAMETERS - PARAMETERS FOR TREE BOOSTER - GENERAL PARAMETERS\n","\n","['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']\n","* ``subsample``:['[default=1]\\n\\n  - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\\n  - range: (0,1]']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master', 'name': 'parameter.rst', 'path': 'doc/parameter.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 8:\n","\n","XGBOOST PARAMETERS - PARAMETERS FOR TREE BOOSTER - GENERAL PARAMETERS\n","\n","['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']\n","* ``eta``:['[default=0.3, alias: ``learning_rate``]\\n\\n  - Step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new features, and ``eta`` shrinks the feature weights to make the boosting process more conservative.\\n  - range: [0,1]']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/parameter.rst?ref=master', 'name': 'parameter.rst', 'path': 'doc/parameter.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 9:\n","\n","\n","\n","\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/jvm/scaladocs/xgboost4j-spark/index.rst?ref=master', 'name': 'index.rst', 'path': 'doc/jvm/scaladocs/xgboost4j-spark/index.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/jvm/scaladocs/xgboost4j-spark/index.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 10:\n","\n"," - INTRODUCTION\n","\n","['**XGBoost** is an implementation of the famous gradient boosting algorithm. This model is often described as a *blackbox*, meaning it works well but it is not trivial to understand how. Indeed, the model is made of hundreds (thousands?) of decision trees. You may wonder how possible a human would be able to have a general view of the model?\\n\\nWhile XGBoost is known for its fast speed and accurate predictive power, it also comes with various functions to help you understand the model.\\nThe purpose of this RMarkdown document is to demonstrate how easily we can leverage the functions already implemented in **XGBoost R** package. Of course, everything showed below can be applied to the dataset you may have to manipulate at work or wherever!\\n\\nFirst we will prepare the **Otto** dataset and train a model, then we will generate two visualisations to get a clue of what is important to the model, finally, we will see how we can leverage these information.']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/demo/kaggle-otto/understandingXGBoostModel.Rmd?ref=master', 'name': 'understandingXGBoostModel.Rmd', 'path': 'demo/kaggle-otto/understandingXGBoostModel.Rmd', 'source': 'https://github.com/dmlc/xgboost/blob/master/demo/kaggle-otto/understandingXGBoostModel.Rmd'}\n","----------------------------------------------------------------------------------------------------\n","Document 11:\n","\n","DART BOOSTER -  - PARAMETERS\n","\n","['XGBoost mostly combines a huge number of regression trees with a small learning rate.\\nIn this situation, trees added early are significant and trees added late are unimportant.\\n\\nVinayak and Gilad-Bachrach proposed a new method to add dropout techniques from the deep neural net community to boosted trees, and reported better results in some situations.\\n\\nThis is a instruction of new tree booster ``dart``.\\n\\n**************']\n","* ``skip_drop``:[': probability of skipping dropout.\\n\\n  - If a dropout is skipped, new trees are added in the same manner as gbtree.\\n  - range: [0.0, 1.0]\\n\\n*************']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/tutorials/dart.rst?ref=master', 'name': 'dart.rst', 'path': 'doc/tutorials/dart.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dart.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 12:\n","\n","DART BOOSTER -  - PARAMETERS\n","\n","['XGBoost mostly combines a huge number of regression trees with a small learning rate.\\nIn this situation, trees added early are significant and trees added late are unimportant.\\n\\nVinayak and Gilad-Bachrach proposed a new method to add dropout techniques from the deep neural net community to boosted trees, and reported better results in some situations.\\n\\nThis is a instruction of new tree booster ``dart``.\\n\\n**************']\n","* ``rate_drop``:[': dropout rate.\\n\\n  - range: [0.0, 1.0]']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/tutorials/dart.rst?ref=master', 'name': 'dart.rst', 'path': 'doc/tutorials/dart.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dart.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 13:\n","\n","DART BOOSTER -  - PARAMETERS\n","\n","['XGBoost mostly combines a huge number of regression trees with a small learning rate.\\nIn this situation, trees added early are significant and trees added late are unimportant.\\n\\nVinayak and Gilad-Bachrach proposed a new method to add dropout techniques from the deep neural net community to boosted trees, and reported better results in some situations.\\n\\nThis is a instruction of new tree booster ``dart``.\\n\\n**************']\n","* ``sample_type``:[': type of sampling algorithm.\\n\\n  - ``uniform``: (default) dropped trees are selected uniformly.\\n  - ``weighted``: dropped trees are selected in proportion to weight.']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/tutorials/dart.rst?ref=master', 'name': 'dart.rst', 'path': 'doc/tutorials/dart.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dart.rst'}\n","----------------------------------------------------------------------------------------------------\n","Document 14:\n","\n"," - XGBOOST R TUTORIAL\n","\n","['## Introduction\\n\\n\\n**XGBoost** is short for e**X**treme **G**radient **Boost**ing package.\\n\\nThe purpose of this Vignette is to show you how to use **XGBoost** to build a model and make predictions.\\n\\nIt is an efficient and scalable implementation of gradient boosting framework by @friedman2000additive and @friedman2001greedy. Two solvers are included:\\n\\n- *linear* model ;\\n- *tree learning* algorithm.\\n\\nIt supports various objective functions, including *regression*, *classification* and *ranking*. The package is made to be extendible, so that users are also allowed to define their own objective functions easily.\\n\\nIt has been [used](https://github.com/dmlc/xgboost) to win several [Kaggle](http://www.kaggle.com) competitions.\\n\\nIt has several features:\\n\\n* Speed: it can automatically do parallel computation on *Windows* and *Linux*, with *OpenMP*. It is generally over 10 times faster than the classical `gbm`.\\n* Input Type: it takes several types of input data:\\n    * *Dense* Matrix: *R*\\'s *dense* matrix, i.e. `matrix` ;\\n    * *Sparse* Matrix: *R*\\'s *sparse* matrix, i.e. `Matrix::dgCMatrix` ;\\n    * Data File: local data files ;\\n    * `xgb.DMatrix`: its own class (recommended).\\n* Sparsity: it accepts *sparse* input for both *tree booster*  and *linear booster*, and is optimized for *sparse* input ;\\n* Customization: it supports customized objective functions and evaluation functions.\\n\\n## Installation\\n\\n\\n### GitHub version\\n\\n\\nFor weekly updated version (highly recommended), install from *GitHub*:\\n\\n\\n```r\\ninstall.packages(\"drat\", repos=\"https://cran.rstudio.com\")\\ndrat:::addRepo(\"dmlc\")\\ninstall.packages(\"xgboost\", repos=\"http://dmlc.ml/drat/\", type = \"source\")\\n```\\n\\n> *Windows* users will need to install [Rtools](http://cran.r-project.org/bin/windows/Rtools/) first.\\n\\n### CRAN version\\n\\n\\nThe version 0.4-2 is on CRAN, and you can install it by:\\n\\n\\n```r\\ninstall.packages(\"xgboost\")\\n```\\n\\nFormerly available versions can be obtained from the CRAN [archive](http://cran.r-project.org/src/contrib/Archive/xgboost)\\n\\n## Learning\\n\\n\\nFor the purpose of this tutorial we will load **XGBoost** package.\\n\\n\\n```r\\nrequire(xgboost)\\n```\\n\\n### Dataset presentation\\n\\n\\nIn this example, we are aiming to predict whether a mushroom can be eaten or not (like in many tutorials, example data are the same as you will use on in your every day life :-).\\n\\nMushroom data is cited from UCI Machine Learning Repository. @Bache+Lichman:2013.\\n\\n### Dataset loading\\n\\n\\nWe will load the `agaricus` datasets embedded with the package and will link them to variables.\\n\\nThe datasets are already split in:\\n\\n* `train`: will be used to build the model ;\\n* `test`: will be used to assess the quality of our model.\\n\\nWhy *split* the dataset in two parts?\\n\\nIn the first part we will build our model. In the second part we will want to test it and assess its quality. Without dividing the dataset we would test the model on the data which the algorithm have already seen.\\n\\n\\n```r\\ndata(agaricus.train, package=\\'xgboost\\')\\ndata(agaricus.test, package=\\'xgboost\\')\\ntrain <- agaricus.train\\ntest <- agaricus.test\\n```\\n\\n> In the real world, it would be up to you to make this division between `train` and `test` data. The way to do it is out of scope for this article, however `caret` package may [help](http://topepo.github.io/caret/data-splitting.html).\\n\\nEach variable is a `list` containing two things, `label` and `data`:\\n\\n\\n```r\\nstr(train)\\n```\\n\\n```\\n## List of 2\\n##  $ data :Formal class \\'dgCMatrix\\' [package \"Matrix\"] with 6 slots\\n##   .. ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...\\n##   .. ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991 ...\\n##   .. ..@ Dim     : int [1:2] 6513 126\\n##   .. ..@ Dimnames:List of 2\\n##   .. .. ..$ : NULL\\n##   .. .. ..$ : chr [1:126] \"cap-shape=bell\" \"cap-shape=conical\" \"cap-shape=convex\" \"cap-shape=flat\" ...\\n##   .. ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...\\n##   .. ..@ factors : list()\\n##  $ label: num [1:6513] 1 0 0 1 0 0 0 1 0 0 ...\\n```\\n\\n`label` is the outcome of our dataset meaning it is the binary *classification* we will try to predict.\\n\\nLet\\'s discover the dimensionality of our datasets.\\n\\n\\n```r\\ndim(train$data)\\n```\\n\\n```\\n## [1] 6513  126\\n```\\n\\n```r\\ndim(test$data)\\n```\\n\\n```\\n## [1] 1611  126\\n```\\n\\nThis dataset is very small to not make the **R** package too heavy, however **XGBoost** is built to manage huge datasets very efficiently.\\n\\nAs seen below, the `data` are stored in a `dgCMatrix` which is a *sparse* matrix and `label` vector is a `numeric` vector (`{0,1}`):\\n\\n\\n```r\\nclass(train$data)[1]\\n```\\n\\n```\\n## [1] \"dgCMatrix\"\\n```\\n\\n```r\\nclass(train$label)\\n```\\n\\n```\\n## [1] \"numeric\"\\n```\\n\\n### Basic Training using XGBoost\\n\\n\\nThis step is the most critical part of the process for the quality of our model.\\n\\n#### Basic training\\n\\nWe are using the `train` data. As explained above, both `data` and `label` are stored in a `list`.\\n\\nIn a *sparse* matrix, cells containing `0` are not stored in memory. Therefore, in a dataset mainly made of `0`, memory size is reduced. It is very common to have such a dataset.\\n\\nWe will train decision tree model using the following parameters:\\n\\n* `objective = \"binary:logistic\"`: we will train a binary classification model ;\\n* `max.depth = 2`: the trees won\\'t be deep, because our case is very simple ;\\n* `nthread = 2`: the number of CPU threads we are going to use;\\n* `nrounds = 2`: there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.\\n\\n\\n```r\\nbstSparse <- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n> The more complex the relationship between your features and your `label` is, the more passes you need.\\n\\n#### Parameter variations\\n\\n##### Dense matrix\\n\\nAlternatively, you can put your dataset in a *dense* matrix, i.e. a basic **R** matrix.\\n\\n\\n```r\\nbstDense <- xgboost(data = as.matrix(train$data), label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n##### xgb.DMatrix\\n\\n**XGBoost** offers a way to group them in a `xgb.DMatrix`. You can even add other meta data in it. This will be useful for the most advanced features we will discover later.\\n\\n\\n```r\\ndtrain <- xgb.DMatrix(data = train$data, label = train$label)\\nbstDMatrix <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n##### Verbose option\\n\\n**XGBoost** has several features to help you view the learning progress internally. The purpose is to help you to set the best parameters, which is the key of your model quality.\\n\\nOne of the simplest way to see the training progress is to set the `verbose` option (see below for more advanced techniques).\\n\\n\\n```r\\n# verbose = 0, no message\\nbst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\", verbose = 0)\\n```\\n\\n\\n```r\\n# verbose = 1, print evaluation metric\\nbst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\", verbose = 1)\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n\\n```r\\n# verbose = 2, also print information about tree\\nbst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\", verbose = 2)\\n```\\n\\n```\\n## [11:41:01] amalgamation/../src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\\n## [0]\\ttrain-error:0.046522\\n## [11:41:01] amalgamation/../src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n## Basic prediction using XGBoost\\n\\n\\n## Perform the prediction\\n\\n\\nThe purpose of the model we have built is to classify new data. As explained before, we will use the `test` dataset for this step.\\n\\n\\n```r\\npred <- predict(bst, test$data)\\n\\n# size of the prediction vector\\nprint(length(pred))\\n```\\n\\n```\\n## [1] 1611\\n```\\n\\n```r\\n# limit display of predictions to the first 10\\nprint(head(pred))\\n```\\n\\n```\\n## [1] 0.28583017 0.92392391 0.28583017 0.28583017 0.05169873 0.92392391\\n```\\n\\nThese numbers doesn\\'t look like *binary classification* `{0,1}`. We need to perform a simple transformation before being able to use these results.\\n\\n## Transform the regression in a binary classification\\n\\n\\nThe only thing that **XGBoost** does is a *regression*. **XGBoost** is using `label` vector to build its *regression* model.\\n\\nHow can we use a *regression* model to perform a binary classification?\\n\\nIf we think about the meaning of a regression applied to our data, the numbers we get are probabilities that a datum will be classified as `1`. Therefore, we will set the rule that if this probability for a specific datum is `> 0.5` then the observation is classified as `1` (or `0` otherwise).\\n\\n\\n```r\\nprediction <- as.numeric(pred > 0.5)\\nprint(head(prediction))\\n```\\n\\n```\\n## [1] 0 1 0 0 0 1\\n```\\n\\n## Measuring model performance\\n\\n\\nTo measure the model performance, we will compute a simple metric, the *average error*.\\n\\n\\n```r\\nerr <- mean(as.numeric(pred > 0.5) != test$label)\\nprint(paste(\"test-error=\", err))\\n```\\n\\n```\\n## [1] \"test-error= 0.0217256362507759\"\\n```\\n\\n> Note that the algorithm has not seen the `test` data during the model construction.\\n\\nSteps explanation:\\n\\n1. `as.numeric(pred > 0.5)` applies our rule that when the probability (<=> regression <=> prediction) is `> 0.5` the observation is classified as `1` and `0` otherwise ;\\n2. `probabilityVectorPreviouslyComputed != test$label` computes the vector of error between true data and computed probabilities ;\\n3. `mean(vectorOfErrors)` computes the *average error* itself.\\n\\nThe most important thing to remember is that **to do a classification, you just do a regression to the** `label` **and then apply a threshold**.\\n\\n*Multiclass* classification works in a similar way.\\n\\nThis metric is **0.02** and is pretty low: our yummly mushroom model works well!\\n\\n## Advanced features\\n\\n\\nMost of the features below have been implemented to help you to improve your model by offering a better understanding of its content.\\n\\n\\n### Dataset preparation\\n\\n\\nFor the following advanced features, we need to put data in `xgb.DMatrix` as explained above.\\n\\n\\n```r\\ndtrain <- xgb.DMatrix(data = train$data, label=train$label)\\ndtest <- xgb.DMatrix(data = test$data, label=test$label)\\n```\\n\\n### Measure learning progress with xgb.train\\n\\n\\nBoth `xgboost` (simple) and `xgb.train` (advanced) functions train models.\\n\\nOne of the special features of `xgb.train` is the capacity to follow the progress of the learning after each round. Because of the way boosting works, there is a time when having too many rounds lead to overfitting. You can see this feature as a cousin of a cross-validation method. The following techniques will help you to avoid overfitting or optimizing the learning time in stopping it as soon as possible.\\n\\nOne way to measure progress in the learning of a model is to provide to **XGBoost** a second dataset already classified. Therefore it can learn on the first dataset and test its model on the second one. Some metrics are measured after each round during the learning.\\n\\n> in some way it is similar to what we have done above with the average error. The main difference is that above it was after building the model, and now it is during the construction that we measure errors.\\n\\nFor the purpose of this example, we use `watchlist` parameter. It is a list of `xgb.DMatrix`, each of them tagged with a name.\\n\\n\\n```r\\nwatchlist <- list(train=dtrain, test=dtest)\\n\\nbst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\ttest-error:0.042831\\n## [1]\\ttrain-error:0.022263\\ttest-error:0.021726\\n```\\n\\n**XGBoost** has computed at each round the same average error metric seen above (we set `nrounds` to 2, that is why we have two lines). Obviously, the `train-error` number is related to the training dataset (the one the algorithm learns from) and the `test-error` number to the test dataset.\\n\\nBoth training and test error related metrics are very similar, and in some way, it makes sense: what we have learned from the training dataset matches the observations from the test dataset.\\n\\nIf with your own dataset you do not have such results, you should think about how you divided your dataset in training and test. May be there is something to fix. Again, `caret` package may [help](http://topepo.github.io/caret/data-splitting.html).\\n\\nFor a better understanding of the learning progression, you may want to have some specific metric or even use multiple evaluation metrics.\\n\\n\\n```r\\nbst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, eval.metric = \"error\", eval.metric = \"logloss\", objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\ttrain-logloss:0.233376\\ttest-error:0.042831\\ttest-logloss:0.226686\\n## [1]\\ttrain-error:0.022263\\ttrain-logloss:0.136658\\ttest-error:0.021726\\ttest-logloss:0.137874\\n```\\n\\n> `eval.metric` allows us to monitor two new metrics for each round, `logloss` and `error`.\\n\\n### Linear boosting\\n\\n\\nUntil now, all the learnings we have performed were based on boosting trees. **XGBoost** implements a second algorithm, based on linear boosting. The only difference with the previous command is `booster = \"gblinear\"` parameter (and removing `eta` parameter).\\n\\n\\n```r\\nbst <- xgb.train(data=dtrain, booster = \"gblinear\", nthread = 2, nrounds=2, watchlist=watchlist, eval.metric = \"error\", eval.metric = \"logloss\", objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.024720\\ttrain-logloss:0.184616\\ttest-error:0.022967\\ttest-logloss:0.184234\\n## [1]\\ttrain-error:0.004146\\ttrain-logloss:0.069885\\ttest-error:0.003724\\ttest-logloss:0.068081\\n```\\n\\nIn this specific case, *linear boosting* gets slightly better performance metrics than a decision tree based algorithm.\\n\\nIn simple cases, this will happen because there is nothing better than a linear algorithm to catch a linear link. However, decision trees are much better to catch a non linear link between predictors and outcome. Because there is no silver bullet, we advise you to check both algorithms with your own datasets to have an idea of what to use.\\n\\n### Manipulating xgb.DMatrix\\n\\n\\n#### Save / Load\\n\\nLike saving models, `xgb.DMatrix` object (which groups both dataset and outcome) can also be saved using `xgb.DMatrix.save` function.\\n\\n\\n```r\\nxgb.DMatrix.save(dtrain, \"dtrain.buffer\")\\n```\\n\\n```\\n## [1] TRUE\\n```\\n\\n```r\\n# to load it in, simply call xgb.DMatrix\\ndtrain2 <- xgb.DMatrix(\"dtrain.buffer\")\\n```\\n\\n```\\n## [11:41:01] 6513x126 matrix with 143286 entries loaded from dtrain.buffer\\n```\\n\\n```r\\nbst <- xgb.train(data=dtrain2, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\ttest-error:0.042831\\n## [1]\\ttrain-error:0.022263\\ttest-error:0.021726\\n```\\n\\n\\n\\n#### Information extraction\\n\\nInformation can be extracted from an `xgb.DMatrix` using `getinfo` function. Hereafter we will extract `label` data.\\n\\n\\n```r\\nlabel = getinfo(dtest, \"label\")\\npred <- predict(bst, dtest)\\nerr <- as.numeric(sum(as.integer(pred > 0.5) != label))/length(label)\\nprint(paste(\"test-error=\", err))\\n```\\n\\n```\\n## [1] \"test-error= 0.0217256362507759\"\\n```\\n\\n### View feature importance/influence from the learnt model\\n\\n\\nFeature importance is similar to R gbm package\\'s relative influence (rel.inf).\\n\\n```\\nimportance_matrix <- xgb.importance(model = bst)\\nprint(importance_matrix)\\nxgb.plot.importance(importance_matrix = importance_matrix)\\n```\\n\\n#### View the trees from a model\\n\\n\\nYou can dump the tree you learned using `xgb.dump` into a text file.\\n\\n\\n```r\\nxgb.dump(bst, with_stats = TRUE)\\n```\\n\\n```\\n##  [1] \"booster[0]\"\\n##  [2] \"0:[f28<-1.00136e-05] yes=1,no=2,missing=1,gain=4000.53,cover=1628.25\"\\n##  [3] \"1:[f55<-1.00136e-05] yes=3,no=4,missing=3,gain=1158.21,cover=924.5\"\\n##  [4] \"3:leaf=1.71218,cover=812\"\\n##  [5] \"4:leaf=-1.70044,cover=112.5\"\\n##  [6] \"2:[f108<-1.00136e-05] yes=5,no=6,missing=5,gain=198.174,cover=703.75\"\\n##  [7] \"5:leaf=-1.94071,cover=690.5\"\\n##  [8] \"6:leaf=1.85965,cover=13.25\"\\n##  [9] \"booster[1]\"\\n## [10] \"0:[f59<-1.00136e-05] yes=1,no=2,missing=1,gain=832.545,cover=788.852\"\\n## [11] \"1:[f28<-1.00136e-05] yes=3,no=4,missing=3,gain=569.725,cover=768.39\"\\n## [12] \"3:leaf=0.784718,cover=458.937\"\\n## [13] \"4:leaf=-0.96853,cover=309.453\"\\n## [14] \"2:leaf=-6.23624,cover=20.4624\"\\n```\\n\\nYou can plot the trees from your model using ```xgb.plot.tree``\\n\\n```\\nxgb.plot.tree(model = bst)\\n```\\n\\n> if you provide a path to `fname` parameter you can save the trees to your hard drive.\\n\\n#### Save and load models\\n\\n\\nMaybe your dataset is big, and it takes time to train a model on it? May be you are not a big fan of losing time in redoing the same task again and again? In these very rare cases, you will want to save your model and load it when required.\\n\\nHelpfully for you, **XGBoost** implements such functions.\\n\\n\\n```r\\n# save model to binary local file\\nxgb.save(bst, \"xgboost.model\")\\n```\\n\\n```\\n## [1] TRUE\\n```\\n\\n> `xgb.save` function should return TRUE if everything goes well and crashes otherwise.\\n\\nAn interesting test to see how identical our saved model is to the original one would be to compare the two predictions.\\n\\n\\n```r\\n# load binary model to R\\nbst2 <- xgb.load(\"xgboost.model\")\\npred2 <- predict(bst2, test$data)\\n\\n# And now the test\\nprint(paste(\"sum(abs(pred2-pred))=\", sum(abs(pred2-pred))))\\n```\\n\\n```\\n## [1] \"sum(abs(pred2-pred))= 0\"\\n```\\n\\n\\n\\n> result is `0`? We are good!\\n\\nIn some very specific cases, like when you want to pilot **XGBoost** from `caret` package, you will want to save the model as a *R* binary vector. See below how to do it.\\n\\n\\n```r\\n# save model to R\\'s raw vector\\nrawVec <- xgb.save.raw(bst)\\n\\n# print class\\nprint(class(rawVec))\\n```\\n\\n```\\n## [1] \"raw\"\\n```\\n\\n```r\\n# load binary model to R\\nbst3 <- xgb.load(rawVec)\\npred3 <- predict(bst3, test$data)\\n\\n# pred3 should be identical to pred\\nprint(paste(\"sum(abs(pred3-pred))=\", sum(abs(pred3-pred))))\\n```\\n\\n```\\n## [1] \"sum(abs(pred3-pred))= 0\"\\n```\\n\\n> Again `0`? It seems that `XGBoost` works pretty well!\\n\\n## References']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/R-package/xgboostPresentation.md?ref=master', 'name': 'xgboostPresentation.md', 'path': 'doc/R-package/xgboostPresentation.md', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/R-package/xgboostPresentation.md'}\n","----------------------------------------------------------------------------------------------------\n","Document 15:\n","\n"," - DEMO FOR GLM\n","\n","['\"\"\"\\nimport os\\n\\nimport xgboost as xgb\\n\\n##\\n#  this script demonstrate how to fit generalized linear model in xgboost\\n#  basically, we are using linear model, instead of tree for our boosters\\n##\\nCURRENT_DIR = os.path.dirname(__file__)\\ndtrain = xgb.DMatrix(\\n    os.path.join(CURRENT_DIR, \"../data/agaricus.txt.train?format=libsvm\")\\n)\\ndtest = xgb.DMatrix(\\n    os.path.join(CURRENT_DIR, \"../data/agaricus.txt.test?format=libsvm\")\\n)\\n# change booster to gblinear, so that we are fitting a linear model\\n# alpha is the L1 regularizer\\n# lambda is the L2 regularizer\\n# you can also set lambda_bias which is L2 regularizer on the bias term\\nparam = {\\n    \"objective\": \"binary:logistic\",\\n    \"booster\": \"gblinear\",\\n    \"alpha\": 0.0001,\\n    \"lambda\": 1,\\n}\\n\\n# normally, you do not need to set eta (step_size)\\n# XGBoost uses a parallel coordinate descent algorithm (shotgun),\\n# there could be affection on convergence with parallelization on certain cases\\n# setting eta to be smaller value, e.g 0.5 can make the optimization more stable\\n# param[\\'eta\\'] = 1\\n\\n##\\n# the rest of settings are the same\\n##\\nwatchlist = [(dtest, \"eval\"), (dtrain, \"train\")]\\nnum_round = 4\\nbst = xgb.train(param, dtrain, num_round, watchlist)\\npreds = bst.predict(dtest)\\nlabels = dtest.get_label()\\nprint(\\n    \"error=%f\"\\n    % (\\n        sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i])\\n        / float(len(preds))\\n    )\\n)']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/demo/guide-python/generalized_linear_model.py?ref=master', 'name': 'generalized_linear_model.py', 'path': 'demo/guide-python/generalized_linear_model.py', 'source': 'https://github.com/dmlc/xgboost/blob/master/demo/guide-python/generalized_linear_model.py'}\n"]}]},{"cell_type":"markdown","source":["\n","## Re-Ranking"],"metadata":{"id":"vWvLfsg3ynWQ"}},{"cell_type":"code","source":["!pip install --upgrade --quiet flashrank"],"metadata":{"id":"VB3n4TtjwB2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.retrievers import ContextualCompressionRetriever\n","from langchain.retrievers.document_compressors import FlashrankRerank\n","\n","compressor = FlashrankRerank()\n","\n","compression_retriever = ContextualCompressionRetriever(\n","    base_compressor=compressor, base_retriever=retriever_from_llm\n",")\n","\n","compressed_docs = compression_retriever.invoke(\n","    \"What is the parameter booster do in XGBOOST?\"\n",")\n","# print([doc.metadata[\"id\"] for doc in compressed_docs])"],"metadata":{"id":"FoZ0wJerynT4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722350803368,"user_tz":-330,"elapsed":23935,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"053f8252-a674-4403-90d6-abfa13f55a91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['Question 1:', 'Can you explain the role of the booster algorithm in XGBOOST?', 'Question 2:', 'In XGBOOST, what function does the booster perform during the training process?', 'Question 3:', \"How does XGBOOST's booster differ from other gradient boosting methods and what is its specific contribution?\"]\n"]}]},{"cell_type":"code","source":["pretty_print_docs(compressed_docs)"],"metadata":{"id":"PzSYS-hOynQ9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722350803368,"user_tz":-330,"elapsed":8,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"abd6ab69-59d8-428b-fabe-b89ad355c2fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Document 1:\n","\n"," - INTRODUCTION\n","\n","['**XGBoost** is an implementation of the famous gradient boosting algorithm. This model is often described as a *blackbox*, meaning it works well but it is not trivial to understand how. Indeed, the model is made of hundreds (thousands?) of decision trees. You may wonder how possible a human would be able to have a general view of the model?\\n\\nWhile XGBoost is known for its fast speed and accurate predictive power, it also comes with various functions to help you understand the model.\\nThe purpose of this RMarkdown document is to demonstrate how easily we can leverage the functions already implemented in **XGBoost R** package. Of course, everything showed below can be applied to the dataset you may have to manipulate at work or wherever!\\n\\nFirst we will prepare the **Otto** dataset and train a model, then we will generate two visualisations to get a clue of what is important to the model, finally, we will see how we can leverage these information.']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/demo/kaggle-otto/understandingXGBoostModel.Rmd?ref=master', 'name': 'understandingXGBoostModel.Rmd', 'path': 'demo/kaggle-otto/understandingXGBoostModel.Rmd', 'source': 'https://github.com/dmlc/xgboost/blob/master/demo/kaggle-otto/understandingXGBoostModel.Rmd', 'relevance_score': 0.9991937}\n","----------------------------------------------------------------------------------------------------\n","Document 2:\n","\n"," -  - STANDALONE RANDOM FOREST WITH XGBOOST API\n","\n","\n","* ``booster``:[\"should be set to ``gbtree``, as we are training forests. Note that as this\\n  is the default, this parameter needn't be set explicitly.\"]\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/tutorials/rf.rst?ref=master', 'name': 'rf.rst', 'path': 'doc/tutorials/rf.rst', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/tutorials/rf.rst', 'relevance_score': 0.99913836}\n","----------------------------------------------------------------------------------------------------\n","Document 3:\n","\n"," - XGBOOST R TUTORIAL\n","\n","['## Introduction\\n\\n\\n**XGBoost** is short for e**X**treme **G**radient **Boost**ing package.\\n\\nThe purpose of this Vignette is to show you how to use **XGBoost** to build a model and make predictions.\\n\\nIt is an efficient and scalable implementation of gradient boosting framework by @friedman2000additive and @friedman2001greedy. Two solvers are included:\\n\\n- *linear* model ;\\n- *tree learning* algorithm.\\n\\nIt supports various objective functions, including *regression*, *classification* and *ranking*. The package is made to be extendible, so that users are also allowed to define their own objective functions easily.\\n\\nIt has been [used](https://github.com/dmlc/xgboost) to win several [Kaggle](http://www.kaggle.com) competitions.\\n\\nIt has several features:\\n\\n* Speed: it can automatically do parallel computation on *Windows* and *Linux*, with *OpenMP*. It is generally over 10 times faster than the classical `gbm`.\\n* Input Type: it takes several types of input data:\\n    * *Dense* Matrix: *R*\\'s *dense* matrix, i.e. `matrix` ;\\n    * *Sparse* Matrix: *R*\\'s *sparse* matrix, i.e. `Matrix::dgCMatrix` ;\\n    * Data File: local data files ;\\n    * `xgb.DMatrix`: its own class (recommended).\\n* Sparsity: it accepts *sparse* input for both *tree booster*  and *linear booster*, and is optimized for *sparse* input ;\\n* Customization: it supports customized objective functions and evaluation functions.\\n\\n## Installation\\n\\n\\n### GitHub version\\n\\n\\nFor weekly updated version (highly recommended), install from *GitHub*:\\n\\n\\n```r\\ninstall.packages(\"drat\", repos=\"https://cran.rstudio.com\")\\ndrat:::addRepo(\"dmlc\")\\ninstall.packages(\"xgboost\", repos=\"http://dmlc.ml/drat/\", type = \"source\")\\n```\\n\\n> *Windows* users will need to install [Rtools](http://cran.r-project.org/bin/windows/Rtools/) first.\\n\\n### CRAN version\\n\\n\\nThe version 0.4-2 is on CRAN, and you can install it by:\\n\\n\\n```r\\ninstall.packages(\"xgboost\")\\n```\\n\\nFormerly available versions can be obtained from the CRAN [archive](http://cran.r-project.org/src/contrib/Archive/xgboost)\\n\\n## Learning\\n\\n\\nFor the purpose of this tutorial we will load **XGBoost** package.\\n\\n\\n```r\\nrequire(xgboost)\\n```\\n\\n### Dataset presentation\\n\\n\\nIn this example, we are aiming to predict whether a mushroom can be eaten or not (like in many tutorials, example data are the same as you will use on in your every day life :-).\\n\\nMushroom data is cited from UCI Machine Learning Repository. @Bache+Lichman:2013.\\n\\n### Dataset loading\\n\\n\\nWe will load the `agaricus` datasets embedded with the package and will link them to variables.\\n\\nThe datasets are already split in:\\n\\n* `train`: will be used to build the model ;\\n* `test`: will be used to assess the quality of our model.\\n\\nWhy *split* the dataset in two parts?\\n\\nIn the first part we will build our model. In the second part we will want to test it and assess its quality. Without dividing the dataset we would test the model on the data which the algorithm have already seen.\\n\\n\\n```r\\ndata(agaricus.train, package=\\'xgboost\\')\\ndata(agaricus.test, package=\\'xgboost\\')\\ntrain <- agaricus.train\\ntest <- agaricus.test\\n```\\n\\n> In the real world, it would be up to you to make this division between `train` and `test` data. The way to do it is out of scope for this article, however `caret` package may [help](http://topepo.github.io/caret/data-splitting.html).\\n\\nEach variable is a `list` containing two things, `label` and `data`:\\n\\n\\n```r\\nstr(train)\\n```\\n\\n```\\n## List of 2\\n##  $ data :Formal class \\'dgCMatrix\\' [package \"Matrix\"] with 6 slots\\n##   .. ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...\\n##   .. ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991 ...\\n##   .. ..@ Dim     : int [1:2] 6513 126\\n##   .. ..@ Dimnames:List of 2\\n##   .. .. ..$ : NULL\\n##   .. .. ..$ : chr [1:126] \"cap-shape=bell\" \"cap-shape=conical\" \"cap-shape=convex\" \"cap-shape=flat\" ...\\n##   .. ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...\\n##   .. ..@ factors : list()\\n##  $ label: num [1:6513] 1 0 0 1 0 0 0 1 0 0 ...\\n```\\n\\n`label` is the outcome of our dataset meaning it is the binary *classification* we will try to predict.\\n\\nLet\\'s discover the dimensionality of our datasets.\\n\\n\\n```r\\ndim(train$data)\\n```\\n\\n```\\n## [1] 6513  126\\n```\\n\\n```r\\ndim(test$data)\\n```\\n\\n```\\n## [1] 1611  126\\n```\\n\\nThis dataset is very small to not make the **R** package too heavy, however **XGBoost** is built to manage huge datasets very efficiently.\\n\\nAs seen below, the `data` are stored in a `dgCMatrix` which is a *sparse* matrix and `label` vector is a `numeric` vector (`{0,1}`):\\n\\n\\n```r\\nclass(train$data)[1]\\n```\\n\\n```\\n## [1] \"dgCMatrix\"\\n```\\n\\n```r\\nclass(train$label)\\n```\\n\\n```\\n## [1] \"numeric\"\\n```\\n\\n### Basic Training using XGBoost\\n\\n\\nThis step is the most critical part of the process for the quality of our model.\\n\\n#### Basic training\\n\\nWe are using the `train` data. As explained above, both `data` and `label` are stored in a `list`.\\n\\nIn a *sparse* matrix, cells containing `0` are not stored in memory. Therefore, in a dataset mainly made of `0`, memory size is reduced. It is very common to have such a dataset.\\n\\nWe will train decision tree model using the following parameters:\\n\\n* `objective = \"binary:logistic\"`: we will train a binary classification model ;\\n* `max.depth = 2`: the trees won\\'t be deep, because our case is very simple ;\\n* `nthread = 2`: the number of CPU threads we are going to use;\\n* `nrounds = 2`: there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.\\n\\n\\n```r\\nbstSparse <- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n> The more complex the relationship between your features and your `label` is, the more passes you need.\\n\\n#### Parameter variations\\n\\n##### Dense matrix\\n\\nAlternatively, you can put your dataset in a *dense* matrix, i.e. a basic **R** matrix.\\n\\n\\n```r\\nbstDense <- xgboost(data = as.matrix(train$data), label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n##### xgb.DMatrix\\n\\n**XGBoost** offers a way to group them in a `xgb.DMatrix`. You can even add other meta data in it. This will be useful for the most advanced features we will discover later.\\n\\n\\n```r\\ndtrain <- xgb.DMatrix(data = train$data, label = train$label)\\nbstDMatrix <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n##### Verbose option\\n\\n**XGBoost** has several features to help you view the learning progress internally. The purpose is to help you to set the best parameters, which is the key of your model quality.\\n\\nOne of the simplest way to see the training progress is to set the `verbose` option (see below for more advanced techniques).\\n\\n\\n```r\\n# verbose = 0, no message\\nbst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\", verbose = 0)\\n```\\n\\n\\n```r\\n# verbose = 1, print evaluation metric\\nbst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\", verbose = 1)\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n\\n```r\\n# verbose = 2, also print information about tree\\nbst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\", verbose = 2)\\n```\\n\\n```\\n## [11:41:01] amalgamation/../src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\\n## [0]\\ttrain-error:0.046522\\n## [11:41:01] amalgamation/../src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\\n## [1]\\ttrain-error:0.022263\\n```\\n\\n## Basic prediction using XGBoost\\n\\n\\n## Perform the prediction\\n\\n\\nThe purpose of the model we have built is to classify new data. As explained before, we will use the `test` dataset for this step.\\n\\n\\n```r\\npred <- predict(bst, test$data)\\n\\n# size of the prediction vector\\nprint(length(pred))\\n```\\n\\n```\\n## [1] 1611\\n```\\n\\n```r\\n# limit display of predictions to the first 10\\nprint(head(pred))\\n```\\n\\n```\\n## [1] 0.28583017 0.92392391 0.28583017 0.28583017 0.05169873 0.92392391\\n```\\n\\nThese numbers doesn\\'t look like *binary classification* `{0,1}`. We need to perform a simple transformation before being able to use these results.\\n\\n## Transform the regression in a binary classification\\n\\n\\nThe only thing that **XGBoost** does is a *regression*. **XGBoost** is using `label` vector to build its *regression* model.\\n\\nHow can we use a *regression* model to perform a binary classification?\\n\\nIf we think about the meaning of a regression applied to our data, the numbers we get are probabilities that a datum will be classified as `1`. Therefore, we will set the rule that if this probability for a specific datum is `> 0.5` then the observation is classified as `1` (or `0` otherwise).\\n\\n\\n```r\\nprediction <- as.numeric(pred > 0.5)\\nprint(head(prediction))\\n```\\n\\n```\\n## [1] 0 1 0 0 0 1\\n```\\n\\n## Measuring model performance\\n\\n\\nTo measure the model performance, we will compute a simple metric, the *average error*.\\n\\n\\n```r\\nerr <- mean(as.numeric(pred > 0.5) != test$label)\\nprint(paste(\"test-error=\", err))\\n```\\n\\n```\\n## [1] \"test-error= 0.0217256362507759\"\\n```\\n\\n> Note that the algorithm has not seen the `test` data during the model construction.\\n\\nSteps explanation:\\n\\n1. `as.numeric(pred > 0.5)` applies our rule that when the probability (<=> regression <=> prediction) is `> 0.5` the observation is classified as `1` and `0` otherwise ;\\n2. `probabilityVectorPreviouslyComputed != test$label` computes the vector of error between true data and computed probabilities ;\\n3. `mean(vectorOfErrors)` computes the *average error* itself.\\n\\nThe most important thing to remember is that **to do a classification, you just do a regression to the** `label` **and then apply a threshold**.\\n\\n*Multiclass* classification works in a similar way.\\n\\nThis metric is **0.02** and is pretty low: our yummly mushroom model works well!\\n\\n## Advanced features\\n\\n\\nMost of the features below have been implemented to help you to improve your model by offering a better understanding of its content.\\n\\n\\n### Dataset preparation\\n\\n\\nFor the following advanced features, we need to put data in `xgb.DMatrix` as explained above.\\n\\n\\n```r\\ndtrain <- xgb.DMatrix(data = train$data, label=train$label)\\ndtest <- xgb.DMatrix(data = test$data, label=test$label)\\n```\\n\\n### Measure learning progress with xgb.train\\n\\n\\nBoth `xgboost` (simple) and `xgb.train` (advanced) functions train models.\\n\\nOne of the special features of `xgb.train` is the capacity to follow the progress of the learning after each round. Because of the way boosting works, there is a time when having too many rounds lead to overfitting. You can see this feature as a cousin of a cross-validation method. The following techniques will help you to avoid overfitting or optimizing the learning time in stopping it as soon as possible.\\n\\nOne way to measure progress in the learning of a model is to provide to **XGBoost** a second dataset already classified. Therefore it can learn on the first dataset and test its model on the second one. Some metrics are measured after each round during the learning.\\n\\n> in some way it is similar to what we have done above with the average error. The main difference is that above it was after building the model, and now it is during the construction that we measure errors.\\n\\nFor the purpose of this example, we use `watchlist` parameter. It is a list of `xgb.DMatrix`, each of them tagged with a name.\\n\\n\\n```r\\nwatchlist <- list(train=dtrain, test=dtest)\\n\\nbst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\ttest-error:0.042831\\n## [1]\\ttrain-error:0.022263\\ttest-error:0.021726\\n```\\n\\n**XGBoost** has computed at each round the same average error metric seen above (we set `nrounds` to 2, that is why we have two lines). Obviously, the `train-error` number is related to the training dataset (the one the algorithm learns from) and the `test-error` number to the test dataset.\\n\\nBoth training and test error related metrics are very similar, and in some way, it makes sense: what we have learned from the training dataset matches the observations from the test dataset.\\n\\nIf with your own dataset you do not have such results, you should think about how you divided your dataset in training and test. May be there is something to fix. Again, `caret` package may [help](http://topepo.github.io/caret/data-splitting.html).\\n\\nFor a better understanding of the learning progression, you may want to have some specific metric or even use multiple evaluation metrics.\\n\\n\\n```r\\nbst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, eval.metric = \"error\", eval.metric = \"logloss\", objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\ttrain-logloss:0.233376\\ttest-error:0.042831\\ttest-logloss:0.226686\\n## [1]\\ttrain-error:0.022263\\ttrain-logloss:0.136658\\ttest-error:0.021726\\ttest-logloss:0.137874\\n```\\n\\n> `eval.metric` allows us to monitor two new metrics for each round, `logloss` and `error`.\\n\\n### Linear boosting\\n\\n\\nUntil now, all the learnings we have performed were based on boosting trees. **XGBoost** implements a second algorithm, based on linear boosting. The only difference with the previous command is `booster = \"gblinear\"` parameter (and removing `eta` parameter).\\n\\n\\n```r\\nbst <- xgb.train(data=dtrain, booster = \"gblinear\", nthread = 2, nrounds=2, watchlist=watchlist, eval.metric = \"error\", eval.metric = \"logloss\", objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.024720\\ttrain-logloss:0.184616\\ttest-error:0.022967\\ttest-logloss:0.184234\\n## [1]\\ttrain-error:0.004146\\ttrain-logloss:0.069885\\ttest-error:0.003724\\ttest-logloss:0.068081\\n```\\n\\nIn this specific case, *linear boosting* gets slightly better performance metrics than a decision tree based algorithm.\\n\\nIn simple cases, this will happen because there is nothing better than a linear algorithm to catch a linear link. However, decision trees are much better to catch a non linear link between predictors and outcome. Because there is no silver bullet, we advise you to check both algorithms with your own datasets to have an idea of what to use.\\n\\n### Manipulating xgb.DMatrix\\n\\n\\n#### Save / Load\\n\\nLike saving models, `xgb.DMatrix` object (which groups both dataset and outcome) can also be saved using `xgb.DMatrix.save` function.\\n\\n\\n```r\\nxgb.DMatrix.save(dtrain, \"dtrain.buffer\")\\n```\\n\\n```\\n## [1] TRUE\\n```\\n\\n```r\\n# to load it in, simply call xgb.DMatrix\\ndtrain2 <- xgb.DMatrix(\"dtrain.buffer\")\\n```\\n\\n```\\n## [11:41:01] 6513x126 matrix with 143286 entries loaded from dtrain.buffer\\n```\\n\\n```r\\nbst <- xgb.train(data=dtrain2, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = \"binary:logistic\")\\n```\\n\\n```\\n## [0]\\ttrain-error:0.046522\\ttest-error:0.042831\\n## [1]\\ttrain-error:0.022263\\ttest-error:0.021726\\n```\\n\\n\\n\\n#### Information extraction\\n\\nInformation can be extracted from an `xgb.DMatrix` using `getinfo` function. Hereafter we will extract `label` data.\\n\\n\\n```r\\nlabel = getinfo(dtest, \"label\")\\npred <- predict(bst, dtest)\\nerr <- as.numeric(sum(as.integer(pred > 0.5) != label))/length(label)\\nprint(paste(\"test-error=\", err))\\n```\\n\\n```\\n## [1] \"test-error= 0.0217256362507759\"\\n```\\n\\n### View feature importance/influence from the learnt model\\n\\n\\nFeature importance is similar to R gbm package\\'s relative influence (rel.inf).\\n\\n```\\nimportance_matrix <- xgb.importance(model = bst)\\nprint(importance_matrix)\\nxgb.plot.importance(importance_matrix = importance_matrix)\\n```\\n\\n#### View the trees from a model\\n\\n\\nYou can dump the tree you learned using `xgb.dump` into a text file.\\n\\n\\n```r\\nxgb.dump(bst, with_stats = TRUE)\\n```\\n\\n```\\n##  [1] \"booster[0]\"\\n##  [2] \"0:[f28<-1.00136e-05] yes=1,no=2,missing=1,gain=4000.53,cover=1628.25\"\\n##  [3] \"1:[f55<-1.00136e-05] yes=3,no=4,missing=3,gain=1158.21,cover=924.5\"\\n##  [4] \"3:leaf=1.71218,cover=812\"\\n##  [5] \"4:leaf=-1.70044,cover=112.5\"\\n##  [6] \"2:[f108<-1.00136e-05] yes=5,no=6,missing=5,gain=198.174,cover=703.75\"\\n##  [7] \"5:leaf=-1.94071,cover=690.5\"\\n##  [8] \"6:leaf=1.85965,cover=13.25\"\\n##  [9] \"booster[1]\"\\n## [10] \"0:[f59<-1.00136e-05] yes=1,no=2,missing=1,gain=832.545,cover=788.852\"\\n## [11] \"1:[f28<-1.00136e-05] yes=3,no=4,missing=3,gain=569.725,cover=768.39\"\\n## [12] \"3:leaf=0.784718,cover=458.937\"\\n## [13] \"4:leaf=-0.96853,cover=309.453\"\\n## [14] \"2:leaf=-6.23624,cover=20.4624\"\\n```\\n\\nYou can plot the trees from your model using ```xgb.plot.tree``\\n\\n```\\nxgb.plot.tree(model = bst)\\n```\\n\\n> if you provide a path to `fname` parameter you can save the trees to your hard drive.\\n\\n#### Save and load models\\n\\n\\nMaybe your dataset is big, and it takes time to train a model on it? May be you are not a big fan of losing time in redoing the same task again and again? In these very rare cases, you will want to save your model and load it when required.\\n\\nHelpfully for you, **XGBoost** implements such functions.\\n\\n\\n```r\\n# save model to binary local file\\nxgb.save(bst, \"xgboost.model\")\\n```\\n\\n```\\n## [1] TRUE\\n```\\n\\n> `xgb.save` function should return TRUE if everything goes well and crashes otherwise.\\n\\nAn interesting test to see how identical our saved model is to the original one would be to compare the two predictions.\\n\\n\\n```r\\n# load binary model to R\\nbst2 <- xgb.load(\"xgboost.model\")\\npred2 <- predict(bst2, test$data)\\n\\n# And now the test\\nprint(paste(\"sum(abs(pred2-pred))=\", sum(abs(pred2-pred))))\\n```\\n\\n```\\n## [1] \"sum(abs(pred2-pred))= 0\"\\n```\\n\\n\\n\\n> result is `0`? We are good!\\n\\nIn some very specific cases, like when you want to pilot **XGBoost** from `caret` package, you will want to save the model as a *R* binary vector. See below how to do it.\\n\\n\\n```r\\n# save model to R\\'s raw vector\\nrawVec <- xgb.save.raw(bst)\\n\\n# print class\\nprint(class(rawVec))\\n```\\n\\n```\\n## [1] \"raw\"\\n```\\n\\n```r\\n# load binary model to R\\nbst3 <- xgb.load(rawVec)\\npred3 <- predict(bst3, test$data)\\n\\n# pred3 should be identical to pred\\nprint(paste(\"sum(abs(pred3-pred))=\", sum(abs(pred3-pred))))\\n```\\n\\n```\\n## [1] \"sum(abs(pred3-pred))= 0\"\\n```\\n\\n> Again `0`? It seems that `XGBoost` works pretty well!\\n\\n## References']\n","Metadata: {'doc_api': 'https://api.github.com/repos/dmlc/xgboost/contents/doc/R-package/xgboostPresentation.md?ref=master', 'name': 'xgboostPresentation.md', 'path': 'doc/R-package/xgboostPresentation.md', 'source': 'https://github.com/dmlc/xgboost/blob/master/doc/R-package/xgboostPresentation.md', 'relevance_score': 0.9988533}\n"]}]},{"cell_type":"markdown","source":["## Generation Layer"],"metadata":{"id":"Q9aFh7SiynOA"}},{"cell_type":"code","source":["def get_docs(docs):\n","  return f\"\\n{'-' * 100}\\n\".join(\n","            [\n","                f\"Document {i+1}:\\n\\n{d.page_content}\\nSource: {d.metadata['source']}\"\n","                for i, d in enumerate(docs)\n","            ]\n","          )\n"],"metadata":{"id":"sZamtIlu73ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint\n","\n","repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","# repo_id = 'dunzhang/stella_en_1.5B_v5'\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id=repo_id,\n","    task=\"text-generation\",\n","    max_new_tokens=512,\n","    do_sample=False,\n","    repetition_penalty=1.03,\n","    temperature = 0.8\n",")"],"metadata":{"id":"tt4HpaXYym12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722350803369,"user_tz":-330,"elapsed":6,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"c35da622-1ca9-4b0a-80c4-cb2c3cb872a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["from langchain_core.runnables import RunnableParallel, RunnablePassthrough"],"metadata":{"id":"TQxoxBTi9u0L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","\n","template = \"\"\"\n","###\n","You love to explain technical concepts, given you have approriate context.\n","Use the following pieces of context to answer the question given to you.\n","If the answer is not there in the context, just say that you don't know the answer, don't try to make up an answer.\n","Your answer must write the references to the source of context the otheriwse it wil be considered as plagarism. The refence link will be provided in the in the metadata.\n","\n","### Context :\n","{context}\n","\n","### Question:\n","{question}\n","\n","### Decription of a good answer :\n","* A good answer will provide reference like provided in academic papers and websites\n","* A good answer will not contain information which is not provided in the context\n","* An excellent answer will provide examples with codes to answer the question if it can be provided\n","\"\"\"\n","\n","gen_promt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")"],"metadata":{"id":"GwQl4RCw2ygn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = gen_promt|llm"],"metadata":{"id":"zVnJeb1v4DNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.invoke({'context':'idk','question':\"what is boosters parameter in xgboost? \"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"82oDg1qI4n4j","executionInfo":{"status":"ok","timestamp":1722350806909,"user_tz":-330,"elapsed":528,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"0a1246dc-3390-4f09-fdc6-ab484e72eae9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n### Answer: \\nThe boosters parameter in XGBoost refers to the type of boosting algorithm used during the training process. XGBoost supports various boosting algorithms such as Gradient Boosting Decision Tree (GBDT), Gradient Boosting Regressor (GBR), and Multi:Softmax for multiclass objective functions. By specifying the booster type, we can choose the appropriate algorithm for our specific problem. For instance, GBDT is suitable for regression problems, while GBR and Multi:Softmax are better options for classification problems.\\n\\nReference:\\n- Chen, T., Meng, Q., Ma, W., Wang, T., Zhu, Y., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1130-1141.\\n- https://xgboost.readthedocs.io/en/stable/parameters.html#booster\\n\\n### Example:\\n```python\\nimport xgboost as xgb\\nX_train, y_train = load_data() # assuming load_data function loads data\\n\\n# Using Gradient Boosting Decision Tree for regression\\ngbdt_reg = xgb.XGBRegressor(objective='reg:squarederror',\\n                           booster='gbtree',\\n                           num_round=10)\\ngbdt_reg.fit(X_train, y_train)\\n\\n# Using Gradient Boosting Regressor for regression\\ngbr_reg = xgb.XGBRegressor(objective='reg:squarederror',\\n                          booster='gblinear',\\n                          num_round=10)\\ngbr_reg.fit(X_train, y_train)\\n\\n# Using Multi:Softmax for multiclass classification\\nmulticlass_softmax = xgb.XGBClassifier(objective='multi:softmax',\\n                                     num_class=5,\\n                                     num_round=10)\\nmulticlass_softmax.fit(X_train, y_train)\\n```\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":["chain2 = compression_retriever|get_docs"],"metadata":{"id":"XbGKy3RQFZ9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\n","    chain2.invoke('what is boosters parameter in xgboost?')\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YMjcAhqG7bm","executionInfo":{"status":"ok","timestamp":1722351008628,"user_tz":-330,"elapsed":27178,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"cdb10fc8-332a-4820-8419-2453895718ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['Version 1:', 'Can you explain the function and significance of the booster parameter when using XGBoost?', 'Version 2:', 'How would I set or modify the booster type for an XGBoost model?', 'Version 3:', 'Could you provide some insights into the impact of various booster algorithms in XGBoost and how to select the best one for a specific dataset?']\n"]},{"output_type":"stream","name":"stdout","text":["Document 1:\n","\n","XGBOOST PARAMETERS - PARAMETERS FOR CATEGORICAL FEATURE - GENERAL PARAMETERS\n","\n","['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']\n","* ``skip_drop``:['[default=0.0]\\n\\n  - Probability of skipping the dropout procedure during a boosting iteration.\\n\\n    - If a dropout is skipped, new trees are added in the same manner as ``gbtree``.\\n    - Note that non-zero ``skip_drop`` has higher priority than ``rate_drop`` or ``one_drop``.\\n\\n  - range: [0.0, 1.0]\\n\\nParameters for Linear Booster (``booster=gblinear``)\\n====================================================']\n","Source: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n","----------------------------------------------------------------------------------------------------\n","Document 2:\n","\n"," -  - STANDALONE RANDOM FOREST WITH XGBOOST API\n","\n","\n","* ``booster``:[\"should be set to ``gbtree``, as we are training forests. Note that as this\\n  is the default, this parameter needn't be set explicitly.\"]\n","Source: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/rf.rst\n","----------------------------------------------------------------------------------------------------\n","Document 3:\n","\n","XGBOOST PARAMETERS - PARAMETERS FOR TREE BOOSTER - GENERAL PARAMETERS\n","\n","['Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\\n\\n- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\\n- **Booster parameters** depend on which booster you have chosen\\n- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\\n- **Command line parameters** relate to behavior of CLI version of XGBoost.\\n\\n.. note:: Parameters in R package\\n\\n  In R-package, you can use ``.`` (dot) to replace underscore in the parameters, for example, you can use ``max.depth`` to indicate ``max_depth``. The underscore parameters are also valid in R.\\n\\n.. contents::\\n  :backlinks: none\\n  :local:\\n\\n\\n.. _global_config:\\n\\n********************']\n","* ``sampling_method``:['[default= ``uniform``]\\n\\n  - The method to use to sample the training instances.\\n  - ``uniform``: each training instance has an equal probability of being selected. Typically set\\n    ``subsample`` >= 0.5 for good results.\\n  - ``gradient_based``: the selection probability for each training instance is proportional to the\\n    *regularized absolute value* of gradients (more specifically, :math:`\\\\sqrt{g^2+\\\\lambda h^2}`).\\n    ``subsample`` may be set to as low as 0.1 without loss of model accuracy. Note that this\\n    sampling method is only supported when ``tree_method`` is set to ``hist`` and the device is ``cuda``; other tree\\n    methods only support ``uniform`` sampling.']\n","Source: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n"]}]},{"cell_type":"code","source":["RAG_chain = (\n","    {\n","    \"context\": retriever_from_llm|get_docs,\n","    \"question\": RunnablePassthrough()\n","    }\n","    |chain\n",")"],"metadata":{"id":"Tx7sKj6n9LJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(RAG_chain.invoke('what is boosters parameter in xgboost?'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXOpHVu1CaY2","executionInfo":{"status":"ok","timestamp":1722350941906,"user_tz":-330,"elapsed":11137,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"fae27507-5c64-475f-cd4b-2b10d5a8bbe1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['Version 1:', 'Can you explain the function and significance of the booster parameter when using XGBoost?', 'Version 2:', 'How would I set or modify the booster type for an XGBoost model?', 'Version 3:', 'Could you provide some insights into the impact of various booster algorithms in XGBoost and how to select the best one for a specific dataset?']\n"]},{"output_type":"stream","name":"stdout","text":["* An excellent answer will provide explanation of the concept related to the question if it can be provided\n","\n","### Answer:\n","Boosters are models used in the XGBoost library for making predictions. The main boosters used in XGBoost are 'gbtree' and 'gblinear'. 'gbtree' is used for classification and regression problems while 'gblinear' is used for regression problems only. Other boosters like 'dart' are also available in XGBoost, which are variations of the standard tree boosters and provide better performance in certain situations.\n","\n","Booster parameters are specific to the chosen booster and control the way the model is trained and the resulting model's behavior. For example, for the 'gbtree' booster, some important parameters include 'max_depth', 'learning_rate', and 'num_rounds'. These parameters control the maximum depth of the tree, the step size shrinkage used to prevent overfitting, and the number of boosting rounds respectively.\n","\n","Reference:\n","* <https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst>\n","* <https://github.com/dmlc/xgboost/blob/master/doc/tutorials/rf.rst>\n","* <https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dart.rst>\n","\n","Examples:\n","Here is an example of setting booster and booster parameters for the 'gbtree' booster in Python:\n","```python\n","import xgboost as xgb\n","\n","# Data preprocessing and splitting\n","X_train, X_test, y_train, y_test = load_data()\n","\n","# Define the parameters for the gbtree booster\n","params = {\n","    'max_depth': 3,\n","    'learning_rate': 0.1,\n","    'num_rounds': 100\n","}\n","\n","# Create the DMatrix data object\n","dtrain = xgb.DMatrix(data=X_train, label=y_train)\n","dtest = xgb.DMatrix(data=X_test, label=y_test)\n","\n","# Initialize the gbtree booster with the specified parameters\n","model = xgb\n"]}]},{"cell_type":"markdown","source":["### CONTD\n"],"metadata":{"id":"lpno1gx4oP3c"}},{"cell_type":"code","source":["import time\n","import random\n","while True :\n","  time.sleep(random.randint(100,500))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"id":"ZoUyGrNSoR6v","executionInfo":{"status":"error","timestamp":1722359860412,"user_tz":-330,"elapsed":8,"user":{"displayName":"Samriddh Lakhmani","userId":"02521612938973610721"}},"outputId":"193b99db-e65a-479c-8cf0-fdce341c514f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'time' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-245-0532aa9f185e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"]}]}]}